{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8Ym4VN3AL1uugOqlUV5K0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/grokking-deep-learning/blob/main/chapter_10_neural_learning_about_edges_and_corners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10 | Neural learning about edges and corners"
      ],
      "metadata": {
        "id": "Z0ugU8JDvDpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx96BMZ5u_Ii",
        "outputId": "f02d19d3-8cb6-4b64-a6b8-dd328197aced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "hidden_size: 10000\n",
            "I: 0 \t Training correct: 0.0009 \t Test correct: 0.0444\n",
            "I: 50 \t Training correct: 0.0070 \t Test correct: 0.4642\n",
            "I: 100 \t Training correct: 0.0122 \t Test correct: 0.7449\n",
            "I: 150 \t Training correct: 0.0128 \t Test correct: 0.7932\n",
            "I: 200 \t Training correct: 0.0133 \t Test correct: 0.8115\n",
            "I: 250 \t Training correct: 0.0132 \t Test correct: 0.8242\n",
            "I: 299 \t Training correct: 0.0139 \t Test correct: 0.8250\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Freeze the random seed for reproducability.\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load the dataset.\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Take the first 1000 images, and normalize the features between 0 and 1.\n",
        "images, labels = (X_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])\n",
        "\n",
        "# Convert to one hot encoding\n",
        "one_hot_labels = np.zeros((len(labels),10))\n",
        "for i,l in enumerate(labels):\n",
        "    one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "test_images = X_test.reshape(len(X_test),28*28) / 255\n",
        "test_labels = np.zeros((len(y_test),10))\n",
        "for i,l in enumerate(y_test):\n",
        "    test_labels[i][l] = 1\n",
        "\n",
        "# Activation functions\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "    return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "    temp = np.exp(x)\n",
        "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 2\n",
        "iterations = 300\n",
        "input_dim = 784\n",
        "output_dim = 10\n",
        "batch_size = 128\n",
        "\n",
        "input_rows = 28\n",
        "input_cols = 28\n",
        "\n",
        "kernel_rows = 3\n",
        "kernel_cols = 3\n",
        "num_kernels = 16\n",
        "hidden_size = ((input_rows - kernel_rows) * (input_cols - kernel_cols)) * num_kernels\n",
        "\n",
        "print(f\"hidden_size: {hidden_size}\")\n",
        "\n",
        "# Initialize the network.\n",
        "kernels = 0.02 * np.random.random((kernel_rows*kernel_cols, num_kernels)) - 0.01\n",
        "weights_1_2 = 0.2 * np.random.random((hidden_size, output_dim)) - 0.1\n",
        "\n",
        "def get_image_section(layer, row_from, row_to, col_from, col_to):\n",
        "    \"\"\" Select a subregion in a batch of images. \"\"\"\n",
        "    section = layer[:, row_from:row_to, col_from:col_to]\n",
        "    return section.reshape(-1,1, row_to-row_from, col_to-col_from)\n",
        "\n",
        "# Training loop\n",
        "for j in range(iterations):\n",
        "    correct_cnt = 0\n",
        "    for i in range(int(len(images) / batch_size)):\n",
        "        batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
        "        input, target = images[batch_start:batch_end], labels[batch_start:batch_end]\n",
        "\n",
        "        # Forward pass\n",
        "        layer_0 = input\n",
        "        layer_0 = layer_0.reshape(layer_0.shape[0], 28, 28)\n",
        "\n",
        "        sections = list()\n",
        "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
        "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
        "                section = get_image_section(layer_0,\n",
        "                                            row_start,\n",
        "                                            row_start + kernel_rows,\n",
        "                                            col_start,\n",
        "                                            col_start + kernel_rows)\n",
        "                sections.append(section)\n",
        "\n",
        "        expanded_input = np.concatenate(sections, axis=1)\n",
        "        es = expanded_input.shape\n",
        "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
        "\n",
        "        kernel_output = flattened_input.dot(kernels)\n",
        "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
        "        # Dropout\n",
        "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
        "        layer_1 *= dropout_mask * 2\n",
        "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
        "        prediction = layer_2\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            true_label = labels[batch_start+k:batch_start+k+1]\n",
        "            pred_label = prediction[k:k+1]\n",
        "            correct_cnt += int(np.argmax(pred_label) == np.argmax(true_label))\n",
        "\n",
        "        # Back progation\n",
        "        layer_2_delta = (target - prediction) / (batch_size * layer_2.shape[0])\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
        "        layer_1_delta *= dropout_mask\n",
        "\n",
        "        # Update the weights\n",
        "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "        lld_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
        "        k_update = flattened_input.T.dot(lld_reshape)\n",
        "        kernels -= alpha * k_update\n",
        "\n",
        "    # Every 50 iterations and the final iteration once finished training.\n",
        "    if (j % 50 == 0 or j == iterations - 1):\n",
        "\n",
        "        # Evaluate on the test set.\n",
        "        test_correct_cnt = 0\n",
        "        for i in range(int(len(test_images) / batch_size)):\n",
        "            batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
        "            input, target = test_images[batch_start:batch_end], test_labels[batch_start:batch_end]\n",
        "\n",
        "            # Forward pass\n",
        "            layer_0 = input\n",
        "            layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
        "\n",
        "            sections = list()\n",
        "\n",
        "            for row_start in range(layer_0.shape[1] - kernel_rows):\n",
        "                for col_start in range(layer_0.shape[2] - kernel_cols):\n",
        "                    section = get_image_section(layer_0,\n",
        "                                                row_start,\n",
        "                                                row_start + kernel_rows,\n",
        "                                                col_start,\n",
        "                                                col_start + kernel_rows)\n",
        "                    sections.append(section)\n",
        "\n",
        "            expanded_input = np.concatenate(sections, axis=1)\n",
        "            es = expanded_input.shape\n",
        "            flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
        "\n",
        "            kernel_output = flattened_input.dot(kernels)\n",
        "            layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
        "            # Dropout\n",
        "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
        "            layer_1 *= dropout_mask * 2\n",
        "            layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
        "            prediction = layer_2\n",
        "\n",
        "            for k in range(batch_size):\n",
        "                true_label = test_labels[batch_start+k:batch_start+k+1]\n",
        "                pred_label = prediction[k:k+1]\n",
        "                test_correct_cnt += int(np.argmax(pred_label) == np.argmax(true_label))\n",
        "\n",
        "        print(f\"I: {j} \\t Training correct: {correct_cnt/float(len(y_train)):.4f} \\t Test correct: {test_correct_cnt/float(len(y_test)):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two Convlutional Layers"
      ],
      "metadata": {
        "id": "41I0DLrPtfyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from typing import List\n",
        "\n",
        "class Tensor (object):\n",
        "\n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "\n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "\n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "\n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        "\n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "\n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "\n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if (self.creators is not None and\n",
        "               (self.all_children_grads_accounted_for() or\n",
        "                grad_origin is None)):\n",
        "\n",
        "                if (self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "\n",
        "                if (self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if (self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)\n",
        "\n",
        "                if (self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "\n",
        "                if (self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if (\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if (\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "                if (self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "\n",
        "                if (self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad* (ones - (self * self)))\n",
        "\n",
        "                if (self.creation_op == \"relu\"):\n",
        "                    self.creators[0].backward(self.grad * (self > Tensor(0)))\n",
        "\n",
        "                if (self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "\n",
        "                if (self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "\n",
        "                # Handling convolution (conv2d) operation\n",
        "                if (self.creation_op == \"conv2d\"):\n",
        "                    x, weight, bias = self.creators\n",
        "                    batch_size, out_channels, out_height, out_width = self.grad.data.shape\n",
        "                    _, in_channels, kernel_size, _ = weight.data.shape\n",
        "\n",
        "                    # Initialize gradients\n",
        "                    weight_grad = np.zeros_like(weight.data)\n",
        "                    input_grad = np.zeros_like(x.data)\n",
        "                    bias_grad = np.zeros_like(bias.data)\n",
        "\n",
        "                    # Calculate padding\n",
        "                    pad_h = max((out_height - 1) - (x.data.shape[2] - kernel_size), 0)\n",
        "                    pad_w = max((out_width - 1) - (x.data.shape[3] - kernel_size), 0)\n",
        "                    pad_top = pad_h // 2\n",
        "                    pad_bottom = pad_h - pad_top\n",
        "                    pad_left = pad_w // 2\n",
        "                    pad_right = pad_w - pad_left\n",
        "\n",
        "                    # Pad input if necessary\n",
        "                    if pad_h > 0 or pad_w > 0:\n",
        "                        x_padded = np.pad(x.data, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), 'constant')\n",
        "                    else:\n",
        "                        x_padded = x.data\n",
        "\n",
        "                    # Compute gradients\n",
        "                    for b in range(batch_size):\n",
        "                        for c_out in range(out_channels):\n",
        "                            for c_in in range(in_channels):\n",
        "                                for h_out in range(out_height):\n",
        "                                    h_in = h_out\n",
        "                                    for w_out in range(out_width):\n",
        "                                        w_in = w_out\n",
        "                                        # Weight gradient\n",
        "                                        weight_grad[c_out, c_in] += \\\n",
        "                                            self.grad.data[b, c_out, h_out, w_out] * \\\n",
        "                                            x_padded[b, c_in, h_in:h_in + kernel_size, w_in:w_in + kernel_size]\n",
        "\n",
        "                                        # Input gradient\n",
        "                                        input_grad[b, c_in, h_in:h_in + kernel_size, w_in:w_in + kernel_size] += \\\n",
        "                                            self.grad.data[b, c_out, h_out, w_out] * \\\n",
        "                                            weight.data[c_out, c_in]\n",
        "\n",
        "                    # Bias gradient\n",
        "                    bias_grad = self.grad.data.sum(axis=(0, 2, 3))\n",
        "\n",
        "                    # Backpropagate gradients\n",
        "                    x.backward(Tensor(input_grad))\n",
        "                    weight.backward(Tensor(weight_grad))\n",
        "                    bias.backward(Tensor(bias_grad))\n",
        "\n",
        "                # Handling max pooling (maxpool2d)\n",
        "                if self.creation_op == \"maxpool2d\":\n",
        "                    x = self.creators[0]\n",
        "                    batch_size, channels, height, width = x.data.shape\n",
        "                    kernel_size = self.grad.data.shape[2]\n",
        "                    stride = self.grad.data.shape[3]\n",
        "                    input_grad = np.zeros_like(x.data)\n",
        "\n",
        "                    # Backpropagate through max pooling\n",
        "                    for b in range(batch_size):\n",
        "                        for c in range(channels):\n",
        "                            for h in range(0, height - kernel_size + 1, stride):\n",
        "                                for w in range(0, width - kernel_size + 1, stride):\n",
        "                                    patch = x.data[b, c, h:h + kernel_size, w:w + kernel_size]\n",
        "                                    max_val = np.max(patch)\n",
        "                                    grad_patch = (patch == max_val) * self.grad.data[b, c, h // stride, w // stride]\n",
        "                                    input_grad[b, c, h:h + kernel_size, w:w + kernel_size] += grad_patch\n",
        "\n",
        "                    x.backward(Tensor(input_grad))\n",
        "\n",
        "                # Handling the flatten operation\n",
        "                if self.creation_op == \"flatten\":\n",
        "                    original_shape = self.creators[0].data.shape\n",
        "                    self.creators[0].backward(Tensor(self.grad.data.reshape(original_shape)))\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return np.shape(self.data)\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data > other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\">\")\n",
        "        return Tensor(self.data > other.data)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)\n",
        "\n",
        "    def sum(self, dim):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "\n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "\n",
        "        if (self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "\n",
        "    def transpose(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "\n",
        "        return Tensor(self.data.transpose())\n",
        "\n",
        "    def mm(self, x):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())\n",
        "\n",
        "    def sigmoid(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)), autograd=True, creators=[self], creation_op = \"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(np.tanh(self.data), autograd=True, creators=[self], creation_op = \"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "\n",
        "    def relu(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(np.maximum(self.data, 0), autograd=True, creators=[self], creation_op = \"relu\")\n",
        "        return Tensor(np.maximum(self.data, 0))\n",
        "\n",
        "    def index_select(self, indices):\n",
        "        if (self.autograd):\n",
        "            new = Tensor(self.data[indices.data], autograd=True, creators=[self], creation_op = \"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "\n",
        "    def cross_entropy(self, target_indices):\n",
        "        \"\"\" Numerically stable cross entropy \"\"\"\n",
        "        temp = np.exp(self.data - np.max(self.data))\n",
        "        softmax_output = temp / (np.sum(temp, axis=len(self.data.shape)-1, keepdims=True) + 1e-10)\n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t), -1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "\n",
        "        # Add a small epsilon to avoid log(0)\n",
        "        epsilon = 1e-10\n",
        "        loss = -(np.log(p + epsilon) * target_dist).sum(1).mean()\n",
        "\n",
        "        if self.autograd:\n",
        "            out = Tensor(loss, autograd=True, creators=[self], creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "\n",
        "    def _extract_patches(self, input_data, kernel_size, stride=1, padding=0):\n",
        "        \"\"\"\n",
        "        Extracts patches from the input tensor for convolution operations.\n",
        "\n",
        "        Parameters:\n",
        "        - input_data (numpy.ndarray): Input data of shape (batch_size, in_channels, height, width).\n",
        "        - kernel_size (int): Size of the convolution kernel.\n",
        "        - stride (int): Stride of the convolution. Default is 1.\n",
        "        - padding (int): Zero-padding added to both sides of the input. Default is 0.\n",
        "\n",
        "        Returns:\n",
        "        - patches (numpy.ndarray): Extracted patches of shape\n",
        "                                  (batch_size, out_height, out_width, in_channels, kernel_size, kernel_size).\n",
        "        \"\"\"\n",
        "        batch_size, in_channels, height, width = input_data.shape\n",
        "\n",
        "        # Apply padding if needed\n",
        "        if padding > 0:\n",
        "            input_data = np.pad(input_data,\n",
        "                                ((0, 0), (0, 0), (padding, padding), (padding, padding)),\n",
        "                                mode='constant')\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        out_height = (height + 2 * padding - kernel_size) // stride + 1\n",
        "        out_width = (width + 2 * padding - kernel_size) // stride + 1\n",
        "\n",
        "        # Initialize an array to hold the patches\n",
        "        patches = np.zeros((batch_size, out_height, out_width, in_channels, kernel_size, kernel_size))\n",
        "\n",
        "        # Extract patches by sliding over the input tensor\n",
        "        for b in range(batch_size):\n",
        "            for c in range(in_channels):\n",
        "                for i in range(0, out_height):\n",
        "                    for j in range(0, out_width):\n",
        "                        patch = input_data[b, c, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size]\n",
        "                        patches[b, i, j, c, :, :] = patch\n",
        "\n",
        "        return patches\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.params: List[Tensor] = []\n",
        "\n",
        "    def forward(self, inp: Tensor) -> Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_params(self) -> List[Tensor]:\n",
        "        return self.params\n",
        "\n",
        "class Conv2d(Layer):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        k = 1.0 / (in_channels * kernel_size * kernel_size)\n",
        "        kernel_shape = (out_channels, in_channels, kernel_size, kernel_size)\n",
        "        self.weight = Tensor(np.random.uniform(-np.sqrt(k), np.sqrt(k), kernel_shape), autograd=True)\n",
        "        self.bias = Tensor(np.zeros(out_channels), autograd=True)\n",
        "        self.params = [self.weight, self.bias]\n",
        "\n",
        "    def _extract_patches(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Extract patches from input for convolution\"\"\"\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        out_height = (height - self.kernel_size) // self.stride + 1\n",
        "        out_width = (width - self.kernel_size) // self.stride + 1\n",
        "\n",
        "        patches = np.zeros((batch_size, out_height, out_width, channels,\n",
        "                          self.kernel_size, self.kernel_size))\n",
        "\n",
        "        for h in range(0, height - self.kernel_size + 1, self.stride):\n",
        "            for w in range(0, width - self.kernel_size + 1, self.stride):\n",
        "                patch = x[:, :, h:h+self.kernel_size, w:w+self.kernel_size]\n",
        "                patches[:, h//self.stride, w//self.stride] = patch\n",
        "\n",
        "        return patches\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        batch_size = x.data.shape[0]\n",
        "        patches = self._extract_patches(x.data)\n",
        "\n",
        "        # Reshape patches for matrix multiplication\n",
        "        patches_reshaped = patches.reshape(batch_size, -1,\n",
        "                                         self.in_channels * self.kernel_size * self.kernel_size)\n",
        "        weight_reshaped = self.weight.data.reshape(self.out_channels, -1)\n",
        "\n",
        "        # Perform convolution as matrix multiplication\n",
        "        output = np.zeros((batch_size, self.out_channels,\n",
        "                          patches.shape[1], patches.shape[2]))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            out = patches_reshaped[b].dot(weight_reshaped.T)\n",
        "            output[b] = out.reshape(patches.shape[1], patches.shape[2], -1).transpose(2, 0, 1)\n",
        "\n",
        "        output += self.bias.data.reshape(1, -1, 1, 1)\n",
        "\n",
        "        return Tensor(output, autograd=True, creators=[x, self.weight, self.bias],\n",
        "                     creation_op=\"conv2d\")\n",
        "\n",
        "    def get_parameters(self) -> List[Tensor]:\n",
        "        return [self.weight, self.bias]\n",
        "\n",
        "class MaxPool2d(Layer):\n",
        "    def __init__(self, kernel_size: int, stride: int = None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride if stride is not None else kernel_size\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        batch_size, channels, height, width = x.data.shape\n",
        "        out_height = (height - self.kernel_size) // self.stride + 1\n",
        "        out_width = (width - self.kernel_size) // self.stride + 1\n",
        "\n",
        "        output = np.zeros((batch_size, channels, out_height, out_width))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for h in range(0, height - self.kernel_size + 1, self.stride):\n",
        "                    for w in range(0, width - self.kernel_size + 1, self.stride):\n",
        "                        output[b, c, h//self.stride, w//self.stride] = np.max(\n",
        "                            x.data[b, c, h:h+self.kernel_size, w:w+self.kernel_size])\n",
        "\n",
        "        return Tensor(output, autograd=True, creators=[x], creation_op=\"maxpool2d\")\n",
        "\n",
        "    def get_parameters(self) -> List[Tensor]:\n",
        "        return []\n",
        "\n",
        "class Flatten(Layer):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        batch_size = x.data.shape[0]\n",
        "        return Tensor(x.data.reshape(batch_size, -1), autograd=True,\n",
        "                     creators=[x], creation_op=\"flatten\")\n",
        "\n",
        "    def get_parameters(self) -> List[Tensor]:\n",
        "        return []\n",
        "\n",
        "class SGD(object):\n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "\n",
        "    def step(self, zero=True):\n",
        "        for p in self.parameters:\n",
        "            p.data -= p.grad.data * self.alpha\n",
        "\n",
        "            if (zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / (n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "\n",
        "        self.parameters.append(self.weight)\n",
        "        self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.mm(self.weight) + self.bias.expand(0, len(input.data))\n",
        "\n",
        "class Sequential(Layer):\n",
        "    def __init__(self, layers=list(), training=True):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.training = training\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "    def forward(self, input):\n",
        "        for l in self.layers:\n",
        "            input = l.forward(input)\n",
        "        return input\n",
        "\n",
        "class Dropout(Layer):\n",
        "    def __init__(self, p=0.5, training=True):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        self.training = training\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Only apply dropout when training.\n",
        "        if self.training:\n",
        "            # Multiply by 1 / (1 - p) to balance out the extra sensitivity.\n",
        "            self.mask = np.random.binomial(1, 1-self.p, input.shape) / (1-self.p)\n",
        "            return input * Tensor(self.mask, autograd=input.autograd)\n",
        "        return input\n",
        "\n",
        "    def backward(self, grad):\n",
        "        return grad * self.mask\n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "class MSELoss(Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        return ((pred - target) * (pred - target)).sum(0)\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "\n",
        "class Relu(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.relu()\n",
        "\n",
        "# Create the dataset.\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# One hot encoding.\n",
        "# y_train = np.eye(10)[y_train]\n",
        "# y_test = np.eye(10)[y_test]\n",
        "\n",
        "# Take the first 1000 samples.\n",
        "X_train = X_train[:100]\n",
        "y_train = y_train[:100]\n",
        "X_test = X_test[:100]\n",
        "y_test = y_test[:100]\n",
        "\n",
        "# Normalize the data.\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Hyperparameters.\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize the model.\n",
        "model = Sequential([\n",
        "    Conv2d(in_channels=1, out_channels=16, kernel_size=3),  # 28x28 -> 26x26\n",
        "    MaxPool2d(kernel_size=2),  # 26x26 -> 13x13\n",
        "    Relu(),\n",
        "    Conv2d(in_channels=16, out_channels=32, kernel_size=3),  # 13x13 -> 11x11\n",
        "    MaxPool2d(kernel_size=2),  # 11x11 -> 5x5\n",
        "    Relu(),\n",
        "    Dropout(p=0.1),\n",
        "    Flatten(),\n",
        "    Linear(32 * 5 * 5, 128),\n",
        "    Linear(128, 10),\n",
        "    Dropout(p=0.1),\n",
        "    Relu(),\n",
        "])\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = SGD(model.get_parameters(), alpha=learning_rate)\n",
        "\n",
        "n_samples = len(X_train)\n",
        "\n",
        "# Training loop.\n",
        "for epoch in range(epochs):\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "    # Iterate over the batches.\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "        batch_x = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        x = Tensor(batch_x.reshape(-1, 1, 28, 28), autograd=True)\n",
        "        target = Tensor(batch_y, autograd=True)\n",
        "\n",
        "        pred = model.forward(x)\n",
        "\n",
        "        for j in range(len(pred.data)):\n",
        "            pred_label = np.argmax(pred.data[j])\n",
        "            if pred_label == batch_y[j]:\n",
        "                correct += 1\n",
        "\n",
        "        loss = criterion.forward(pred, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        model.eval()\n",
        "        # Evaluate on the test set.\n",
        "        test_correct = 0\n",
        "        for i in range(len(X_test)):\n",
        "            x = Tensor(X_test[i].reshape(-1, 1, 28, 28), autograd=True)\n",
        "            pred = model.forward(x)\n",
        "            pred_label = np.argmax(pred.data)\n",
        "            if pred_label == y_test[i]:\n",
        "                test_correct += 1\n",
        "        print(f\"Train: {correct / len(X_train)} Test: {test_correct / len(X_test)}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / (n_samples/batch_size):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Tlq7DVuUGA",
        "outputId": "77b1b964-f755-4ae7-825a-0416c88f7a46"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "Train: 0.07 Test: 0.15\n",
            "Epoch 1, Average Loss: 2.9366\n",
            "epoch: 1\n",
            "Train: 0.11 Test: 0.08\n",
            "Epoch 2, Average Loss: 2.9705\n",
            "epoch: 2\n",
            "Train: 0.13 Test: 0.09\n",
            "Epoch 3, Average Loss: 2.9351\n",
            "epoch: 3\n",
            "Train: 0.16 Test: 0.12\n",
            "Epoch 4, Average Loss: 2.8856\n",
            "epoch: 4\n",
            "Train: 0.18 Test: 0.12\n",
            "Epoch 5, Average Loss: 2.7451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Data loading and transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ZA7asaSB7q",
        "outputId": "d46e493d-cd37-4371-a587-6d6f5f6ff4a1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 7798467.28it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1888525.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13273550.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5081496.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1/938], Loss: 2.3006\n",
            "Epoch [1/10], Step [101/938], Loss: 0.3920\n",
            "Epoch [1/10], Step [201/938], Loss: 0.2025\n",
            "Epoch [1/10], Step [301/938], Loss: 0.0853\n",
            "Epoch [1/10], Step [401/938], Loss: 0.1538\n",
            "Epoch [1/10], Step [501/938], Loss: 0.1636\n",
            "Epoch [1/10], Step [601/938], Loss: 0.0883\n",
            "Epoch [1/10], Step [701/938], Loss: 0.1175\n",
            "Epoch [1/10], Step [801/938], Loss: 0.2090\n",
            "Epoch [1/10], Step [901/938], Loss: 0.0457\n",
            "Epoch [2/10], Step [1/938], Loss: 0.0742\n",
            "Epoch [2/10], Step [101/938], Loss: 0.0204\n",
            "Epoch [2/10], Step [201/938], Loss: 0.2195\n",
            "Epoch [2/10], Step [301/938], Loss: 0.0675\n",
            "Epoch [2/10], Step [401/938], Loss: 0.0487\n",
            "Epoch [2/10], Step [501/938], Loss: 0.0655\n",
            "Epoch [2/10], Step [601/938], Loss: 0.0456\n",
            "Epoch [2/10], Step [701/938], Loss: 0.0888\n",
            "Epoch [2/10], Step [801/938], Loss: 0.0446\n",
            "Epoch [2/10], Step [901/938], Loss: 0.0525\n",
            "Epoch [3/10], Step [1/938], Loss: 0.0120\n",
            "Epoch [3/10], Step [101/938], Loss: 0.0134\n",
            "Epoch [3/10], Step [201/938], Loss: 0.0073\n",
            "Epoch [3/10], Step [301/938], Loss: 0.0649\n",
            "Epoch [3/10], Step [401/938], Loss: 0.0491\n",
            "Epoch [3/10], Step [501/938], Loss: 0.0038\n",
            "Epoch [3/10], Step [601/938], Loss: 0.0363\n",
            "Epoch [3/10], Step [701/938], Loss: 0.0372\n",
            "Epoch [3/10], Step [801/938], Loss: 0.1608\n",
            "Epoch [3/10], Step [901/938], Loss: 0.0721\n",
            "Epoch [4/10], Step [1/938], Loss: 0.0065\n",
            "Epoch [4/10], Step [101/938], Loss: 0.0167\n",
            "Epoch [4/10], Step [201/938], Loss: 0.0705\n",
            "Epoch [4/10], Step [301/938], Loss: 0.0061\n",
            "Epoch [4/10], Step [401/938], Loss: 0.0399\n",
            "Epoch [4/10], Step [501/938], Loss: 0.0256\n",
            "Epoch [4/10], Step [601/938], Loss: 0.0576\n",
            "Epoch [4/10], Step [701/938], Loss: 0.0110\n",
            "Epoch [4/10], Step [801/938], Loss: 0.0035\n",
            "Epoch [4/10], Step [901/938], Loss: 0.0211\n",
            "Epoch [5/10], Step [1/938], Loss: 0.0273\n",
            "Epoch [5/10], Step [101/938], Loss: 0.0309\n",
            "Epoch [5/10], Step [201/938], Loss: 0.0066\n",
            "Epoch [5/10], Step [301/938], Loss: 0.0308\n",
            "Epoch [5/10], Step [401/938], Loss: 0.0365\n",
            "Epoch [5/10], Step [501/938], Loss: 0.0393\n",
            "Epoch [5/10], Step [601/938], Loss: 0.0779\n",
            "Epoch [5/10], Step [701/938], Loss: 0.0608\n",
            "Epoch [5/10], Step [801/938], Loss: 0.0016\n",
            "Epoch [5/10], Step [901/938], Loss: 0.0101\n",
            "Epoch [6/10], Step [1/938], Loss: 0.0113\n",
            "Epoch [6/10], Step [101/938], Loss: 0.0101\n",
            "Epoch [6/10], Step [201/938], Loss: 0.0380\n",
            "Epoch [6/10], Step [301/938], Loss: 0.0045\n",
            "Epoch [6/10], Step [401/938], Loss: 0.0172\n",
            "Epoch [6/10], Step [501/938], Loss: 0.0808\n",
            "Epoch [6/10], Step [601/938], Loss: 0.1587\n",
            "Epoch [6/10], Step [701/938], Loss: 0.0445\n",
            "Epoch [6/10], Step [801/938], Loss: 0.0054\n",
            "Epoch [6/10], Step [901/938], Loss: 0.0026\n",
            "Epoch [7/10], Step [1/938], Loss: 0.0113\n",
            "Epoch [7/10], Step [101/938], Loss: 0.0402\n",
            "Epoch [7/10], Step [201/938], Loss: 0.0038\n",
            "Epoch [7/10], Step [301/938], Loss: 0.0012\n",
            "Epoch [7/10], Step [401/938], Loss: 0.0777\n",
            "Epoch [7/10], Step [501/938], Loss: 0.0286\n",
            "Epoch [7/10], Step [601/938], Loss: 0.0395\n",
            "Epoch [7/10], Step [701/938], Loss: 0.0043\n",
            "Epoch [7/10], Step [801/938], Loss: 0.0106\n",
            "Epoch [7/10], Step [901/938], Loss: 0.0497\n",
            "Epoch [8/10], Step [1/938], Loss: 0.0766\n",
            "Epoch [8/10], Step [101/938], Loss: 0.0023\n",
            "Epoch [8/10], Step [201/938], Loss: 0.0040\n",
            "Epoch [8/10], Step [301/938], Loss: 0.0213\n",
            "Epoch [8/10], Step [401/938], Loss: 0.0031\n",
            "Epoch [8/10], Step [501/938], Loss: 0.0820\n",
            "Epoch [8/10], Step [601/938], Loss: 0.0319\n",
            "Epoch [8/10], Step [701/938], Loss: 0.0010\n",
            "Epoch [8/10], Step [801/938], Loss: 0.0115\n",
            "Epoch [8/10], Step [901/938], Loss: 0.0133\n",
            "Epoch [9/10], Step [1/938], Loss: 0.0016\n",
            "Epoch [9/10], Step [101/938], Loss: 0.0588\n",
            "Epoch [9/10], Step [201/938], Loss: 0.0004\n",
            "Epoch [9/10], Step [301/938], Loss: 0.0514\n",
            "Epoch [9/10], Step [401/938], Loss: 0.0856\n",
            "Epoch [9/10], Step [501/938], Loss: 0.0207\n",
            "Epoch [9/10], Step [601/938], Loss: 0.0005\n",
            "Epoch [9/10], Step [701/938], Loss: 0.0027\n",
            "Epoch [9/10], Step [801/938], Loss: 0.0009\n",
            "Epoch [9/10], Step [901/938], Loss: 0.0421\n",
            "Epoch [10/10], Step [1/938], Loss: 0.0354\n",
            "Epoch [10/10], Step [101/938], Loss: 0.0043\n",
            "Epoch [10/10], Step [201/938], Loss: 0.0012\n",
            "Epoch [10/10], Step [301/938], Loss: 0.0111\n",
            "Epoch [10/10], Step [401/938], Loss: 0.0547\n",
            "Epoch [10/10], Step [501/938], Loss: 0.0182\n",
            "Epoch [10/10], Step [601/938], Loss: 0.0007\n",
            "Epoch [10/10], Step [701/938], Loss: 0.0153\n",
            "Epoch [10/10], Step [801/938], Loss: 0.0470\n",
            "Epoch [10/10], Step [901/938], Loss: 0.0210\n",
            "Test Accuracy: 99.24%\n"
          ]
        }
      ]
    }
  ]
}