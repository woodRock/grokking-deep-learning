{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjsYaRju37AQe2/GSROlKA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/grokking-deep-learning/blob/main/chapter_12_neural_networks__that_write_like_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12 | Neural networks that write like Shakespeare"
      ],
      "metadata": {
        "id": "vQ_Rfg2TqNlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "znSgRPHBdAnw"
      },
      "outputs": [],
      "source": [
        "# Download reviews.txt and labels.txt from here: https://github.com/udacity/deep-learning/tree/master/sentiment-network\n",
        "\n",
        "def pretty_print_review_and_label(i):\n",
        "   print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
        "\n",
        "g = open('reviews.txt','r') # What we know!\n",
        "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
        "g.close()\n",
        "\n",
        "g = open('labels.txt','r') # What we WANT to know!\n",
        "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
        "g.close()\n",
        "\n",
        "\n",
        "# Preprocess dataset:\n",
        "\n",
        "import sys\n",
        "\n",
        "f = open('reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "f = open('labels.txt')\n",
        "raw_labels = f.readlines()\n",
        "f.close()\n",
        "\n",
        "# Take only the first 1,000 reviews\n",
        "raw_reviews = raw_reviews[:1000]\n",
        "raw_labels = raw_labels[:1000]\n",
        "\n",
        "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
        "\n",
        "vocab = set()\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        if(len(word)>0):\n",
        "            vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "\n",
        "input_dataset = list()\n",
        "for sent in tokens:\n",
        "    sent_indices = list()\n",
        "    for word in sent:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(list(set(sent_indices)))\n",
        "\n",
        "target_dataset = list()\n",
        "for label in raw_labels:\n",
        "    if label == 'positive\\n':\n",
        "        target_dataset.append(1)\n",
        "    else:\n",
        "        target_dataset.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Freeze the seed for reprodicablity.\n",
        "np.random.seed(1)\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid2deriv(output):\n",
        "    return output * (1 - output)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "    return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "    temp = np.exp(x)\n",
        "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "alpha = 2,\n",
        "hidden_size = 400\n",
        "input_dim = len(vocab)\n",
        "output_dim = 2\n",
        "\n",
        "# Convert the text to vector embeddings\n",
        "vocab_size = len(vocab)\n",
        "one_hot_labels = np.zeros((len(input_dataset),vocab_size), dtype=np.int32)\n",
        "for i, sentence in enumerate(input_dataset):\n",
        "    for word_idx in sentence:\n",
        "        one_hot_labels[i][word_idx] = 1\n",
        "text = one_hot_labels\n",
        "\n",
        "# Convert the class labels to one hot enocding\n",
        "one_hot_labels = np.zeros((len(target_dataset), output_dim), dtype=np.int32)\n",
        "for i, l in enumerate(target_dataset):\n",
        "    one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "# Initialize the neural network\n",
        "weights_0_1 = 0.02 * np.random.random((len(vocab),hidden_size)) - 0.01\n",
        "weights_1_2 =  0.2 * np.random.random((hidden_size,output_dim)) -  0.1\n",
        "\n",
        "def similar(target='beautiful'):\n",
        "    target_index = word2index[target]\n",
        "\n",
        "    scores = Counter()\n",
        "    for word,index in word2index.items():\n",
        "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
        "        squared_difference = raw_difference * raw_difference\n",
        "        scores[word] = -math.sqrt(sum(squared_difference))\n",
        "    return scores.most_common(10)\n",
        "\n",
        "# Activation function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid2deriv(output):\n",
        "    return output * (1 - output)\n",
        "\n",
        "# Training loop\n",
        "for j in range(epochs):\n",
        "    correct_cnt = 0\n",
        "    for i in range(int(len(text) / batch_size)):\n",
        "        batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
        "        input, target = text[batch_start:batch_end], labels[batch_start:batch_end]\n",
        "\n",
        "        # Forward pass\n",
        "        layer_0 = input\n",
        "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
        "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
        "        layer_1 *= dropout_mask * 2\n",
        "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
        "        prediction = layer_2\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            pred_label = prediction[k:k+1]\n",
        "            true_label = labels[batch_start+k:batch_start+k+1]\n",
        "            correct_cnt += int(np.argmax(pred_label) == np.argmax(true_label))\n",
        "\n",
        "        # Back propagation\n",
        "        layer_2_delta = (target - prediction) / (batch_size * layer_2.shape[0])\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
        "        layer_1_delta *= dropout_mask\n",
        "\n",
        "        # Update the weights.\n",
        "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "    # Every 10 reviews\n",
        "    if (j % 10 == 0 or j == epochs - 1):\n",
        "        progress = j/float(len(text))\n",
        "        accuracy = correct_cnt / float(len(labels))\n",
        "        print(f\"Epoch: {j} \\t Progress: {progress:.4f} \\t accuracy: {accuracy:.4f} \\t similar: {similar('terrible')}\")\n",
        "\n",
        "print(similar('terrible'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNx5vtIidU7b",
        "outputId": "72267f08-7d05-4e40-a04f-b29c32f7ae97"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \t Progress: 0.0000 \t accuracy: 0.5370 \t similar: [('terrible', -0.0), ('devil', -0.14585044334391642), ('bewitched', -0.14750447714964798), ('excels', -0.14787151798216422), ('wafer', -0.14843285634264014), ('specified', -0.14878658646887943), ('playboy', -0.14919392139629584), ('proved', -0.14930569558805984), ('standing', -0.14955506388014483), ('antagonists', -0.14991448569655494)]\n",
            "Epoch: 10 \t Progress: 0.0100 \t accuracy: 0.8930 \t similar: [('terrible', -0.0), ('specified', -0.1496393894006238), ('devil', -0.14971996478357716), ('excels', -0.15045022465034433), ('standing', -0.151404078416331), ('wafer', -0.15174087615117907), ('bewitched', -0.151881552634404), ('proved', -0.15213647413522108), ('firebird', -0.15216852759098515), ('warburton', -0.1523734015192894)]\n",
            "Epoch: 20 \t Progress: 0.0200 \t accuracy: 0.9220 \t similar: [('terrible', -0.0), ('specified', -0.15273528455672525), ('excels', -0.15465105932684167), ('there', -0.15488043921083336), ('warburton', -0.15539855398987817), ('standing', -0.15554382561793603), ('firebird', -0.1555728108628993), ('plot', -0.15563111982452382), ('devil', -0.15563992886220449), ('narrative', -0.15606397309179168)]\n",
            "Epoch: 30 \t Progress: 0.0300 \t accuracy: 0.9430 \t similar: [('terrible', -0.0), ('plot', -0.15560366773888), ('specified', -0.1567488653545554), ('there', -0.15727110517429804), ('disappointed', -0.1575943149513159), ('budget', -0.15811432253752297), ('act', -0.15822390765981154), ('entire', -0.15856110357439374), ('make', -0.15868246869981031), ('self', -0.15887087303229283)]\n",
            "Epoch: 40 \t Progress: 0.0400 \t accuracy: 0.9690 \t similar: [('terrible', -0.0), ('plot', -0.15538208940858342), ('disappointed', -0.15784034583177364), ('make', -0.1581934830074386), ('budget', -0.15875462555735886), ('self', -0.15923064950642024), ('act', -0.1595676071988218), ('entire', -0.15960143286923675), ('can', -0.16036828933685374), ('so', -0.1603832110312965)]\n",
            "Epoch: 50 \t Progress: 0.0500 \t accuracy: 0.9860 \t similar: [('terrible', -0.0), ('plot', -0.15496512445617286), ('make', -0.15736220714599944), ('disappointed', -0.1578897527627297), ('budget', -0.1592289061097407), ('self', -0.15946709147210608), ('so', -0.1597867919592065), ('entire', -0.16059249647757584), ('act', -0.16096488686918659), ('crap', -0.16129052284984685)]\n",
            "Epoch: 60 \t Progress: 0.0600 \t accuracy: 0.9890 \t similar: [('terrible', -0.0), ('plot', -0.15476478240735284), ('make', -0.15658983622133735), ('disappointed', -0.15795897088205807), ('self', -0.159726078022447), ('so', -0.15979841920407126), ('budget', -0.15985595939137565), ('crap', -0.1616001718302047), ('entire', -0.16176132226255427), ('even', -0.16201234082484428)]\n",
            "Epoch: 70 \t Progress: 0.0700 \t accuracy: 0.9950 \t similar: [('terrible', -0.0), ('plot', -0.154640412273907), ('make', -0.15617775551662944), ('disappointed', -0.15795843335786125), ('self', -0.16006387165966643), ('so', -0.1601216590094467), ('budget', -0.16046476447461375), ('crap', -0.16198494395588875), ('now', -0.1621494071588408), ('even', -0.16230777814492348)]\n",
            "Epoch: 80 \t Progress: 0.0800 \t accuracy: 0.9970 \t similar: [('terrible', -0.0), ('plot', -0.15478773103792154), ('make', -0.15604075532389114), ('disappointed', -0.1579542311026475), ('self', -0.16042950890349364), ('so', -0.16091849066623395), ('budget', -0.16101649597444775), ('now', -0.16222631741364768), ('take', -0.16236726902909793), ('crap', -0.16244579044279664)]\n",
            "Epoch: 90 \t Progress: 0.0900 \t accuracy: 0.9980 \t similar: [('terrible', -0.0), ('plot', -0.15512290400809695), ('make', -0.1562475697177887), ('disappointed', -0.1580713763965971), ('self', -0.1608983430492718), ('budget', -0.16167582194285435), ('so', -0.16210138895508144), ('now', -0.1623343256476683), ('take', -0.16247393287402798), ('crap', -0.1630058332618199)]\n",
            "Epoch: 99 \t Progress: 0.0990 \t accuracy: 0.9990 \t similar: [('terrible', -0.0), ('plot', -0.15540194962004975), ('make', -0.1566907489223858), ('disappointed', -0.15809758504285198), ('self', -0.1612615837121043), ('budget', -0.16219189104618456), ('now', -0.16240912577840397), ('take', -0.16260921732604106), ('lame', -0.16319441189365025), ('so', -0.1633443968869364)]\n",
            "[('terrible', -0.0), ('plot', -0.15540194962004975), ('make', -0.1566907489223858), ('disappointed', -0.15809758504285198), ('self', -0.1612615837121043), ('budget', -0.16219189104618456), ('now', -0.16240912577840397), ('take', -0.16260921732604106), ('lame', -0.16319441189365025), ('so', -0.1633443968869364)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(positive=[\"terrible\", \"good\"], negative = [\"bad\"]):\n",
        "    norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
        "    norms.resize(norms.shape[0], 1)\n",
        "\n",
        "    normed_weights = weights_0_1 * norms\n",
        "\n",
        "    query_vector = np.zeros(len(weights_0_1[0]))\n",
        "    for word in positive:\n",
        "        query_vector += normed_weights[word2index[word]]\n",
        "    for word in negative:\n",
        "        query_vector -= normed_weights[word2index[word]]\n",
        "\n",
        "    scores = Counter()\n",
        "    for word, index in word2index.items():\n",
        "        raw_difference = weights_0_1[index] - query_vector\n",
        "        squared_difference = raw_difference * raw_difference\n",
        "        scores[word] = -math.sqrt(sum(squared_difference))\n",
        "\n",
        "    # Ignore the first word, as it will be the positive query.\n",
        "    return scores.most_common(10)[1:]\n",
        "\n",
        "print(f'terrible, good (+) bad (-): {analogy([\"terrible\", \"good\"], [\"bad\"])}')\n",
        "print(f'elizabeth, he (+) she (-): {analogy([\"elizabeth\", \"he\"], [\"she\"])}')\n",
        "print(f'king, woman (+) man (-): {analogy([\"king\", \"woman\"], [\"man\"])}')\n",
        "print(f'elizabeth, he (+) she (-): {analogy([\"elizabeth\", \"he\"], [\"she\"])}')\n",
        "print(f'man, woman (+) king (-): {analogy([\"man\", \"woman\"], [\"king\"])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2JA4M0htyie",
        "outputId": "8f54a04e-70b5-4c11-adf5-ce95894f02f9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terrible, good (+) bad (-): [('wizards', -0.10692333205332843), ('admission', -0.1070622760446349), ('subtitles', -0.1077462892312441), ('underneath', -0.10780070899988253), ('reminds', -0.1078994709849344), ('cronies', -0.1080483996658244), ('fudoh', -0.10843258757640356), ('terrifying', -0.10852476507161836), ('ultraviolent', -0.10858590871986315)]\n",
            "elizabeth, he (+) she (-): [('eurail', -0.10633683960172827), ('survivial', -0.10658863713010631), ('wry', -0.10680218224664625), ('losses', -0.10694106329941419), ('smashes', -0.10701497643476447), ('wizards', -0.10709894589216934), ('topping', -0.10716170296396138), ('problems', -0.10720804063097321), ('tsuyako', -0.10721238916544781)]\n",
            "king, woman (+) man (-): [('eurail', -0.10639393035597863), ('survivial', -0.10650613768863797), ('smashes', -0.10669524182794579), ('wry', -0.10679103677355847), ('problems', -0.10691569328894408), ('losses', -0.10696007879276626), ('bigger', -0.10716130144318609), ('noise', -0.1071750717249476), ('exiting', -0.10723333240923617)]\n",
            "elizabeth, he (+) she (-): [('eurail', -0.10633683960172827), ('survivial', -0.10658863713010631), ('wry', -0.10680218224664625), ('losses', -0.10694106329941419), ('smashes', -0.10701497643476447), ('wizards', -0.10709894589216934), ('topping', -0.10716170296396138), ('problems', -0.10720804063097321), ('tsuyako', -0.10721238916544781)]\n",
            "man, woman (+) king (-): [('ultraviolent', -0.10642979478365845), ('eurail', -0.10653898004896209), ('wry', -0.10681496173020877), ('elaborate', -0.10709014094511075), ('noise', -0.10709540020863563), ('wizards', -0.10715732633804571), ('losses', -0.10718532152004655), ('smashes', -0.1071882461899018), ('niven', -0.10721553665606313)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
        "norms.resize(norms.shape[0],1)\n",
        "normed_weights = weights_0_1 * norms\n",
        "\n",
        "def make_sentence_vector(words):\n",
        "    indices = list(map(lambda x: word2index[x],\\\n",
        "                       filter(lambda x: x in word2index, words)))\n",
        "    return np.mean(normed_weights[indices], axis=0)\n",
        "\n",
        "reviews2vectors = list()\n",
        "for review in tokens:\n",
        "    sentence_vector = make_sentence_vector(review)\n",
        "    reviews2vectors.append(sentence_vector)\n",
        "reviews2vectors = np.array(reviews2vectors)\n",
        "\n",
        "def most_similar_reviews(review):\n",
        "    v = make_sentence_vector(review)\n",
        "    scores = Counter()\n",
        "    for i, val in enumerate(reviews2vectors.dot(v)):\n",
        "        scores[i] = val\n",
        "    most_similar = list()\n",
        "\n",
        "    for idx, score in scores.most_common(3):\n",
        "        most_similar.append(raw_reviews[idx][0:40])\n",
        "    return most_similar\n",
        "\n",
        "most_similar_reviews([\"boring\",\"awful\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb1ZlAO2daDb",
        "outputId": "508831c4-baea-44e7-a418-4091a5f1d128"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this piece ain  t really worth a comment',\n",
              " 'just a few words . . . . this movie real',\n",
              " 'this movie was terrible  i rented it not']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1,2,3])\n",
        "b = np.array([0.1,0.2,0.3])\n",
        "c = np.array([-1,-0.5,0])\n",
        "d = np.array([0,0,0])\n",
        "\n",
        "identity = np.eye(3)\n",
        "print(identity)\n",
        "\n",
        "print(a.dot(identity))\n",
        "print(b.dot(identity))\n",
        "print(c.dot(identity))\n",
        "print(d.dot(identity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUgho5xsxXx9",
        "outputId": "da113cac-c642-4a88-e991-0ebb8d3d3959"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "[1. 2. 3.]\n",
            "[0.1 0.2 0.3]\n",
            "[-1.  -0.5  0. ]\n",
            "[0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "this = np.array([2,4,6])\n",
        "movie = np.array([10,10,10])\n",
        "rocks = np.array([1,1,1])\n",
        "\n",
        "print(this + movie + rocks)\n",
        "print((this.dot(identity) + movie).dot(identity) + rocks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgh-qU0Ox4D5",
        "outputId": "65d80d4e-e196-4a40-9ad4-84e2c5e65a48"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13 15 17]\n",
            "[13. 15. 17.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Freeze the seed for reprodicability.\n",
        "np.random.seed(1)\n",
        "\n",
        "# Activation function\n",
        "def softmax(x_):\n",
        "  x = np.atleast_2d(x_)\n",
        "  temp = np.exp(x)\n",
        "  return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "# Initialize the network\n",
        "word_vectors = {}\n",
        "word_vectors['yankees'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['bears'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['braves'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['red'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['sox'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['lose'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['defeat'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['beat'] = np.array([[0.,0.,0.]])\n",
        "word_vectors['tie'] = np.array([[0.,0.,0.]])\n",
        "sentence2output = np.random.rand(3, len(word_vectors))\n",
        "identity = np.eye(3)\n",
        "\n",
        "# Hyperparmeters\n",
        "epochs = 100\n",
        "alpha = 0.01\n",
        "\n",
        "for _ in range(epochs):\n",
        "    # Foward pass\n",
        "    layer_0 = word_vectors['red']\n",
        "    layer_1 = layer_0.dot(identity) + word_vectors['sox']\n",
        "    layer_2 = layer_1.dot(identity) + word_vectors['defeat']\n",
        "\n",
        "    pred = softmax(layer_2.dot(sentence2output))\n",
        "    # print(f\"pred: {pred}\")\n",
        "\n",
        "    # Back propagation\n",
        "    y = np.array([1,0,0,0,0,0,0,0,0]) # One hot vector for Yankees\n",
        "\n",
        "    pred_delta = pred - y\n",
        "    layer_2_delta = pred_delta.dot(sentence2output.T)\n",
        "    defeat_delta =  layer_2_delta * 1\n",
        "    layer_1_delta = layer_2_delta.dot(identity.T)\n",
        "    sox_delta = layer_1_delta * 1\n",
        "    layer_0_delta = layer_1_delta.dot(identity.T)\n",
        "    alpha = 0.01\n",
        "\n",
        "    word_vectors['red'] -= layer_0_delta * alpha\n",
        "    word_vectors['sox'] -= sox_delta * alpha\n",
        "    word_vectors['defeat'] -= defeat_delta * alpha\n",
        "\n",
        "    identity -= np.outer(layer_0, layer_1_delta) * alpha\n",
        "    identity -= np.outer(layer_1, layer_2_delta) * alpha\n",
        "    sentence2output -= np.outer(layer_2, pred_delta) * alpha\n",
        "\n",
        "# Foward pass\n",
        "layer_0 = word_vectors['red']\n",
        "layer_1 = layer_0.dot(identity) + word_vectors['sox']\n",
        "layer_2 = layer_1.dot(identity) + word_vectors['defeat']\n",
        "pred = softmax(layer_2.dot(sentence2output))\n",
        "\n",
        "assert np.argmax(pred) == np.argmax(y), \"Prediction does not match target\""
      ],
      "metadata": {
        "id": "NFxGNvrszsbf"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset"
      ],
      "metadata": {
        "id": "bJ1JrvVG4RHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n",
        "! tar -xvf tasks_1-20_v1-2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kewsl-Iq4HQj",
        "outputId": "a4753f81-5789-4265-8d44-dc1d477e6142"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-24 10:14:08--  https://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n",
            "Resolving www.thespermwhale.com (www.thespermwhale.com)... 50.31.160.191\n",
            "Connecting to www.thespermwhale.com (www.thespermwhale.com)|50.31.160.191|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15719851 (15M) [application/x-gzip]\n",
            "Saving to: ‘tasks_1-20_v1-2.tar.gz’\n",
            "\n",
            "tasks_1-20_v1-2.tar 100%[===================>]  14.99M  9.57MB/s    in 1.6s    \n",
            "\n",
            "2024-08-24 10:14:10 (9.57 MB/s) - ‘tasks_1-20_v1-2.tar.gz’ saved [15719851/15719851]\n",
            "\n",
            "tasks_1-20_v1-2/\n",
            "tasks_1-20_v1-2/hn/\n",
            "tasks_1-20_v1-2/hn/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/hn/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/hn/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/hn/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/hn/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/hn/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/hn/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/hn/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/hn/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/hn/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/hn/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/hn/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/hn/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/hn/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/hn/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/hn/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/hn/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/hn/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/hn/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/hn/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/hn/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/hn/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/hn/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/hn/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled/\n",
            "tasks_1-20_v1-2/shuffled/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-10k/\n",
            "tasks_1-20_v1-2/en-10k/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-valid/\n",
            "tasks_1-20_v1-2/en-valid/qa8_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa11_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa7_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa13_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa7_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa19_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa12_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa18_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa6_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa9_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa8_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa2_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa12_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa11_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa9_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa1_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa7_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa16_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa4_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa2_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa5_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa16_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa18_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa13_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa11_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa1_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa5_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa15_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa20_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa18_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa19_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa9_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa17_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa15_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa5_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa20_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa14_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa4_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa15_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa10_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa8_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa6_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa17_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa10_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa3_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa3_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa16_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa3_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa14_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa19_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa4_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa1_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa2_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa13_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa20_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa6_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa14_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa12_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa17_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa10_valid.txt\n",
            "tasks_1-20_v1-2/hn-10k/\n",
            "tasks_1-20_v1-2/hn-10k/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/\n",
            "tasks_1-20_v1-2/shuffled-10k/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/\n",
            "tasks_1-20_v1-2/en-valid-10k/qa8_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa11_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa7_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa13_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa7_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa19_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa12_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa18_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa6_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa9_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa8_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa2_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa12_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa11_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa9_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa1_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa7_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa16_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa4_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa2_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa5_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa16_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa18_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa13_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa11_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa1_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa5_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa15_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa20_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa18_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa19_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa9_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa17_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa15_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa5_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa20_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa14_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa4_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa15_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa10_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa8_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa6_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa17_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa10_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa3_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa3_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa16_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa3_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa14_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa19_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa4_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa1_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa2_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa13_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa20_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa6_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa14_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa12_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa17_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa10_valid.txt\n",
            "tasks_1-20_v1-2/en/\n",
            "tasks_1-20_v1-2/en/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/en/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/en/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/en/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/en/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/en/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/en/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/en/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/en/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/en/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/en/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/en/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/en/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/en/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/en/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/en/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/en/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/en/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/en/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/en/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/en/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/en/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/en/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/en/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/en/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/en/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/LICENSE.txt\n",
            "tasks_1-20_v1-2/README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "f = open('tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt','r')\n",
        "raw = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list()\n",
        "for line in raw[0:1000]:\n",
        "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
        "\n",
        "print(tokens[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuvOtKme4Yb3",
        "outputId": "c8c33b87-4da7-4150-9bdf-54ab4693ca61"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset.\n",
        "vocab = set()\n",
        "for sentence in tokens:\n",
        "    for word in sentence:\n",
        "        vocab.add(word)\n",
        "\n",
        "vocab = list(vocab)\n",
        "\n",
        "word2index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "    word2index[word] = i\n",
        "\n",
        "def words2indices(sentence):\n",
        "    idx = list()\n",
        "    for word in sentence:\n",
        "        idx.append(word2index[word])\n",
        "    return idx\n",
        "\n",
        "# Activation function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "# Freeze the seed for reproducability.\n",
        "np.random.seed(1)\n",
        "embed_size = 10\n",
        "\n",
        "# word embeddings\n",
        "embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1\n",
        "\n",
        "# embedding -> embedding (initially the identity matrix)\n",
        "recurrent = np.eye(embed_size)\n",
        "\n",
        "# sentence embedding for empty sentence\n",
        "start = np.zeros(embed_size)\n",
        "\n",
        "# embedding -> output weights\n",
        "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
        "\n",
        "# one hot lookups (for loss function)\n",
        "one_hot = np.eye(len(vocab))\n",
        "\n",
        "# Forwawrd oass\n",
        "def predict(sentence):\n",
        "\n",
        "    layers = list()\n",
        "    layer = {}\n",
        "    layer['hidden'] = start\n",
        "    layers.append(layer)\n",
        "\n",
        "    loss = 0\n",
        "    # forward propagate\n",
        "    preds = list()\n",
        "    for target_i in range(len(sentence)):\n",
        "        layer = {}\n",
        "        # try to predict the next term\n",
        "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
        "        loss += -np.log(layer['pred'][sentence[target_i]])\n",
        "        # generate the next hidden state\n",
        "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sentence[target_i]]\n",
        "        layers.append(layer)\n",
        "\n",
        "    return layers, loss\n",
        "\n",
        "# Training loop\n",
        "for j in range(epochs):\n",
        "    alpha = 0.001\n",
        "    sentence = words2indices(tokens[j % len(tokens)][1:])\n",
        "    layers,loss = predict(sentence)\n",
        "\n",
        "    # Forward pass\n",
        "    for layer_idx in reversed(range(len(layers))):\n",
        "        layer = layers[layer_idx]\n",
        "        target = sentence[layer_idx-1]\n",
        "\n",
        "        if(layer_idx > 0):  # if not the first layer\n",
        "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
        "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
        "\n",
        "            # if the last layer - don't pull from a later one becasue it doesn't exist\n",
        "            if(layer_idx == len(layers)-1):\n",
        "                layer['hidden_delta'] = new_hidden_delta\n",
        "            else:\n",
        "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
        "        else: # if the first layer\n",
        "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
        "\n",
        "    # Back progpation\n",
        "    start -= layers[0]['hidden_delta'] * alpha / float(len(sentence))\n",
        "    for layer_idx, layer in enumerate(layers[1:]):\n",
        "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sentence))\n",
        "\n",
        "        embed_idx = sentence[layer_idx]\n",
        "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sentence))\n",
        "\n",
        "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sentence))\n",
        "\n",
        "    if (j % 1000 == 0 or j == epochs - 1):\n",
        "        perplexity = np.exp(loss / len(sentence))\n",
        "        print(f\"Epoch: {j} \\t Perplexitty: {perplexity}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9pz3E5D5Iy5",
        "outputId": "37fd6794-ec87-42ab-fb79-66675702474e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \t Perplexitty: 82.1100559632168\n",
            "Epoch: 1000 \t Perplexitty: 81.99090217554809\n",
            "Epoch: 2000 \t Perplexitty: 81.85525912134636\n",
            "Epoch: 3000 \t Perplexitty: 81.63645524243188\n",
            "Epoch: 4000 \t Perplexitty: 81.21583375974387\n",
            "Epoch: 5000 \t Perplexitty: 80.33159335718055\n",
            "Epoch: 6000 \t Perplexitty: 78.27503632847272\n",
            "Epoch: 7000 \t Perplexitty: 72.33614108276905\n",
            "Epoch: 8000 \t Perplexitty: 45.30273062756402\n",
            "Epoch: 9000 \t Perplexitty: 24.562240422415016\n",
            "Epoch: 10000 \t Perplexitty: 19.791584022300416\n",
            "Epoch: 11000 \t Perplexitty: 18.278806376933236\n",
            "Epoch: 12000 \t Perplexitty: 16.59387367652661\n",
            "Epoch: 13000 \t Perplexitty: 14.088323498980094\n",
            "Epoch: 14000 \t Perplexitty: 10.822503000255418\n",
            "Epoch: 15000 \t Perplexitty: 8.202594684975473\n",
            "Epoch: 16000 \t Perplexitty: 6.855140486994354\n",
            "Epoch: 17000 \t Perplexitty: 6.020045586130928\n",
            "Epoch: 18000 \t Perplexitty: 5.430417275951099\n",
            "Epoch: 19000 \t Perplexitty: 5.040622868602438\n",
            "Epoch: 20000 \t Perplexitty: 4.8132115025067765\n",
            "Epoch: 21000 \t Perplexitty: 4.671143945581807\n",
            "Epoch: 22000 \t Perplexitty: 4.582442989085527\n",
            "Epoch: 23000 \t Perplexitty: 4.52354698086239\n",
            "Epoch: 24000 \t Perplexitty: 4.465862822956182\n",
            "Epoch: 25000 \t Perplexitty: 4.392631981353176\n",
            "Epoch: 26000 \t Perplexitty: 4.301750842560685\n",
            "Epoch: 27000 \t Perplexitty: 4.197248864833723\n",
            "Epoch: 28000 \t Perplexitty: 4.0842789472699925\n",
            "Epoch: 29000 \t Perplexitty: 3.9734591366247627\n",
            "Epoch: 29999 \t Perplexitty: 4.515723588143858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_index = 4\n",
        "l,_ = predict(words2indices(tokens[sent_index]))\n",
        "\n",
        "print(tokens[sent_index])\n",
        "\n",
        "for i,each_layer in enumerate(l[1:-1]):\n",
        "    input = tokens[sent_index][i]\n",
        "    true = tokens[sent_index][i+1]\n",
        "    pred = vocab[each_layer['pred'].argmax()]\n",
        "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\n",
        "          \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2nryUbHDBIi",
        "outputId": "4eb3a80c-0cdf-4a3e-f335-78227352a0a1"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sandra', 'moved', 'to', 'the', 'garden.']\n",
            "Prev Input:sandra      True:moved          Pred:is\n",
            "Prev Input:moved       True:to             Pred:to\n",
            "Prev Input:to          True:the            Pred:the\n",
            "Prev Input:the         True:garden.        Pred:bedroom.\n"
          ]
        }
      ]
    }
  ]
}