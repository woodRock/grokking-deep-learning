{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUe+P3R1MkRTdPI6K6ogoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/grokking-deep-learning/blob/main/chapter_11_neural_networks_that_understand_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 11 | Neural networks that understand language"
      ],
      "metadata": {
        "id": "jxcc1tw__40n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words"
      ],
      "metadata": {
        "id": "O4m2DlaB0OUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "onehots = {}\n",
        "onehots['cat'] = np.array([1,0,0,0])\n",
        "onehots['the'] = np.array([0,1,0,0])\n",
        "onehots['dog'] = np.array([0,0,1,0])\n",
        "onehots['sat'] = np.array([0,0,0,1])\n",
        "\n",
        "sentence = ['the', 'cat', 'sat']\n",
        "x = onehots[sentence[0]] + onehots[sentence[1]] + onehots[sentence[2]]\n",
        "print(f\"Sentence encoding: {x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z75dV8GuBzhX",
        "outputId": "f15b786b-8eac-42eb-deb0-47a3c9c227f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence encoding: [1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "b3iwKVoL0Pwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the reviews.txt file\n",
        "f = open('reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close\n",
        "\n",
        "f = open('labels.txt')\n",
        "raw_labels = f.readlines()\n",
        "f.close\n",
        "\n",
        "# Tokenize the reviews\n",
        "tokens = list(map(lambda x: set(x.split(\" \")), raw_reviews))\n",
        "\n",
        "vocab = set()\n",
        "for sentence in tokens:\n",
        "    for word in sentence:\n",
        "        if (len(word) > 0):\n",
        "            vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "word2index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "    word2index[word] = i\n",
        "\n",
        "input_dataset = list()\n",
        "for sentence in tokens:\n",
        "    sentence_indices = list()\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sentence_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(list(set(sentence_indices)))\n",
        "\n",
        "target_dataset = list()\n",
        "for label in raw_labels:\n",
        "    if label == 'positive\\n':\n",
        "        target_dataset.append(1)\n",
        "    else:\n",
        "        target_dataset.append(0)\n",
        "\n",
        "size = 1_000\n",
        "test_input_dataset = input_dataset[size:size+size]\n",
        "test_target_dataset = target_dataset[size:size+size]\n",
        "input_dataset = input_dataset[:size]\n",
        "target_dataset = target_dataset[:size]"
      ],
      "metadata": {
        "id": "AX1Mu-UFCthf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the class labels to onehot encoding\n",
        "output_dim = 2\n",
        "one_hot_labels = np.zeros((len(target_dataset),output_dim), dtype=np.float32)\n",
        "for i,l in enumerate(target_dataset):\n",
        "    one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "labels[:10]\n",
        "\n",
        "# Test dataset\n",
        "output_dim = 2\n",
        "one_hot_labels = np.zeros((len(test_target_dataset),output_dim), dtype=np.float32)\n",
        "for i,l in enumerate(target_dataset):\n",
        "    one_hot_labels[i][l] = 1\n",
        "test_labels = one_hot_labels\n",
        "test_labels[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUMpfje-Elye",
        "outputId": "a292edb9-de58-4dd2-89b0-2277a4f740ae"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the class labels to onehot encoding\n",
        "vocab_size = len(vocab)\n",
        "one_hot_labels = np.zeros((len(input_dataset),vocab_size), dtype=np.float32)\n",
        "for i, sentence in enumerate(input_dataset):\n",
        "    for word_idx in sentence:\n",
        "        one_hot_labels[i][word_idx] = 1\n",
        "text = one_hot_labels\n",
        "text[:10]\n",
        "\n",
        "# Test dataset\n",
        "one_hot_labels = np.zeros((len(test_input_dataset),vocab_size), dtype=np.float32)\n",
        "for i, sentence in enumerate(test_input_dataset):\n",
        "    for word_idx in sentence:\n",
        "        one_hot_labels[i][word_idx] = 1\n",
        "test_text = one_hot_labels\n",
        "test_text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VMF7FMmKGTU",
        "outputId": "b4827c5b-b90c-425e-ebd4-2238089947bc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "    return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "    temp = np.exp(x)\n",
        "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 2\n",
        "iterations = 100\n",
        "hidden_size = 100\n",
        "input_dim = len(vocab)\n",
        "output_dim = 2\n",
        "batch_size = 100\n",
        "\n",
        "# Initialize the neural network.\n",
        "weights_0_1 = 0.02 * np.random.random((input_dim, hidden_size)) - 0.01\n",
        "weights_1_2 = 0.2 * np.random.random((hidden_size, output_dim)) - 0.1\n",
        "\n",
        "# Training loop\n",
        "for j in range(iterations):\n",
        "    correct_cnt = 0\n",
        "    for i in range(int(len(text) / batch_size)):\n",
        "        batch_start, batch_end = ((i * batch_size), ((i+1)*batch_size))\n",
        "        input, target = text[batch_start:batch_end], labels[batch_start:batch_end]\n",
        "\n",
        "        # Foward pass\n",
        "        layer_0 = input\n",
        "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
        "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
        "        layer_1 *= dropout_mask * 2\n",
        "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
        "        prediction = layer_2\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            pred_label = prediction[k:k+1]\n",
        "            true_label = labels[batch_start+k:batch_start+k+1]\n",
        "            correct_cnt += int(np.argmax(pred_label) == np.argmax(true_label))\n",
        "\n",
        "        # Back propagation\n",
        "        layer_2_delta = (target - prediction) / (batch_size * prediction.shape[0])\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)* tanh2deriv(layer_1)\n",
        "        layer_1_delta *= dropout_mask\n",
        "\n",
        "        # Update the weights\n",
        "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "    if (j % 10 == 0 or j == iterations - 1):\n",
        "\n",
        "        # Evaluate on the test set.\n",
        "        test_correct_cnt = 0\n",
        "\n",
        "        for i in range(len(test_text)):\n",
        "            input, target = test_text[i:i+1], test_labels[i:i+1]\n",
        "            # Foward pass\n",
        "            layer_0 = input\n",
        "            layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
        "            layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
        "            prediction = layer_2\n",
        "\n",
        "            test_correct_cnt += int(np.argmax(prediction) == np.argmax(target))\n",
        "\n",
        "        print(f\"I: {j}\\tTraining accuracy: {correct_cnt/float(len(labels))} Test accuracy: {test_correct_cnt / float(len(test_labels))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lI0hI-3LbYw",
        "outputId": "5bf9c566-ebdc-4dea-ef9a-7bf98625b8e4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I: 0\tTraining accuracy: 0.494 Test accuracy: 0.509\n",
            "I: 10\tTraining accuracy: 0.78 Test accuracy: 0.502\n",
            "I: 20\tTraining accuracy: 0.847 Test accuracy: 0.486\n",
            "I: 30\tTraining accuracy: 0.874 Test accuracy: 0.48\n",
            "I: 40\tTraining accuracy: 0.904 Test accuracy: 0.489\n",
            "I: 50\tTraining accuracy: 0.926 Test accuracy: 0.493\n",
            "I: 60\tTraining accuracy: 0.95 Test accuracy: 0.498\n",
            "I: 70\tTraining accuracy: 0.966 Test accuracy: 0.505\n",
            "I: 80\tTraining accuracy: 0.978 Test accuracy: 0.508\n",
            "I: 90\tTraining accuracy: 0.987 Test accuracy: 0.51\n",
            "I: 99\tTraining accuracy: 0.991 Test accuracy: 0.506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to embedding layer"
      ],
      "metadata": {
        "id": "epEM8kcu0I-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.05\n",
        "iterations = 10\n",
        "hidden_size = 100\n",
        "input_dim = len(vocab)  # Assuming vocab is defined elsewhere\n",
        "output_dim = 2\n",
        "batch_size = 10\n",
        "\n",
        "# Initialize the neural network\n",
        "weights_0_1 = 0.2 * np.random.random((input_dim, hidden_size)) - 0.1\n",
        "weights_1_2 = 0.2 * np.random.random((hidden_size, output_dim)) - 0.1\n",
        "\n",
        "def similar(target='beautiful'):\n",
        "    target_index = word2index[target]\n",
        "    scores = Counter()\n",
        "    for word, index in word2index.items():\n",
        "        raw_difference = weights_0_1[index] - weights_0_1[target_index]\n",
        "        squared_difference = raw_difference * raw_difference\n",
        "        scores[word] = -math.sqrt(np.sum(squared_difference))\n",
        "    return scores.most_common(10)\n",
        "\n",
        "# Activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "def evaluate(X, y):\n",
        "    correct_cnt = 0\n",
        "    for i in range(len(X)):\n",
        "        input_data = np.array([X[i]], dtype=np.int32)\n",
        "        layer_1 = sigmoid(weights_0_1[input_data].sum(axis=1))\n",
        "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
        "        prediction = np.argmax(layer_2)\n",
        "        if prediction == np.argmax(y[i]):\n",
        "            correct_cnt += 1\n",
        "    return correct_cnt / len(X)\n",
        "\n",
        "# Training loop\n",
        "for j in range(iterations):\n",
        "    correct_cnt = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_end = min(i + batch_size, len(X_train))\n",
        "        input_batch, target = X_train[i:batch_end], y_train[i:batch_end]\n",
        "\n",
        "        # Convert input to an integer array.\n",
        "        input_batch = np.array(input_batch, dtype=np.int32)\n",
        "\n",
        "        # Forward pass\n",
        "        layer_0 = weights_0_1[input_batch]\n",
        "        layer_1 = sigmoid(layer_0.sum(axis=1))\n",
        "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
        "        layer_1 *= dropout_mask * 2\n",
        "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
        "\n",
        "        prediction = layer_2\n",
        "\n",
        "        # Count correct predictions\n",
        "        correct_cnt += np.sum(np.argmax(prediction, axis=1) == np.argmax(target, axis=1))\n",
        "\n",
        "        # Back propagation\n",
        "        layer_2_delta = (prediction - target) * sigmoid_derivative(layer_2)\n",
        "        layer_1_delta = np.dot(layer_2_delta, weights_1_2.T) * sigmoid_derivative(layer_1) * dropout_mask\n",
        "\n",
        "        # Update weights_1_2\n",
        "        weights_1_2 -= alpha * np.dot(layer_1.T, layer_2_delta)\n",
        "\n",
        "        # Update weights_0_1 (word embeddings)\n",
        "        for idx, word_idx in enumerate(input_batch):\n",
        "            weights_0_1[word_idx] -= alpha * layer_1_delta[idx]\n",
        "\n",
        "    # Calculate and display accuracies\n",
        "    train_accuracy = correct_cnt / len(X_train)\n",
        "    test_accuracy = evaluate(X_test, y_test)\n",
        "\n",
        "    print(f\"Epoch {j+1}/{iterations}\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.2%}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
        "    print(f\"terrible: {similar('terrible')}\\n\")\n",
        "\n",
        "print(\"Final evaluation:\")\n",
        "print(f\"Training Accuracy: {evaluate(X_train, y_train):.2%}\")\n",
        "print(f\"Test Accuracy: {evaluate(X_test, y_test):.2%}\")\n",
        "print(f\"terrible: {similar('terrible')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "4yBSir0NTNFB",
        "outputId": "f407ecc7-63f3-4185-f0b0-239321d7be49"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-034211dda3b6>:29: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Training Accuracy: 48.00%\n",
            "Test Accuracy: 48.50%\n",
            "terrible: [('terrible', -0.0), ('yb', -0.6271164376013738), ('lyrically', -0.6317240614076646), ('shamble', -0.636646820823081), ('oxy', -0.6372006103711703), ('yvette', -0.6387138586459286), ('obscuring', -0.6390080214657136), ('buckley', -0.6393306246892859), ('disservices', -0.6397892262824619), ('towelheads', -0.6426212275558818)]\n",
            "\n",
            "Epoch 2/10\n",
            "Training Accuracy: 44.88%\n",
            "Test Accuracy: 48.50%\n",
            "terrible: [('terrible', -0.0), ('yb', -0.6271164376013738), ('lyrically', -0.6317240614076646), ('shamble', -0.636646820823081), ('oxy', -0.6372006103711703), ('yvette', -0.6387138586459286), ('obscuring', -0.6390080214657136), ('buckley', -0.6393306246892859), ('disservices', -0.6397892262824619), ('towelheads', -0.6426212275558818)]\n",
            "\n",
            "Epoch 3/10\n",
            "Training Accuracy: 48.25%\n",
            "Test Accuracy: 48.50%\n",
            "terrible: [('terrible', -0.0), ('yb', -0.6271164376013738), ('lyrically', -0.6317240614076646), ('shamble', -0.636646820823081), ('oxy', -0.6372006103711703), ('yvette', -0.6387138586459286), ('obscuring', -0.6390080214657136), ('buckley', -0.6393306246892859), ('disservices', -0.6397892262824619), ('towelheads', -0.6426212275558818)]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-034211dda3b6>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Update weights_0_1 (word embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mweights_0_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_1_delta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Calculate and display accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def similar(target='beautiful'):\n",
        "    target_index = word2index[target]\n",
        "    scores = Counter()\n",
        "    for word, index in word2index.items():\n",
        "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
        "        squared_difference = raw_difference * raw_difference\n",
        "        scores[word] =  -math.sqrt(sum(squared_difference))\n",
        "\n",
        "    return scores.most_common(10)\n",
        "\n",
        "print(f\"Words similar to beautiful:\\n{similar('beautiful')}\")\n",
        "print(f\"Words similar to terrible:\\n{similar('terrible')}\")\n",
        "print(f\"Words similar to good:\\n{similar('good')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A6mVFYKl667",
        "outputId": "e0632032-d5fd-4da4-c141-46bfcabcc1c5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to beautiful:\n",
            "[('beautiful', -0.0), ('his', -0.07071780387973033), ('fine', -0.07366165803828469), ('god', -0.07371790999687042), ('ever', -0.07402884728794429), ('perhaps', -0.07475378824368628), ('mostly', -0.0749826641492825), ('come', -0.07519590679756649), ('more', -0.07530645237090662), ('carrey', -0.07563617098476084)]\n",
            "Words similar to terrible:\n",
            "[('terrible', -0.0), ('contortion', -0.0652114763026173), ('cortney', -0.06565973471827609), ('rediscoveries', -0.06580153703393334), ('realised', -0.06693199209336141), ('rudest', -0.066963514140267), ('lan', -0.06698015279419733), ('svendsen', -0.06698044295236112), ('charm', -0.06705438246572451), ('insomniacs', -0.06707230338230676)]\n",
            "Words similar to good:\n",
            "[('good', -0.0), ('message', -0.06830673830453966), ('ron', -0.06992446680197126), ('fi', -0.07092333385227206), ('taylor', -0.07115919248803718), ('seasons', -0.07256386495132805), ('son', -0.07257995515395603), ('accident', -0.07267769971425227), ('then', -0.07289819633904321), ('most', -0.07308680442932163)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filling in the blank"
      ],
      "metadata": {
        "id": "u-Kv9L1o0EUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Freeze the random seed for reproducability.\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "f = open(\"reviews.txt\")\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list(map(lambda x: (x.split(\" \")), raw_reviews))\n",
        "word_count = Counter()\n",
        "for sentence in tokens:\n",
        "    for word in sentence:\n",
        "        word_count[word] -= 1\n",
        "vocab = list(set(map(lambda x:x[0], word_count.most_common())))\n",
        "\n",
        "word2index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "  word2index[word] = i\n",
        "\n",
        "concatenated = list()\n",
        "input_dataset = list()\n",
        "for sentence in tokens:\n",
        "    sentence_indices = list()\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sentence_indices.append(word2index[word])\n",
        "            concatenated.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(sentence_indices)\n",
        "concatenated = np.array(concatenated)\n",
        "random.shuffle(input_dataset)"
      ],
      "metadata": {
        "id": "xlIqsxBqyoo8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "epochs = 10\n",
        "alpha = 0.05,\n",
        "iterations = 2\n",
        "hidden_size = 50\n",
        "window = 2\n",
        "negative = 5\n",
        "\n",
        "# Initialize the neural network\n",
        "weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2\n",
        "weights_1_2 = np.random.rand(len(vocab),hidden_size)*0\n",
        "\n",
        "layer_2_target = np.zeros(negative+1)\n",
        "layer_2_target[0] = 1\n",
        "\n",
        "def similar(target='beautiful'):\n",
        "    target_index = word2index[target]\n",
        "\n",
        "    scores = Counter()\n",
        "    for word,index in word2index.items():\n",
        "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
        "        squared_difference = raw_difference * raw_difference\n",
        "        scores[word] = -math.sqrt(sum(squared_difference))\n",
        "    return scores.most_common(10)\n",
        "\n",
        "# Activation function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "# Training loop\n",
        "for rev_i,review in enumerate(input_dataset * iterations):\n",
        "    for target_i in range(len(review) - 1):\n",
        "        # since it's really expensive to predict every vocabulary\n",
        "        # we're only going to predict a random subset\n",
        "        target_samples = [review[target_i]]+list(concatenated\\\n",
        "        [(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
        "\n",
        "        left_context = review[max(0,target_i-window):target_i]\n",
        "        right_context = review[target_i+1:min(len(review),target_i+window)]\n",
        "\n",
        "        # Forward pass\n",
        "        layer_1 = np.mean(weights_0_1[left_context+right_context],axis=0)\n",
        "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
        "\n",
        "        # Back propagation\n",
        "        layer_2_delta = layer_2 - layer_2_target\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
        "\n",
        "        # Update the weights\n",
        "        weights_0_1[left_context+right_context] -= layer_1_delta * alpha\n",
        "        weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1)*alpha\n",
        "\n",
        "    # Every 250 reviews\n",
        "    if(rev_i % 250 == 0):\n",
        "        progress = rev_i/float(len(input_dataset)*iterations)\n",
        "        print(f\"Progress: {progress} similar: {similar('terrible')}\")\n",
        "\n",
        "print(similar('terrible'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvRBO8De1V0y",
        "outputId": "73dd94cf-0b36-4fb4-8b10-cdd8e9379b79"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 0.0 similar: [('terrible', -0.0), ('awoke', -0.3651169249947063), ('masquerades', -0.39108873398737865), ('cheapo', -0.3945828793589487), ('campers', -0.3953999868256407), ('tie', -0.39564375285482867), ('saaad', -0.4013687810025308), ('montoss', -0.4034894640474157), ('coherant', -0.40581171112337355), ('woulda', -0.40705455528722234)]\n",
            "Progress: 0.125 similar: [('terrible', -0.0), ('herself', -0.43816118022724215), ('vijay', -0.4782528014059888), ('taken', -0.48498883756715705), ('perfectly', -0.4881200559183418), ('street', -0.49215364007755397), ('bobby', -0.49961749701662234), ('clear', -0.49963937187159785), ('named', -0.5057191275149554), ('storyline', -0.5079262287364211)]\n",
            "Progress: 0.25 similar: [('terrible', -0.0), ('taken', -0.47195914291046165), ('basically', -0.47977884871181775), ('street', -0.4864328173243087), ('perfectly', -0.48759282558187605), ('storyline', -0.4881298353554536), ('named', -0.4958455232887505), ('co', -0.4962125663149723), ('hilarious', -0.5002888954891581), ('killing', -0.5031762283510208)]\n",
            "Progress: 0.375 similar: [('terrible', -0.0), ('happy', -0.4884051767177819), ('power', -0.49053734809501337), ('believable', -0.49400517835559166), ('taken', -0.4958484380812333), ('clear', -0.5029311864862729), ('straight', -0.5128421786940429), ('romance', -0.5174626847383748), ('act', -0.5206860267005753), ('rest', -0.5262428569940494)]\n",
            "Progress: 0.5 similar: [('terrible', -0.0), ('taken', -0.4916491640793348), ('war', -0.49804171043956846), ('happy', -0.4983877324011055), ('power', -0.5030725161695512), ('act', -0.5211515880776968), ('straight', -0.525099040326707), ('romance', -0.5277117503407731), ('high', -0.5323196808906683), ('full', -0.5328746982394844)]\n",
            "Progress: 0.625 similar: [('terrible', -0.0), ('taken', -0.4897140873560427), ('happy', -0.49331851698808676), ('power', -0.516943752146244), ('straight', -0.5232022582996451), ('high', -0.5259646864117717), ('war', -0.5313550723246453), ('full', -0.5392084766300294), ('song', -0.5414693360053351), ('piece', -0.5418906351330909)]\n",
            "Progress: 0.75 similar: [('terrible', -0.0), ('taken', -0.48937251950103994), ('happy', -0.4919768535482252), ('war', -0.5033112088709698), ('straight', -0.5126721801667554), ('power', -0.5142311352941139), ('believable', -0.5243088852345007), ('high', -0.5307316651078485), ('romance', -0.5400607012116666), ('admit', -0.5419313123846055)]\n",
            "Progress: 0.875 similar: [('terrible', -0.0), ('taken', -0.4936115676340958), ('happy', -0.49806610504601967), ('war', -0.5055759926763926), ('straight', -0.5184831647952214), ('admit', -0.5342414106973171), ('worse', -0.5405289064900701), ('piece', -0.5411075583063879), ('king', -0.5438597446994214), ('male', -0.5453768841610733)]\n",
            "[('terrible', -0.0), ('happy', -0.4980470784418173), ('taken', -0.5055028840491804), ('straight', -0.5279362305542351), ('soon', -0.54754033618782), ('male', -0.5538737356424002), ('worse', -0.554145936953941), ('earth', -0.5560271310969012), ('admit', -0.558745373223558), ('waste', -0.5606519349586312)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(positive=[\"terrible\", \"good\"], negative = [\"bad\"]):\n",
        "    norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
        "    norms.resize(norms.shape[0], 1)\n",
        "\n",
        "    normed_weights = weights_0_1 * norms\n",
        "\n",
        "    query_vector = np.zeros(len(weights_0_1[0]))\n",
        "    for word in positive:\n",
        "        query_vector += normed_weights[word2index[word]]\n",
        "    for word in negative:\n",
        "        query_vector -= normed_weights[word2index[word]]\n",
        "\n",
        "    scores = Counter()\n",
        "    for word, index in word2index.items():\n",
        "        raw_difference = weights_0_1[index] - query_vector\n",
        "        squared_difference = raw_difference * raw_difference\n",
        "        scores[word] = -math.sqrt(sum(squared_difference))\n",
        "\n",
        "    # Ignore the first word, as it will be the positive query.\n",
        "    return scores.most_common(10)[1:]\n",
        "\n",
        "print(f'terrible, good (+) bad (-): {analogy([\"terrible\", \"good\"], [\"bad\"])}')\n",
        "print(f'elizabeth, he (+) she (-): {analogy([\"elizabeth\", \"he\"], [\"she\"])}')\n",
        "print(f'king, woman (+) man (-): {analogy([\"king\", \"woman\"], [\"man\"])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab_kRjQeFrMq",
        "outputId": "be09f345-f616-4aee-84f4-89ba2da48fa9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terrible, good (+) bad (-): [('have', -390.15019358970096), ('in', -390.8835587470175), ('\\n', -391.1089229220066), ('but', -391.1855549494826), ('.', -391.4678701283952), ('good', -391.5047402353443), ('to', -391.5465135186223), ('by', -391.64278036853506), ('this', -391.77344893810215)]\n",
            "elizabeth, he (+) she (-): [('but', -224.79554527807562), ('to', -225.01071145951823), ('\\n', -225.1433560147473), ('this', -225.167789375941), ('by', -225.4759646703603), ('of', -225.8909616565375), ('a', -225.94601938379344), ('the', -225.94690882701872), ('is', -225.98890858036214)]\n",
            "king, woman (+) man (-): [('undercuts', -124.47271859929674), ('yoko', -124.64956857659786), ('flit', -124.6860944853715), ('nemico', -124.69295870741928), ('palde', -124.69518562145726), ('oogling', -124.69656809780903), ('divali', -124.70321163964871), ('kik', -124.70383134273686), ('temperamental', -124.7039864286581)]\n"
          ]
        }
      ]
    }
  ]
}