{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkJoCATssGLaNA7HS/4hVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/grokking-deep-learning/blob/main/chapter_13_introducing_automatic_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13 | Introducing automatic optimization"
      ],
      "metadata": {
        "id": "MKX_EJJBLxQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2sp-cU_H0nf",
        "outputId": "e1c4520a-dcb1-4942-e631-72f2592738ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4 5]\n",
            "[ 2  4  6  8 10]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = np.array(data)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())\n",
        "\n",
        "x = Tensor([1,2,3,4,5])\n",
        "print(x)\n",
        "\n",
        "y = x + x\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to automatic gradient computation (autograd)"
      ],
      "metadata": {
        "id": "Axb48lfEPTUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "    def __init__(self, data, creators=None, creation_op=None):\n",
        "        self.data = np.array(data)\n",
        "        self.creation_op = creation_op\n",
        "        self.creators = creators\n",
        "        self.grad = None\n",
        "\n",
        "    def backward(self, grad):\n",
        "        self.grad = grad\n",
        "        if (self.creation_op == \"add\"):\n",
        "            self.creators[0].backward(grad)\n",
        "            self.creators[1].backward(grad)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Tensor(self.data + other.data, creators=[self,other], creation_op = \"add\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())\n",
        "\n",
        "\n",
        "x = Tensor([1,2,3,4,5])\n",
        "y = Tensor([2,2,2,2,2])\n",
        "z = x + y\n",
        "z.backward(Tensor(np.array([1,1,1,1,1])))"
      ],
      "metadata": {
        "id": "yiRdFtQeL4qF"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x.grad: {x.grad}\")\n",
        "print(f\"y.grad: {y.grad}\")\n",
        "print(f\"z.creators: {z.creators}\")\n",
        "print(f\"z.creation_op: {z.creation_op}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ntZY66uMFq7",
        "outputId": "d90084f1-fb12-47bb-aa81-fdd1e0d04d67"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad: [1 1 1 1 1]\n",
            "y.grad: [1 1 1 1 1]\n",
            "z.creators: [array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
            "z.creation_op: add\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = Tensor([1,2,3,4,5])\n",
        "b = Tensor([2,2,2,2,2])\n",
        "c = Tensor([5,4,3,2,1])\n",
        "d = Tensor([-1,-2,-3,-4,-5])\n",
        "e = a + b\n",
        "f = c + d\n",
        "g = e + f\n",
        "g.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "print(f\"a.grad: {a.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0eFNr6XMMju",
        "outputId": "dc0fc427-c57e-47e0-bde5-eb98cc101552"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.grad: [1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upgrading autograd to support mutliuse tensors"
      ],
      "metadata": {
        "id": "g1FsTNqmPJdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "\n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "\n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "\n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "\n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        "\n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "\n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "\n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if (self.creators is not None and\n",
        "               (self.all_children_grads_accounted_for() or\n",
        "                grad_origin is None)):\n",
        "\n",
        "                if (self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "\n",
        "                if (self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if (self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)\n",
        "\n",
        "                if (self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "\n",
        "                if (self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if (\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if (\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "                if (self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "\n",
        "                if (self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad* (ones - (self * self)))\n",
        "\n",
        "                if (self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "\n",
        "                if (self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)\n",
        "\n",
        "    def sum(self, dim):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "\n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "\n",
        "        if (self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "\n",
        "    def transpose(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "\n",
        "        return Tensor(self.data.transpose())\n",
        "\n",
        "    def mm(self, x):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())\n",
        "\n",
        "    def sigmoid(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)), autograd=True, creators=[self], creation_op = \"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(np.tanh(self.data), autograd=True, creators=[self], creation_op = \"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "\n",
        "    def index_select(self, indices):\n",
        "        if (self.autograd):\n",
        "            new = Tensor(self.data[indices.data], autograd=True, creators=[self], creation_op = \"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "\n",
        "    def cross_entropy(self, target_indices):\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape)-1,keepdims=True)\n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t), -1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "\n",
        "        if (self.autograd):\n",
        "            out = Tensor(loss, autograd=True, creators=[self], creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "\n",
        "a = Tensor([1,2,3,4,5], autograd=True)\n",
        "b = Tensor([2,2,2,2,2], autograd=True)\n",
        "c = Tensor([5,4,3,2,1], autograd=True)\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "\n",
        "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "print(b.grad.data == np.array([2,2,2,2,2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb7lwm4aPI6y",
        "outputId": "72bba213-9afc-4fbc-877a-a79b86cfa815"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True  True  True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Freeze the seed for reproduciability.\n",
        "np.random.seed(0)\n",
        "\n",
        "data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "target = np.array([[0,1,0,1]]).T\n",
        "\n",
        "# Initialize the network\n",
        "weights_0_1 = np.random.rand(2,3)\n",
        "weights_1_2 = np.random.rand(3,1)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "epochs = 10\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Foward pass\n",
        "    layer_1 = data.dot(weights_0_1)\n",
        "    layer_2 = layer_1.dot(weights_1_2)\n",
        "    prediction = layer_2\n",
        "\n",
        "    # Mean squared error\n",
        "    diff = (prediction - target)\n",
        "    squared_diff = diff ** 2\n",
        "    loss = squared_diff.sum(0)\n",
        "\n",
        "    # Back propagation\n",
        "    layer_1_grad = diff.dot(weights_1_2.transpose())\n",
        "    weight_1_2_update = layer_1.transpose().dot(diff)\n",
        "    weight_0_1_update = data.transpose().dot(layer_1_grad)\n",
        "\n",
        "    # Update the weights\n",
        "    weights_1_2 -= weight_1_2_update * alpha\n",
        "    weights_0_1 -= weight_0_1_update * alpha\n",
        "\n",
        "    print(loss[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD2V6zp3a7-v",
        "outputId": "2f6db2aa-0e13-4a56-dfaa-4bbbf88cc17b"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.066439994622396\n",
            "0.4959907791902341\n",
            "0.4180671892167177\n",
            "0.35298133007809646\n",
            "0.2972549636567376\n",
            "0.24923260381633278\n",
            "0.20785392075862477\n",
            "0.17231260916265181\n",
            "0.14193744536652994\n",
            "0.11613979792168387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using autograd to train a neural network"
      ],
      "metadata": {
        "id": "yIt_6VUldBV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Freeze the seed for reproducability.\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create the dataset.\n",
        "X_train = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "y_train = Tensor(np.array([[0,1,0,1]]).T, autograd=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "input_dim = 2\n",
        "hidden_dim = 3\n",
        "output_dim = 1\n",
        "epochs = 10\n",
        "\n",
        "# Initialize the network\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(input_dim,hidden_dim), autograd=True))\n",
        "w.append(Tensor(np.random.rand(hidden_dim,output_dim), autograd=True))\n",
        "\n",
        "# Training loop\n",
        "for i in range(epochs):\n",
        "    # Forward pass\n",
        "    pred = X_train.mm(w[0]).mm(w[1])\n",
        "\n",
        "    # Compute MSE loss\n",
        "    loss = ((pred - y_train)*(pred - y_train)).sum(0)\n",
        "\n",
        "    # Backprogpation\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "\n",
        "    # Update the weights\n",
        "    for w_ in w:\n",
        "        w_.data -= w_.grad.data * alpha\n",
        "        w_.grad.data *= 0\n",
        "\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek5u456IdAeL",
        "outputId": "16688ffa-72b4-4d7c-c9a0-6fb1ce228d79"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.58128304]\n",
            "[0.48988149]\n",
            "[0.41375111]\n",
            "[0.34489412]\n",
            "[0.28210124]\n",
            "[0.2254484]\n",
            "[0.17538853]\n",
            "[0.1324231]\n",
            "[0.09682769]\n",
            "[0.06849361]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding automatic optimization"
      ],
      "metadata": {
        "id": "0Wvn8sSmhZPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(object):\n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "\n",
        "    def step(self, zero=True):\n",
        "        for p in self.parameters:\n",
        "            p.data -= p.grad.data * self.alpha\n",
        "\n",
        "            if (zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "# Create the dataset\n",
        "X_train = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "y_train = Tensor(np.array([[0,1,0,1]]).T, autograd=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "input_dim = 2\n",
        "hidden_size = 3\n",
        "output_dim = 1\n",
        "epochs = 10\n",
        "\n",
        "# Initialize the network\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(input_dim, hidden_size), autograd = True))\n",
        "w.append(Tensor(np.random.rand(hidden_size, output_dim), autograd = True))\n",
        "\n",
        "optim = SGD(parameters=w, alpha=alpha)\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Forward pass\n",
        "    pred = X_train.mm(w[0]).mm(w[1])\n",
        "    loss = ((pred - y_train) * (pred - y_train)).sum(0)\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nir93gk-hYU-",
        "outputId": "e1da0fdc-eceb-4809-e5dd-5bf070b90395"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.94479064]\n",
            "[0.59825449]\n",
            "[0.45744809]\n",
            "[0.35735139]\n",
            "[0.27835386]\n",
            "[0.21376109]\n",
            "[0.16081403]\n",
            "[0.1180269]\n",
            "[0.08428556]\n",
            "[0.05849538]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear layers"
      ],
      "metadata": {
        "id": "adFfnIolmeSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / (n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "\n",
        "        self.parameters.append(self.weight)\n",
        "        self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.mm(self.weight) + self.bias.expand(0, len(input.data))"
      ],
      "metadata": {
        "id": "K6LhmLwnjjWr"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers that contain layers"
      ],
      "metadata": {
        "id": "L-tp0C4PmYqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(Layer):\n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "\n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "# Freeze the seed for reproducability\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create the dataset\n",
        "X_train = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "y_train = Tensor(np.array([[0,1,0,1]]).T, autograd=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.05\n",
        "input_dim = 2\n",
        "hidden_size = 3\n",
        "output_dim = 1\n",
        "epochs = 10\n",
        "\n",
        "# Initialize the modfel\n",
        "model = Sequential([Linear(input_dim,hidden_size), Linear(hidden_size,output_dim)])\n",
        "\n",
        "# Initialize the optimizer\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=alpha)\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Forward pass\n",
        "    pred = model.forward(X_train)\n",
        "    loss = ((pred - y_train) * (pred - y_train)).sum(0)\n",
        "\n",
        "    # Back propagation\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "\n",
        "    optim.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1177aRKktS3",
        "outputId": "3d26de22-b370-4881-fa53-d117490d3cd8"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.33428272]\n",
            "[0.06743796]\n",
            "[0.0521849]\n",
            "[0.04079507]\n",
            "[0.03184365]\n",
            "[0.02479336]\n",
            "[0.01925443]\n",
            "[0.01491699]\n",
            "[0.01153118]\n",
            "[0.00889602]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function layers"
      ],
      "metadata": {
        "id": "kzf3_1jonqAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELoss(Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        return ((pred - target) * (pred - target)).sum(0)\n",
        "\n",
        "import numpy as np\n",
        "# Freeze the seed for reproducability\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create the dataset\n",
        "X_train = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "y_train = Tensor(np.array([[0,1,0,1]]).T, autograd=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.05\n",
        "input_dim = 2\n",
        "hidden_size = 3\n",
        "output_dim = 1\n",
        "epochs = 10\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential([Linear(input_dim, hidden_size), Linear(hidden_size, output_dim)])\n",
        "\n",
        "# Criterion\n",
        "criterion = MSELoss()\n",
        "\n",
        "# Optimizer\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=alpha)\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Forward pass\n",
        "    pred = model.forward(X_train)\n",
        "    loss = criterion.forward(pred, y_train)\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBJ6zAWQnsKO",
        "outputId": "824ea823-c577-4f0c-9f6b-dd42030a6aa4"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.33428272]\n",
            "[0.06743796]\n",
            "[0.0521849]\n",
            "[0.04079507]\n",
            "[0.03184365]\n",
            "[0.02479336]\n",
            "[0.01925443]\n",
            "[0.01491699]\n",
            "[0.01153118]\n",
            "[0.00889602]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-linearities"
      ],
      "metadata": {
        "id": "tgieUQJerzBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "\n",
        "class Relu(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.relu()\n",
        "\n",
        "import numpy as np\n",
        "# Freeze the seed for reproducability\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create the dataset\n",
        "X_train = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "y_train = Tensor(np.array([[0,1,0,1]]).T, autograd=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.05\n",
        "input_dim = 2\n",
        "hidden_size = 3\n",
        "output_dim = 1\n",
        "epochs = 10\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential([Linear(input_dim,hidden_size), Tanh(), Linear(hidden_size, output_dim), Sigmoid()])\n",
        "\n",
        "# Loss function\n",
        "criterion = MSELoss()\n",
        "\n",
        "# Optimizer\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Forward pass\n",
        "    pred = model.forward(X_train)\n",
        "    loss = criterion.forward(pred, y_train)\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMeR2ViYrypC",
        "outputId": "c70e753f-166d-4d16-b54c-d452c5c108cf"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.06372865]\n",
            "[0.75148144]\n",
            "[0.57384259]\n",
            "[0.39574294]\n",
            "[0.2482279]\n",
            "[0.15515294]\n",
            "[0.10423398]\n",
            "[0.07571169]\n",
            "[0.05837623]\n",
            "[0.04700013]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Embedding layer"
      ],
      "metadata": {
        "id": "42KvdQh9uW8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "\n",
        "        # this random initialiation style is just a convention from word2vec\n",
        "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
        "\n",
        "        self.parameters.append(self.weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.weight.index_select(input)"
      ],
      "metadata": {
        "id": "E9U8q62VuZB8"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = Tensor(np.eye(5), autograd=True)\n",
        "x.index_select(Tensor([[1,2,3],[2,3,4]])).backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNWM3ivdw7YD",
        "outputId": "f7ae273e-d27f-437d-cbd6-02534eb5c61f"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2.]\n",
            " [1. 1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "np.random.seed(0)\n",
        "\n",
        "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
        "target = Tensor(np.array([[0,1,0,1]]).T, autograd=True)\n",
        "\n",
        "embed = Embedding(5,3)\n",
        "model = Sequential([embed, Tanh(), Linear(3,1), Sigmoid()])\n",
        "criterion = MSELoss()\n",
        "\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=0.5)\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    # Predict\n",
        "    pred = model.forward(data)\n",
        "\n",
        "    # Compare\n",
        "    loss = criterion.forward(pred, target)\n",
        "\n",
        "    # Learn\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y96M0UDcxu3_",
        "outputId": "f923800b-7836-4ff7-84ed-111d3697af42"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.98874126]\n",
            "[0.6658868]\n",
            "[0.45639889]\n",
            "[0.31608168]\n",
            "[0.2260925]\n",
            "[0.16877423]\n",
            "[0.13120515]\n",
            "[0.10555487]\n",
            "[0.08731868]\n",
            "[0.07387834]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss(object):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Freeze the seed for reproducability.\n",
        "np.random.seed(0)\n",
        "\n",
        "# Load the dataset\n",
        "X_train = Tensor(np.array([1,2,1,2]), autograd=True)\n",
        "y_train = Tensor(np.array([0,1,0,1]), autograd=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.05\n",
        "input_dim = 3\n",
        "hidden_size = 3\n",
        "output_dim = 4\n",
        "epochs = 100\n",
        "\n",
        "# Intializae the model\n",
        "model = Sequential([Embedding(input_dim,hidden_size), Tanh(), Linear(hidden_size,output_dim)])\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=alpha)\n",
        "\n",
        "# Training loop\n",
        "for i in range(epochs):\n",
        "    # Foward pass\n",
        "    pred = model.forward(X_train)\n",
        "    loss = criterion.forward(pred, y_train)\n",
        "    # Back propagation\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    optim.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgaauIRWzm0x",
        "outputId": "55839068-399a-49e9-c410-579738433c3f"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3885032434928422\n",
            "1.1589954572218848\n",
            "0.9716749474445552\n",
            "0.8210884856655897\n",
            "0.7006078064272916\n",
            "0.6039034135741919\n",
            "0.5256911650712581\n",
            "0.4618412956011161\n",
            "0.4092094174112788\n",
            "0.3654141538910436\n",
            "0.32864434909977636\n",
            "0.29751151228726846\n",
            "0.27094154389571456\n",
            "0.24809605233780735\n",
            "0.22831497243195564\n",
            "0.2110743693694357\n",
            "0.19595511500698373\n",
            "0.18261942038294754\n",
            "0.17079310304498868\n",
            "0.16025208120465795\n",
            "0.15081201069040906\n",
            "0.1423202773270627\n",
            "0.13464976758172476\n",
            "0.12769399096647832\n",
            "0.12136323670917579\n",
            "0.11558152676294825\n",
            "0.11028418571360833\n",
            "0.10541589142987418\n",
            "0.10092910253494163\n",
            "0.09678278292307543\n",
            "0.0929413617343458\n",
            "0.08937388097963198\n",
            "0.08605329350287548\n",
            "0.08295588200386533\n",
            "0.08006077603098155\n",
            "0.07734954863993607\n",
            "0.07480587813709627\n",
            "0.07241526323547084\n",
            "0.0701647822365549\n",
            "0.0680428886545614\n",
            "0.0660392371293141\n",
            "0.06414453461282305\n",
            "0.06235041272552896\n",
            "0.06064931791013976\n",
            "0.05903441660150906\n",
            "0.05749951310942289\n",
            "0.05603897830027456\n",
            "0.05464768748131345\n",
            "0.053320966151530716\n",
            "0.052054542497422016\n",
            "0.050844505688642586\n",
            "0.04968726917500821\n",
            "0.04857953830798009\n",
            "0.0475182817112222\n",
            "0.04650070590967707\n",
            "0.045524232797784045\n",
            "0.04458647958735437\n",
            "0.04368524092615306\n",
            "0.04281847292100109\n",
            "0.041984278835488435\n",
            "0.04118089626325207\n",
            "0.040406685604100004\n",
            "0.039660119692760024\n",
            "0.03893977444932758\n",
            "0.03824432043705115\n",
            "0.03757251522736398\n",
            "0.03692319648438448\n",
            "0.03629527569175082\n",
            "0.03568773245388769\n",
            "0.03509960931181526\n",
            "0.0345300070205839\n",
            "0.033978080241497226\n",
            "0.03344303360759873\n",
            "0.0329241181255437\n",
            "0.03242062788105197\n",
            "0.03193189701871739\n",
            "0.03145729697009851\n",
            "0.03099623390678486\n",
            "0.030548146397588145\n",
            "0.030112503251169615\n",
            "0.029688801527329264\n",
            "0.029276564701887473\n",
            "0.028875340971596948\n",
            "0.028484701686861326\n",
            "0.0281042399012446\n",
            "0.02773356902780441\n",
            "0.02737232159325271\n",
            "0.027020148081786537\n",
            "0.02667671586120008\n",
            "0.026341708184570416\n",
            "0.02601482326142806\n",
            "0.025695773392865058\n",
            "0.025384284165543006\n",
            "0.025080093699999428\n",
            "0.024782951949058996\n",
            "0.024492620042522045\n",
            "0.024208869674625856\n",
            "0.0239314825310803\n",
            "0.02366024975274081\n",
            "0.02339497143323133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        if(activation == 'sigmoid'):\n",
        "            self.activation = Sigmoid()\n",
        "        elif(activation == 'tanh'):\n",
        "            self.activation == Tanh()\n",
        "        else:\n",
        "            raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "        self.w_ih = Linear(n_inputs, n_hidden)\n",
        "        self.w_hh = Linear(n_hidden, n_hidden)\n",
        "        self.w_ho = Linear(n_hidden, n_output)\n",
        "\n",
        "        self.parameters += self.w_ih.get_parameters()\n",
        "        self.parameters += self.w_hh.get_parameters()\n",
        "        self.parameters += self.w_ho.get_parameters()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        from_prev_hidden = self.w_hh.forward(hidden)\n",
        "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "        new_hidden = self.activation.forward(combined)\n",
        "        output = self.w_ho.forward(new_hidden)\n",
        "        return output, new_hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)"
      ],
      "metadata": {
        "id": "loyYsrcz3aGL"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n",
        "! tar -xvf tasks_1-20_v1-2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8v9Gh2d5t5j",
        "outputId": "12541934-7f6e-4daf-a90e-c3cf4f85a571"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-24 15:38:41--  https://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n",
            "Resolving www.thespermwhale.com (www.thespermwhale.com)... 50.31.160.191\n",
            "Connecting to www.thespermwhale.com (www.thespermwhale.com)|50.31.160.191|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15719851 (15M) [application/x-gzip]\n",
            "Saving to: ‘tasks_1-20_v1-2.tar.gz.2’\n",
            "\n",
            "tasks_1-20_v1-2.tar 100%[===================>]  14.99M  6.27MB/s    in 2.4s    \n",
            "\n",
            "2024-08-24 15:38:44 (6.27 MB/s) - ‘tasks_1-20_v1-2.tar.gz.2’ saved [15719851/15719851]\n",
            "\n",
            "tasks_1-20_v1-2/\n",
            "tasks_1-20_v1-2/hn/\n",
            "tasks_1-20_v1-2/hn/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/hn/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/hn/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/hn/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/hn/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/hn/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/hn/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/hn/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/hn/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/hn/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/hn/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/hn/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/hn/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/hn/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/hn/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/hn/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/hn/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/hn/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/hn/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/hn/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/hn/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/hn/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/hn/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/hn/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled/\n",
            "tasks_1-20_v1-2/shuffled/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/shuffled/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-10k/\n",
            "tasks_1-20_v1-2/en-10k/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/en-10k/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en-10k/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-valid/\n",
            "tasks_1-20_v1-2/en-valid/qa8_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa11_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa7_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa13_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa7_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa19_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa12_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa18_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa6_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa9_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa8_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa2_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa12_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa11_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa9_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa1_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa7_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa16_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa4_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa2_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa5_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa16_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa18_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa13_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa11_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa1_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa5_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa15_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa20_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa18_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa19_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa9_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa17_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa15_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa5_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa20_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa14_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa4_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa15_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa10_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa8_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa6_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa17_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa10_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa3_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa3_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa16_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa3_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa14_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa19_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa4_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa1_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa2_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa13_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa20_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa6_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa14_train.txt\n",
            "tasks_1-20_v1-2/en-valid/qa12_valid.txt\n",
            "tasks_1-20_v1-2/en-valid/qa17_test.txt\n",
            "tasks_1-20_v1-2/en-valid/qa10_valid.txt\n",
            "tasks_1-20_v1-2/hn-10k/\n",
            "tasks_1-20_v1-2/hn-10k/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/hn-10k/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/\n",
            "tasks_1-20_v1-2/shuffled-10k/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/shuffled-10k/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/\n",
            "tasks_1-20_v1-2/en-valid-10k/qa8_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa11_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa7_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa13_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa7_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa19_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa12_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa18_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa6_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa9_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa8_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa2_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa12_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa11_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa9_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa1_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa7_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa16_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa4_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa2_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa5_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa16_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa18_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa13_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa11_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa1_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa5_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa15_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa20_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa18_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa19_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa9_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa17_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa15_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa5_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa20_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa14_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa4_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa15_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa10_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa8_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa6_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa17_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa10_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa3_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa3_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa16_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa3_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa14_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa19_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa4_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa1_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa2_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa13_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa20_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa6_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa14_train.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa12_valid.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa17_test.txt\n",
            "tasks_1-20_v1-2/en-valid-10k/qa10_valid.txt\n",
            "tasks_1-20_v1-2/en/\n",
            "tasks_1-20_v1-2/en/qa16_basic-induction_train.txt\n",
            "tasks_1-20_v1-2/en/qa13_compound-coreference_train.txt\n",
            "tasks_1-20_v1-2/en/qa13_compound-coreference_test.txt\n",
            "tasks_1-20_v1-2/en/qa14_time-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en/qa5_three-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en/qa17_positional-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en/qa9_simple-negation_train.txt\n",
            "tasks_1-20_v1-2/en/qa12_conjunction_train.txt\n",
            "tasks_1-20_v1-2/en/qa6_yes-no-questions_train.txt\n",
            "tasks_1-20_v1-2/en/qa2_two-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en/qa20_agents-motivations_train.txt\n",
            "tasks_1-20_v1-2/en/qa7_counting_train.txt\n",
            "tasks_1-20_v1-2/en/qa18_size-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\n",
            "tasks_1-20_v1-2/en/qa18_size-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en/qa1_single-supporting-fact_test.txt\n",
            "tasks_1-20_v1-2/en/qa16_basic-induction_test.txt\n",
            "tasks_1-20_v1-2/en/qa8_lists-sets_train.txt\n",
            "tasks_1-20_v1-2/en/qa15_basic-deduction_test.txt\n",
            "tasks_1-20_v1-2/en/qa11_basic-coreference_train.txt\n",
            "tasks_1-20_v1-2/en/qa12_conjunction_test.txt\n",
            "tasks_1-20_v1-2/en/qa10_indefinite-knowledge_test.txt\n",
            "tasks_1-20_v1-2/en/qa19_path-finding_test.txt\n",
            "tasks_1-20_v1-2/en/qa8_lists-sets_test.txt\n",
            "tasks_1-20_v1-2/en/qa4_two-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en/qa10_indefinite-knowledge_train.txt\n",
            "tasks_1-20_v1-2/en/qa19_path-finding_train.txt\n",
            "tasks_1-20_v1-2/en/qa20_agents-motivations_test.txt\n",
            "tasks_1-20_v1-2/en/qa5_three-arg-relations_train.txt\n",
            "tasks_1-20_v1-2/en/qa7_counting_test.txt\n",
            "tasks_1-20_v1-2/en/qa3_three-supporting-facts_test.txt\n",
            "tasks_1-20_v1-2/en/qa14_time-reasoning_train.txt\n",
            "tasks_1-20_v1-2/en/qa17_positional-reasoning_test.txt\n",
            "tasks_1-20_v1-2/en/qa9_simple-negation_test.txt\n",
            "tasks_1-20_v1-2/en/qa4_two-arg-relations_test.txt\n",
            "tasks_1-20_v1-2/en/qa6_yes-no-questions_test.txt\n",
            "tasks_1-20_v1-2/en/qa15_basic-deduction_train.txt\n",
            "tasks_1-20_v1-2/en/qa3_three-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en/qa2_two-supporting-facts_train.txt\n",
            "tasks_1-20_v1-2/en/qa11_basic-coreference_test.txt\n",
            "tasks_1-20_v1-2/LICENSE.txt\n",
            "tasks_1-20_v1-2/README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "f = open('tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt','r')\n",
        "raw = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list()\n",
        "for line in raw[0:1000]:\n",
        "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
        "\n",
        "new_tokens = list()\n",
        "for line in tokens:\n",
        "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
        "\n",
        "tokens = new_tokens\n",
        "\n",
        "vocab = set()\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        vocab.add(word)\n",
        "\n",
        "vocab = list(vocab)\n",
        "\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "\n",
        "def words2indices(sentence):\n",
        "    idx = list()\n",
        "    for word in sentence:\n",
        "        idx.append(word2index[word])\n",
        "    return idx\n",
        "\n",
        "indices = list()\n",
        "for line in tokens:\n",
        "    idx = list()\n",
        "    for w in line:\n",
        "        idx.append(word2index[w])\n",
        "    indices.append(idx)\n",
        "\n",
        "data = np.array(indices)"
      ],
      "metadata": {
        "id": "JTYAg_Jm5ygh"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamters\n",
        "alpha = 0.05\n",
        "epochs = 1_000\n",
        "batch_size = 100\n",
        "input_dim = len(vocab)\n",
        "output_dim = len(vocab)\n",
        "\n",
        "embed = Embedding(vocab_size=input_dim,dim=16)\n",
        "model = RNNCell(n_inputs=16, n_hidden=16, n_output=output_dim)\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=alpha)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(epochs):\n",
        "    batch_size = 100\n",
        "    total_loss = 0\n",
        "\n",
        "    hidden = model.init_hidden(batch_size=batch_size)\n",
        "\n",
        "    for t in range(5):\n",
        "        input = Tensor(data[0:batch_size,t], autograd=True)\n",
        "        rnn_input = embed.forward(input=input)\n",
        "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "\n",
        "    target = Tensor(data[0:batch_size,t+1], autograd=True)\n",
        "    loss = criterion.forward(output, target)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    total_loss += loss.data\n",
        "    if(iter % 200 == 0):\n",
        "        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()\n",
        "        print(\"Loss:\",total_loss / (len(data)/batch_size),\"% Correct:\",p_correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANcjN5s_668Y",
        "outputId": "6b28a871-dce0-43b1-bab0-fccf617b1dbb"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4364644457671538 % Correct: 0.02\n",
            "Loss: 0.17284790681688394 % Correct: 0.25\n",
            "Loss: 0.1582427394973346 % Correct: 0.31\n",
            "Loss: 0.1411711909238484 % Correct: 0.37\n",
            "Loss: 0.13707044352997122 % Correct: 0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "hidden = model.init_hidden(batch_size=batch_size)\n",
        "for t in range(5):\n",
        "    input = Tensor(data[0:batch_size,t], autograd=True)\n",
        "    rnn_input = embed.forward(input=input)\n",
        "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "\n",
        "target = Tensor(data[0:batch_size, t+1], autograd=True)\n",
        "loss = criterion.forward(output, target)\n",
        "\n",
        "context = \"\"\n",
        "for idx in data[0:batch_size][0][0:-1]:\n",
        "    context += vocab[idx] + \" \"\n",
        "pred = vocab[output.data.argmax()]\n",
        "print(f\"Context: {context}\\n pred: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJgJ7xeM94sQ",
        "outputId": "c29dd9f0-b493-4312-d3df-d9900a5de75f"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: - mary moved to the \n",
            " pred: garden.\n"
          ]
        }
      ]
    }
  ]
}