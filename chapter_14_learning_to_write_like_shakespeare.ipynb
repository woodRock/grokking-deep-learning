{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd8vDs09TQ4Uxlb8EoGeej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/grokking-deep-learning/blob/main/chapter_14_learning_to_write_like_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAIWXWPxyL9a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "\n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "\n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "\n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "\n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        "\n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "\n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "\n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and\n",
        "               (self.all_children_grads_accounted_for() or\n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "\n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)\n",
        "\n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "\n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "                if(self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "\n",
        "                if(self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "\n",
        "                if(self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "\n",
        "                if(self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)\n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "\n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "\n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "\n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "\n",
        "        return Tensor(self.data.transpose())\n",
        "\n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "\n",
        "    def sigmoid(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(np.tanh(self.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "\n",
        "    def index_select(self, indices):\n",
        "\n",
        "        if(self.autograd):\n",
        "            new = Tensor(self.data[indices.data],\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "\n",
        "    def softmax(self):\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        return softmax_output\n",
        "\n",
        "    def cross_entropy(self, target_indices):\n",
        "\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "\n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t),-1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "\n",
        "        if(self.autograd):\n",
        "            out = Tensor(loss,\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())\n",
        "\n",
        "class Layer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "\n",
        "\n",
        "class SGD(object):\n",
        "\n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "\n",
        "    def step(self, zero=True):\n",
        "\n",
        "        for p in self.parameters:\n",
        "\n",
        "            p.data -= p.grad.data * self.alpha\n",
        "\n",
        "            if(zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "\n",
        "        self.parameters.append(self.weight)\n",
        "        self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
        "\n",
        "\n",
        "class Sequential(Layer):\n",
        "\n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "\n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "\n",
        "class Embedding(Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "\n",
        "        # this random initialiation style is just a convention from word2vec\n",
        "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
        "\n",
        "        self.parameters.append(self.weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.weight.index_select(input)\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "\n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "\n",
        "class RNNCell(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        if(activation == 'sigmoid'):\n",
        "            self.activation = Sigmoid()\n",
        "        elif(activation == 'tanh'):\n",
        "            self.activation == Tanh()\n",
        "        else:\n",
        "            raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "        self.w_ih = Linear(n_inputs, n_hidden)\n",
        "        self.w_hh = Linear(n_hidden, n_hidden)\n",
        "        self.w_ho = Linear(n_hidden, n_output)\n",
        "\n",
        "        self.parameters += self.w_ih.get_parameters()\n",
        "        self.parameters += self.w_hh.get_parameters()\n",
        "        self.parameters += self.w_ho.get_parameters()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        from_prev_hidden = self.w_hh.forward(hidden)\n",
        "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "        new_hidden = self.activation.forward(combined)\n",
        "        output = self.w_ho.forward(new_hidden)\n",
        "        return output, new_hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Freeze the seed for reproducability.\n",
        "np.random.seed(0)\n",
        "\n",
        "f = open(\"shakespeare.txt\", \"r\")\n",
        "raw = f.read()\n",
        "f.close()\n",
        "\n",
        "vocab = list(set(raw))\n",
        "word2index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "    word2index[word] = i\n",
        "indices = np.array(list(map(lambda x: word2index[x], raw)))"
      ],
      "metadata": {
        "id": "RawqACCJyf9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.05\n",
        "epochs = 1_000\n",
        "batch_size = 32\n",
        "input_dim = len(vocab)\n",
        "output_dim = len(vocab)\n",
        "bptt = 16\n",
        "n_batches = int((indices.shape[0] / (batch_size)))\n",
        "\n",
        "trimmed_indices = indices[:n_batches*batch_size]\n",
        "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
        "batched_indices = batched_indices.transpose()\n",
        "\n",
        "input_batched_indices = batched_indices[0:-1]\n",
        "target_batched_indices = batched_indices[1:]\n",
        "\n",
        "n_bptt = int((n_batches-1)/ bptt)\n",
        "input_batches = input_batched_indices[:n_bptt*bptt]\n",
        "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
        "target_batches = target_batched_indices[:n_bptt*bptt]\n",
        "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
        "\n",
        "\n",
        "embed = Embedding(vocab_size=input_dim, dim=512)\n",
        "model = RNNCell(n_inputs=512, n_hidden=512, n_output=output_dim)\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=alpha)\n"
      ],
      "metadata": {
        "id": "jHMZAdJxzR1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"raw[0:5]: {raw[0:5]}\")\n",
        "print(f\"indices[0:5]: {indices[0:5]}\")\n",
        "print(f\"batched_indices[0:5]: {batched_indices[0:5]}\")\n",
        "print(f\"input_batches[0][0:5]: {input_batches[0][0:5]}\")\n",
        "print(f\"target_batches[0][0:5]: {target_batches[0][0:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXjs1Zd82Cun",
        "outputId": "ca3b4203-8e53-4488-ae2b-e436abda722d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw[0:5]: THE S\n",
            "indices[0:5]: [ 8 49  9 32 50]\n",
            "batched_indices[0:5]: [[ 8 55 32 38 33 32 28 28 32 53 30 32 14 33 14 10 41 28 35 32 33 32 32 51\n",
            "  45 53 53 32 32 10  0  4]\n",
            " [49 15 51 32 35 21  0 58 60 32 38 45 41  4 53 28 32 58 45 39 20 53 39 30\n",
            "  38 20 58 39 45 28 48 32]\n",
            " [ 9 28 33 45 33 33 38 48 30 14 32  0 32 32 32 58 51  6  0 45 32 45 28 33\n",
            "  32 32 51 10  0 58 58 53]\n",
            " [32 30 32  0 30 55 33 10 14 30  0 32 41 10  5 47 30 10 48 47 21 47 58 33\n",
            "  45 14 28 33 32 48 45 39]\n",
            " [50 32 38  5 32 27  0 32 45 33 28 36 28 45 47  4 14 33 17 47 45 33 47  4\n",
            "  21  0 30  0 14 10 53 33]]\n",
            "input_batches[0][0:5]: [[ 8 55 32 38 33 32 28 28 32 53 30 32 14 33 14 10 41 28 35 32 33 32 32 51\n",
            "  45 53 53 32 32 10  0  4]\n",
            " [49 15 51 32 35 21  0 58 60 32 38 45 41  4 53 28 32 58 45 39 20 53 39 30\n",
            "  38 20 58 39 45 28 48 32]\n",
            " [ 9 28 33 45 33 33 38 48 30 14 32  0 32 32 32 58 51  6  0 45 32 45 28 33\n",
            "  32 32 51 10  0 58 58 53]\n",
            " [32 30 32  0 30 55 33 10 14 30  0 32 41 10  5 47 30 10 48 47 21 47 58 33\n",
            "  45 14 28 33 32 48 45 39]\n",
            " [50 32 38  5 32 27  0 32 45 33 28 36 28 45 47  4 14 33 17 47 45 33 47  4\n",
            "  21  0 30  0 14 10 53 33]]\n",
            "target_batches[0][0:5]: [[49 15 51 32 35 21  0 58 60 32 38 45 41  4 53 28 32 58 45 39 20 53 39 30\n",
            "  38 20 58 39 45 28 48 32]\n",
            " [ 9 28 33 45 33 33 38 48 30 14 32  0 32 32 32 58 51  6  0 45 32 45 28 33\n",
            "  32 32 51 10  0 58 58 53]\n",
            " [32 30 32  0 30 55 33 10 14 30  0 32 41 10  5 47 30 10 48 47 21 47 58 33\n",
            "  45 14 28 33 32 48 45 39]\n",
            " [50 32 38  5 32 27  0 32 45 33 28 36 28 45 47  4 14 33 17 47 45 33 47  4\n",
            "  21  0 30  0 14 10 53 33]\n",
            " [26 45 10 47 47 30 38 38 53 32 38 30 58 53 28 32 45 53 55 32  0  0  4 53\n",
            "  60  4  0 32 51 32 10 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(n=30, init_char=' '):\n",
        "    s = \"\"\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    input = Tensor(np.array([word2index[init_char]]))\n",
        "    for i in range(n):\n",
        "        rnn_input = embed.forward(input)\n",
        "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "        # Temperature for sampling: higher = greedier\n",
        "        output.data *= 10\n",
        "        temp_dist = output.softmax()\n",
        "        temp_dist /= temp_dist.sum()\n",
        "\n",
        "        # Samples from pred\n",
        "        m = (temp_dist > np.random.rand()).argmax()\n",
        "        c = vocab[m]\n",
        "        input = Tensor(np.array([m]))\n",
        "        s += c\n",
        "\n",
        "    return s\n",
        "\n",
        "def train(iterations=100):\n",
        "    for j in range(iterations):\n",
        "        total_loss = 0\n",
        "        n_loss = 0\n",
        "        hidden = model.init_hidden(batch_size=batch_size)\n",
        "        for batch_i in range(len(input_batches)):\n",
        "            hidden = Tensor(hidden.data, autograd=True)\n",
        "            loss = None\n",
        "            losses = list()\n",
        "            for t in range(bptt):\n",
        "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
        "                rnn_input = embed.forward(input=input)\n",
        "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
        "                batch_loss = criterion.forward(output, target)\n",
        "                losses.append(batch_loss)\n",
        "                if (t == 0):\n",
        "                    loss = batch_loss\n",
        "                else:\n",
        "                    loss = loss + batch_loss\n",
        "            for loss in losses:\n",
        "                \"\"\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            total_loss += loss.data\n",
        "            loss_str = np.exp(total_loss) / (batch_i + 1)\n",
        "            log = f\"Iter: {j} - Batch: {batch_i+1}/{len(input_batches)} - Loss: {loss_str}\"\n",
        "\n",
        "            if (batch_i == 0):\n",
        "                log += \" - \" + generate_sample(70, '\\n').replace('\\n', ' ')\n",
        "            if (batch_i % 10 == 0 or batch_i - 1 == len(input_batches)):\n",
        "                print(log)\n",
        "        optim.alpha *= 0.99\n",
        "\n",
        "train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "II7qwKEp3Gfu",
        "outputId": "6d6bd0d6-3770-4b4e-a6e8-76e5cb3c23b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0 - Batch: 1/184 - Loss: 26.92697147965263 - eoeoeo ono oooooooooonoonooooonoooooooooonoonononooooooooooonoooooonoo\n",
            "Iter: 0 - Batch: 11/184 - Loss: 2530290404938072.5\n",
            "Iter: 0 - Batch: 21/184 - Loss: 4.490789308467799e+30\n",
            "Iter: 0 - Batch: 31/184 - Loss: 5.3173478202778674e+44\n",
            "Iter: 0 - Batch: 41/184 - Loss: 1.5989553238068635e+58\n",
            "Iter: 0 - Batch: 51/184 - Loss: 4.2376314221215725e+71\n",
            "Iter: 0 - Batch: 61/184 - Loss: 1.3088443310339625e+85\n",
            "Iter: 0 - Batch: 71/184 - Loss: 1.0314156214186484e+99\n",
            "Iter: 0 - Batch: 81/184 - Loss: 8.921351753514044e+112\n",
            "Iter: 0 - Batch: 91/184 - Loss: 5.5622244364080484e+125\n",
            "Iter: 0 - Batch: 101/184 - Loss: 1.7614908128182694e+139\n",
            "Iter: 0 - Batch: 111/184 - Loss: 2.033891852604758e+151\n",
            "Iter: 0 - Batch: 121/184 - Loss: 3.788865056903866e+164\n",
            "Iter: 0 - Batch: 131/184 - Loss: 4.768255224929501e+177\n",
            "Iter: 0 - Batch: 141/184 - Loss: 1.3091859933680031e+190\n",
            "Iter: 0 - Batch: 151/184 - Loss: 1.596731312878909e+202\n",
            "Iter: 0 - Batch: 161/184 - Loss: 9.264279250722261e+213\n",
            "Iter: 0 - Batch: 171/184 - Loss: 1.1943763534107237e+226\n",
            "Iter: 0 - Batch: 181/184 - Loss: 1.1662982877749764e+238\n",
            "Iter: 1 - Batch: 1/184 - Loss: 16.102029584700006 - sosn oos oos oos nosn nos tos tos tos tosn oosn tos on tos tos tosn oo\n",
            "Iter: 1 - Batch: 11/184 - Loss: 1347770167651.1873\n",
            "Iter: 1 - Batch: 21/184 - Loss: 3.0145197819138456e+24\n",
            "Iter: 1 - Batch: 31/184 - Loss: 1.1930327286922369e+36\n",
            "Iter: 1 - Batch: 41/184 - Loss: 4.34824187206616e+47\n",
            "Iter: 1 - Batch: 51/184 - Loss: 1.0150901534230702e+59\n",
            "Iter: 1 - Batch: 61/184 - Loss: 3.2624937339634503e+70\n",
            "Iter: 1 - Batch: 71/184 - Loss: 1.5656312500479807e+82\n",
            "Iter: 1 - Batch: 81/184 - Loss: 3.1374393128237805e+94\n",
            "Iter: 1 - Batch: 91/184 - Loss: 9.560128042362389e+105\n",
            "Iter: 1 - Batch: 101/184 - Loss: 1.211685566056997e+118\n",
            "Iter: 1 - Batch: 111/184 - Loss: 7.62779135340879e+128\n",
            "Iter: 1 - Batch: 121/184 - Loss: 5.2316383225042335e+140\n",
            "Iter: 1 - Batch: 131/184 - Loss: 3.9718209947300075e+152\n",
            "Iter: 1 - Batch: 141/184 - Loss: 1.0290888239981216e+164\n",
            "Iter: 1 - Batch: 151/184 - Loss: 1.0265798066961086e+175\n",
            "Iter: 1 - Batch: 161/184 - Loss: 5.915533747492064e+185\n",
            "Iter: 1 - Batch: 171/184 - Loss: 6.581651704231488e+196\n",
            "Iter: 1 - Batch: 181/184 - Loss: 4.470300284770374e+207\n",
            "Iter: 2 - Batch: 1/184 - Loss: 11.030250894698998 - s nous ous ousn tous ous ousn tous ous ous ous ousn tousn tous ousn no\n",
            "Iter: 2 - Batch: 11/184 - Loss: 95779996827.55122\n",
            "Iter: 2 - Batch: 21/184 - Loss: 1.0368735844780451e+22\n",
            "Iter: 2 - Batch: 31/184 - Loss: 6.587137433601497e+32\n",
            "Iter: 2 - Batch: 41/184 - Loss: 5.613104346763317e+43\n",
            "Iter: 2 - Batch: 51/184 - Loss: 2.1307376482142294e+54\n",
            "Iter: 2 - Batch: 61/184 - Loss: 1.2153424137582939e+65\n",
            "Iter: 2 - Batch: 71/184 - Loss: 1.5431596487629286e+76\n",
            "Iter: 2 - Batch: 81/184 - Loss: 4.7393562438395805e+87\n",
            "Iter: 2 - Batch: 91/184 - Loss: 3.2739564948635676e+98\n",
            "Iter: 2 - Batch: 101/184 - Loss: 6.622850880086576e+109\n",
            "Iter: 2 - Batch: 111/184 - Loss: 8.079138063972977e+119\n",
            "Iter: 2 - Batch: 121/184 - Loss: 1.1462969177359506e+131\n",
            "Iter: 2 - Batch: 131/184 - Loss: 1.646258905736685e+142\n",
            "Iter: 2 - Batch: 141/184 - Loss: 9.305437049724452e+152\n",
            "Iter: 2 - Batch: 151/184 - Loss: 2.5066714342920023e+163\n",
            "Iter: 2 - Batch: 161/184 - Loss: 5.430104299670898e+173\n",
            "Iter: 2 - Batch: 171/184 - Loss: 1.7867958119120955e+184\n",
            "Iter: 2 - Batch: 181/184 - Loss: 2.5675046240494083e+194\n",
            "Iter: 3 - Batch: 1/184 - Loss: 9.252270175310832 - s owe oust ousn tous ousn tous ousn tous now tousn tous oust ous oust \n",
            "Iter: 3 - Batch: 11/184 - Loss: 21799993940.101746\n",
            "Iter: 3 - Batch: 21/184 - Loss: 4.2231927502697e+20\n",
            "Iter: 3 - Batch: 31/184 - Loss: 9.415027721501663e+30\n",
            "Iter: 3 - Batch: 41/184 - Loss: 3.0977093802748977e+41\n",
            "Iter: 3 - Batch: 51/184 - Loss: 5.090199354373684e+51\n",
            "Iter: 3 - Batch: 61/184 - Loss: 9.79520672920997e+61\n",
            "Iter: 3 - Batch: 71/184 - Loss: 5.227839581874237e+72\n",
            "Iter: 3 - Batch: 81/184 - Loss: 5.178612338727863e+83\n",
            "Iter: 3 - Batch: 91/184 - Loss: 1.523531587518105e+94\n",
            "Iter: 3 - Batch: 101/184 - Loss: 9.858209615182188e+104\n",
            "Iter: 3 - Batch: 111/184 - Loss: 4.46040126284181e+114\n",
            "Iter: 3 - Batch: 121/184 - Loss: 2.3673365635954433e+125\n",
            "Iter: 3 - Batch: 131/184 - Loss: 1.2497474617400615e+136\n",
            "Iter: 3 - Batch: 141/184 - Loss: 2.626321093597447e+146\n",
            "Iter: 3 - Batch: 151/184 - Loss: 3.1855931168442895e+156\n",
            "Iter: 3 - Batch: 161/184 - Loss: 3.833547495529247e+166\n",
            "Iter: 3 - Batch: 171/184 - Loss: 6.026607159379067e+176\n",
            "Iter: 3 - Batch: 181/184 - Loss: 2.6901156708412105e+186\n",
            "Iter: 4 - Batch: 1/184 - Loss: 8.714771631283677 -  An tonn to tnoust on tousn tous oust ow tousn tous now tousn tous ow \n",
            "Iter: 4 - Batch: 11/184 - Loss: 8790790273.661835\n",
            "Iter: 4 - Batch: 21/184 - Loss: 5.445317329876086e+19\n",
            "Iter: 4 - Batch: 31/184 - Loss: 6.021947000131991e+29\n",
            "Iter: 4 - Batch: 41/184 - Loss: 1.0145155619580132e+40\n",
            "Iter: 4 - Batch: 51/184 - Loss: 9.288153719245537e+49\n",
            "Iter: 4 - Batch: 61/184 - Loss: 7.559792823990388e+59\n",
            "Iter: 4 - Batch: 71/184 - Loss: 2.1824374273805e+70\n",
            "Iter: 4 - Batch: 81/184 - Loss: 9.367632992225118e+80\n",
            "Iter: 4 - Batch: 91/184 - Loss: 1.438199019572462e+91\n",
            "Iter: 4 - Batch: 101/184 - Loss: 4.013300385251996e+101\n",
            "Iter: 4 - Batch: 111/184 - Loss: 9.666241130155118e+110\n",
            "Iter: 4 - Batch: 121/184 - Loss: 2.5623679520509544e+121\n",
            "Iter: 4 - Batch: 131/184 - Loss: 6.928535904840867e+131\n",
            "Iter: 4 - Batch: 141/184 - Loss: 6.937716530076654e+141\n",
            "Iter: 4 - Batch: 151/184 - Loss: 4.7869616105662135e+151\n",
            "Iter: 4 - Batch: 161/184 - Loss: 3.3048130506093293e+161\n",
            "Iter: 4 - Batch: 171/184 - Loss: 2.958283086893092e+171\n",
            "Iter: 4 - Batch: 181/184 - Loss: 5.475109436831826e+180\n",
            "Iter: 5 - Batch: 1/184 - Loss: 8.44344092419285 -  An thes ton tous ow tous on tous ow touse to tnou tond ou thes to the\n",
            "Iter: 5 - Batch: 11/184 - Loss: 4525093828.720855\n",
            "Iter: 5 - Batch: 21/184 - Loss: 1.1309780096727996e+19\n",
            "Iter: 5 - Batch: 31/184 - Loss: 7.035784770441886e+28\n",
            "Iter: 5 - Batch: 41/184 - Loss: 7.027538376181474e+38\n",
            "Iter: 5 - Batch: 51/184 - Loss: 3.7369489598268784e+48\n",
            "Iter: 5 - Batch: 61/184 - Loss: 1.5402797800866912e+58\n",
            "Iter: 5 - Batch: 71/184 - Loss: 2.569961722781521e+68\n",
            "Iter: 5 - Batch: 81/184 - Loss: 5.597112920031856e+78\n",
            "Iter: 5 - Batch: 91/184 - Loss: 4.951000140220217e+88\n",
            "Iter: 5 - Batch: 101/184 - Loss: 7.231043169393441e+98\n",
            "Iter: 5 - Batch: 111/184 - Loss: 1.1039588406836763e+108\n",
            "Iter: 5 - Batch: 121/184 - Loss: 1.5526968488173217e+118\n",
            "Iter: 5 - Batch: 131/184 - Loss: 2.5082317660561802e+128\n",
            "Iter: 5 - Batch: 141/184 - Loss: 1.3617951131616952e+138\n",
            "Iter: 5 - Batch: 151/184 - Loss: 5.857101841756493e+147\n",
            "Iter: 5 - Batch: 161/184 - Loss: 2.4357535441707144e+157\n",
            "Iter: 5 - Batch: 171/184 - Loss: 1.3852379255312627e+167\n",
            "Iter: 5 - Batch: 181/184 - Loss: 1.3056908901079673e+176\n",
            "Iter: 6 - Batch: 1/184 - Loss: 8.145415801828152 -  An tnou thes ow tonn to thes now thesnn thes to thes to the to thes n\n",
            "Iter: 6 - Batch: 11/184 - Loss: 2576645760.355329\n",
            "Iter: 6 - Batch: 21/184 - Loss: 2.9933087213011753e+18\n",
            "Iter: 6 - Batch: 31/184 - Loss: 1.0841919398665812e+28\n",
            "Iter: 6 - Batch: 41/184 - Loss: 6.918045278125473e+37\n",
            "Iter: 6 - Batch: 51/184 - Loss: 2.2522968995735875e+47\n",
            "Iter: 6 - Batch: 61/184 - Loss: 5.4630783822561e+56\n",
            "Iter: 6 - Batch: 71/184 - Loss: 5.5635159992060154e+66\n",
            "Iter: 6 - Batch: 81/184 - Loss: 6.841700152284312e+76\n",
            "Iter: 6 - Batch: 91/184 - Loss: 3.6832988942923234e+86\n",
            "Iter: 6 - Batch: 101/184 - Loss: 3.1776472847014858e+96\n",
            "Iter: 6 - Batch: 111/184 - Loss: 3.3590778946473707e+105\n",
            "Iter: 6 - Batch: 121/184 - Loss: 2.62079101424538e+115\n",
            "Iter: 6 - Batch: 131/184 - Loss: 2.7177211165194465e+125\n",
            "Iter: 6 - Batch: 141/184 - Loss: 8.724189619429845e+134\n",
            "Iter: 6 - Batch: 151/184 - Loss: 2.4092466194587816e+144\n",
            "Iter: 6 - Batch: 161/184 - Loss: 6.4490234495666506e+153\n",
            "Iter: 6 - Batch: 171/184 - Loss: 2.5648116183508713e+163\n",
            "Iter: 6 - Batch: 181/184 - Loss: 1.3753023375579134e+172\n",
            "Iter: 7 - Batch: 1/184 - Loss: 7.8898153754339635 -  And ow tnou tnou tnou tnou the to serth tnou tnou tnou tnou tnou tnou\n",
            "Iter: 7 - Batch: 11/184 - Loss: 1532512560.1811018\n",
            "Iter: 7 - Batch: 21/184 - Loss: 9.362414724844358e+17\n",
            "Iter: 7 - Batch: 31/184 - Loss: 2.0258030937653037e+27\n",
            "Iter: 7 - Batch: 41/184 - Loss: 8.442964420243652e+36\n",
            "Iter: 7 - Batch: 51/184 - Loss: 1.7476719172808348e+46\n",
            "Iter: 7 - Batch: 61/184 - Loss: 2.753399896141465e+55\n",
            "Iter: 7 - Batch: 71/184 - Loss: 1.8126859798972978e+65\n",
            "Iter: 7 - Batch: 81/184 - Loss: 1.3530210570154187e+75\n",
            "Iter: 7 - Batch: 91/184 - Loss: 4.638749683905591e+84\n",
            "Iter: 7 - Batch: 101/184 - Loss: 2.487909166063745e+94\n",
            "Iter: 7 - Batch: 111/184 - Loss: 1.892879631401119e+103\n",
            "Iter: 7 - Batch: 121/184 - Loss: 8.38488033298119e+112\n",
            "Iter: 7 - Batch: 131/184 - Loss: 5.7207833794482025e+122\n",
            "Iter: 7 - Batch: 141/184 - Loss: 1.1340984748441987e+132\n",
            "Iter: 7 - Batch: 151/184 - Loss: 2.0519647974376708e+141\n",
            "Iter: 7 - Batch: 161/184 - Loss: 3.715698841449773e+150\n",
            "Iter: 7 - Batch: 171/184 - Loss: 1.0850692848673986e+160\n",
            "Iter: 7 - Batch: 181/184 - Loss: 3.4520347547144927e+168\n",
            "Iter: 8 - Batch: 1/184 - Loss: 7.696179359724658 -  And tnou the to serne tnou then the to serth then then tnou then tnou\n",
            "Iter: 8 - Batch: 11/184 - Loss: 930395986.7107152\n",
            "Iter: 8 - Batch: 21/184 - Loss: 3.3653108901079277e+17\n",
            "Iter: 8 - Batch: 31/184 - Loss: 4.570854359291003e+26\n",
            "Iter: 8 - Batch: 41/184 - Loss: 1.2394136192268232e+36\n",
            "Iter: 8 - Batch: 51/184 - Loss: 1.6922845593202954e+45\n",
            "Iter: 8 - Batch: 61/184 - Loss: 1.848950374941707e+54\n",
            "Iter: 8 - Batch: 71/184 - Loss: 8.094207336828476e+63\n",
            "Iter: 8 - Batch: 81/184 - Loss: 3.782383110061866e+73\n",
            "Iter: 8 - Batch: 91/184 - Loss: 8.544679412816803e+82\n",
            "Iter: 8 - Batch: 101/184 - Loss: 2.8733519812982063e+92\n",
            "Iter: 8 - Batch: 111/184 - Loss: 1.5996454154224374e+101\n",
            "Iter: 8 - Batch: 121/184 - Loss: 4.097017501461216e+110\n",
            "Iter: 8 - Batch: 131/184 - Loss: 1.8531113551829724e+120\n",
            "Iter: 8 - Batch: 141/184 - Loss: 2.321166808983831e+129\n",
            "Iter: 8 - Batch: 151/184 - Loss: 2.8591873316853795e+138\n",
            "Iter: 8 - Batch: 161/184 - Loss: 3.611932931813282e+147\n",
            "Iter: 8 - Batch: 171/184 - Loss: 7.72831736532775e+156\n",
            "Iter: 8 - Batch: 181/184 - Loss: 1.5076698607776036e+165\n",
            "Iter: 9 - Batch: 1/184 - Loss: 7.508831695295423 -  And ow the to serth the to serth the to serth the to snow the to sert\n",
            "Iter: 9 - Batch: 11/184 - Loss: 587136339.1540356\n",
            "Iter: 9 - Batch: 21/184 - Loss: 1.3413395439405214e+17\n",
            "Iter: 9 - Batch: 31/184 - Loss: 1.2136838787883953e+26\n",
            "Iter: 9 - Batch: 41/184 - Loss: 2.1688852389849588e+35\n",
            "Iter: 9 - Batch: 51/184 - Loss: 2.0501645871257696e+44\n",
            "Iter: 9 - Batch: 61/184 - Loss: 1.620479058061839e+53\n",
            "Iter: 9 - Batch: 71/184 - Loss: 4.752164081384937e+62\n",
            "Iter: 9 - Batch: 81/184 - Loss: 1.4051186181226507e+72\n",
            "Iter: 9 - Batch: 91/184 - Loss: 2.152875422464874e+81\n",
            "Iter: 9 - Batch: 101/184 - Loss: 4.52452680175724e+90\n",
            "Iter: 9 - Batch: 111/184 - Loss: 1.8440905379652016e+99\n",
            "Iter: 9 - Batch: 121/184 - Loss: 2.8729477431040955e+108\n",
            "Iter: 9 - Batch: 131/184 - Loss: 8.75537489623665e+117\n",
            "Iter: 9 - Batch: 141/184 - Loss: 7.020372321558763e+126\n",
            "Iter: 9 - Batch: 151/184 - Loss: 6.231603151143155e+135\n",
            "Iter: 9 - Batch: 161/184 - Loss: 5.561131649724903e+144\n",
            "Iter: 9 - Batch: 171/184 - Loss: 8.481555181484267e+153\n",
            "Iter: 9 - Batch: 181/184 - Loss: 1.0532835309869258e+162\n",
            "Iter: 10 - Batch: 1/184 - Loss: 7.310525833411795 -  And now the to serth the to serth the to she to snow the to serne the\n",
            "Iter: 10 - Batch: 11/184 - Loss: 383341178.60953534\n",
            "Iter: 10 - Batch: 21/184 - Loss: 5.738392056247028e+16\n",
            "Iter: 10 - Batch: 31/184 - Loss: 3.662726253036416e+25\n",
            "Iter: 10 - Batch: 41/184 - Loss: 4.454372539838582e+34\n",
            "Iter: 10 - Batch: 51/184 - Loss: 3.0574086849403083e+43\n",
            "Iter: 10 - Batch: 61/184 - Loss: 1.775681460495227e+52\n",
            "Iter: 10 - Batch: 71/184 - Loss: 3.47774083167353e+61\n",
            "Iter: 10 - Batch: 81/184 - Loss: 6.628599889221476e+70\n",
            "Iter: 10 - Batch: 91/184 - Loss: 7.088448860820673e+79\n",
            "Iter: 10 - Batch: 101/184 - Loss: 9.519762360469164e+88\n",
            "Iter: 10 - Batch: 111/184 - Loss: 2.8263565043553168e+97\n",
            "Iter: 10 - Batch: 121/184 - Loss: 2.8668660407789347e+106\n",
            "Iter: 10 - Batch: 131/184 - Loss: 6.011761926036905e+115\n",
            "Iter: 10 - Batch: 141/184 - Loss: 3.1359575183820897e+124\n",
            "Iter: 10 - Batch: 151/184 - Loss: 2.1036445150838968e+133\n",
            "Iter: 10 - Batch: 161/184 - Loss: 1.3450980344292225e+142\n",
            "Iter: 10 - Batch: 171/184 - Loss: 1.430164699934844e+151\n",
            "Iter: 10 - Batch: 181/184 - Loss: 1.176188887429564e+159\n",
            "Iter: 11 - Batch: 1/184 - Loss: 7.125029778711158 -  And of the to she to serne the the to serne tnou the to snow the to s\n",
            "Iter: 11 - Batch: 11/184 - Loss: 255357498.53057635\n",
            "Iter: 11 - Batch: 21/184 - Loss: 2.6132764239868176e+16\n",
            "Iter: 11 - Batch: 31/184 - Loss: 1.2226243143148773e+25\n",
            "Iter: 11 - Batch: 41/184 - Loss: 1.0439822112390006e+34\n",
            "Iter: 11 - Batch: 51/184 - Loss: 5.35836221010882e+42\n",
            "Iter: 11 - Batch: 61/184 - Loss: 2.3036831912498937e+51\n",
            "Iter: 11 - Batch: 71/184 - Loss: 3.0276700159630248e+60\n",
            "Iter: 11 - Batch: 81/184 - Loss: 3.866782839025337e+69\n",
            "Iter: 11 - Batch: 91/184 - Loss: 2.976637358262007e+78\n",
            "Iter: 11 - Batch: 101/184 - Loss: 2.632279136661397e+87\n",
            "Iter: 11 - Batch: 111/184 - Loss: 5.676959735683072e+95\n",
            "Iter: 11 - Batch: 121/184 - Loss: 3.941989985239443e+104\n",
            "Iter: 11 - Batch: 131/184 - Loss: 5.806237786067142e+113\n",
            "Iter: 11 - Batch: 141/184 - Loss: 2.022806128993533e+122\n",
            "Iter: 11 - Batch: 151/184 - Loss: 1.0409609061020056e+131\n",
            "Iter: 11 - Batch: 161/184 - Loss: 4.8341963078469e+139\n",
            "Iter: 11 - Batch: 171/184 - Loss: 3.5257253016372183e+148\n",
            "Iter: 11 - Batch: 181/184 - Loss: 1.9840238379260765e+156\n",
            "Iter: 12 - Batch: 1/184 - Loss: 6.951617129879806 -  And now the to serne the now the to serne the now the to she to ser t\n",
            "Iter: 12 - Batch: 11/184 - Loss: 171365813.66694617\n",
            "Iter: 12 - Batch: 21/184 - Loss: 1.2398931622428662e+16\n",
            "Iter: 12 - Batch: 31/184 - Loss: 4.3328687009658046e+24\n",
            "Iter: 12 - Batch: 41/184 - Loss: 2.637761693813975e+33\n",
            "Iter: 12 - Batch: 51/184 - Loss: 1.0230241816537882e+42\n",
            "Iter: 12 - Batch: 61/184 - Loss: 3.2988384358915055e+50\n",
            "Iter: 12 - Batch: 71/184 - Loss: 2.940975803493988e+59\n",
            "Iter: 12 - Batch: 81/184 - Loss: 2.6206655356244397e+68\n",
            "Iter: 12 - Batch: 91/184 - Loss: 1.4865683923354214e+77\n",
            "Iter: 12 - Batch: 101/184 - Loss: 8.911292479447124e+85\n",
            "Iter: 12 - Batch: 111/184 - Loss: 1.4030123997687028e+94\n",
            "Iter: 12 - Batch: 121/184 - Loss: 6.900961559541376e+102\n",
            "Iter: 12 - Batch: 131/184 - Loss: 7.219084935169787e+111\n",
            "Iter: 12 - Batch: 141/184 - Loss: 1.7247353254355658e+120\n",
            "Iter: 12 - Batch: 151/184 - Loss: 6.790144834668548e+128\n",
            "Iter: 12 - Batch: 161/184 - Loss: 2.3113851829447254e+137\n",
            "Iter: 12 - Batch: 171/184 - Loss: 1.1517419101616307e+146\n",
            "Iter: 12 - Batch: 181/184 - Loss: 4.548991277017914e+153\n",
            "Iter: 13 - Batch: 1/184 - Loss: 6.748619000944496 -  And now the to ser the to shat our now the now the to serne tnou the \n",
            "Iter: 13 - Batch: 11/184 - Loss: 114269602.51584244\n",
            "Iter: 13 - Batch: 21/184 - Loss: 5950014783394366.0\n",
            "Iter: 13 - Batch: 31/184 - Loss: 1.5642604562315428e+24\n",
            "Iter: 13 - Batch: 41/184 - Loss: 6.786769364857257e+32\n",
            "Iter: 13 - Batch: 51/184 - Loss: 1.989795336825513e+41\n",
            "Iter: 13 - Batch: 61/184 - Loss: 4.809840751149674e+49\n",
            "Iter: 13 - Batch: 71/184 - Loss: 2.949562099338247e+58\n",
            "Iter: 13 - Batch: 81/184 - Loss: 1.8889792493193502e+67\n",
            "Iter: 13 - Batch: 91/184 - Loss: 8.079807905241702e+75\n",
            "Iter: 13 - Batch: 101/184 - Loss: 3.387458336959159e+84\n",
            "Iter: 13 - Batch: 111/184 - Loss: 3.930842540496663e+92\n",
            "Iter: 13 - Batch: 121/184 - Loss: 1.3960792561485307e+101\n",
            "Iter: 13 - Batch: 131/184 - Loss: 1.0283472011118373e+110\n",
            "Iter: 13 - Batch: 141/184 - Loss: 1.7264748092140715e+118\n",
            "Iter: 13 - Batch: 151/184 - Loss: 5.22212298989648e+126\n",
            "Iter: 13 - Batch: 161/184 - Loss: 1.3185135324337825e+135\n",
            "Iter: 13 - Batch: 171/184 - Loss: 4.489948010351771e+143\n",
            "Iter: 13 - Batch: 181/184 - Loss: 1.2708057357505194e+151\n",
            "Iter: 14 - Batch: 1/184 - Loss: 6.491151167767031 -  And of the now the now the now the to serne the cor the now the to se\n",
            "Iter: 14 - Batch: 11/184 - Loss: 75376133.1220075\n",
            "Iter: 14 - Batch: 21/184 - Loss: 2864639656445988.0\n",
            "Iter: 14 - Batch: 31/184 - Loss: 5.644463975054085e+23\n",
            "Iter: 14 - Batch: 41/184 - Loss: 1.74811325617606e+32\n",
            "Iter: 14 - Batch: 51/184 - Loss: 3.8435143739462904e+40\n",
            "Iter: 14 - Batch: 61/184 - Loss: 6.911319193202751e+48\n",
            "Iter: 14 - Batch: 71/184 - Loss: 2.9723798953234325e+57\n",
            "Iter: 14 - Batch: 81/184 - Loss: 1.3936139050943605e+66\n",
            "Iter: 14 - Batch: 91/184 - Loss: 4.605662349189162e+74\n",
            "Iter: 14 - Batch: 101/184 - Loss: 1.3838084339775708e+83\n",
            "Iter: 14 - Batch: 111/184 - Loss: 1.1895043632540392e+91\n",
            "Iter: 14 - Batch: 121/184 - Loss: 3.050718333362075e+99\n",
            "Iter: 14 - Batch: 131/184 - Loss: 1.5483031469206068e+108\n",
            "Iter: 14 - Batch: 141/184 - Loss: 1.8678978597335683e+116\n",
            "Iter: 14 - Batch: 151/184 - Loss: 4.389661817726844e+124\n",
            "Iter: 14 - Batch: 161/184 - Loss: 8.370994123087986e+132\n",
            "Iter: 14 - Batch: 171/184 - Loss: 1.958641720038845e+141\n",
            "Iter: 14 - Batch: 181/184 - Loss: 4.010091881586682e+148\n",
            "Iter: 15 - Batch: 1/184 - Loss: 6.209904890433505 -  And now the now the to ser the now the to ser the self on the now the\n",
            "Iter: 15 - Batch: 11/184 - Loss: 49554962.06688065\n",
            "Iter: 15 - Batch: 21/184 - Loss: 1389396972305098.0\n",
            "Iter: 15 - Batch: 31/184 - Loss: 2.018863783433629e+23\n",
            "Iter: 15 - Batch: 41/184 - Loss: 4.495971837946056e+31\n",
            "Iter: 15 - Batch: 51/184 - Loss: 7.332100210550335e+39\n",
            "Iter: 15 - Batch: 61/184 - Loss: 9.730707302619067e+47\n",
            "Iter: 15 - Batch: 71/184 - Loss: 2.9741781589816725e+56\n",
            "Iter: 15 - Batch: 81/184 - Loss: 1.029435526186469e+65\n",
            "Iter: 15 - Batch: 91/184 - Loss: 2.6821922408645342e+73\n",
            "Iter: 15 - Batch: 101/184 - Loss: 5.838714972717371e+81\n",
            "Iter: 15 - Batch: 111/184 - Loss: 3.708366600295771e+89\n",
            "Iter: 15 - Batch: 121/184 - Loss: 6.821603908568411e+97\n",
            "Iter: 15 - Batch: 131/184 - Loss: 2.3455092882697925e+106\n",
            "Iter: 15 - Batch: 141/184 - Loss: 2.0603960219868612e+114\n",
            "Iter: 15 - Batch: 151/184 - Loss: 3.815625166650975e+122\n",
            "Iter: 15 - Batch: 161/184 - Loss: 5.57216258340883e+130\n",
            "Iter: 15 - Batch: 171/184 - Loss: 9.036429887866703e+138\n",
            "Iter: 15 - Batch: 181/184 - Loss: 1.3360362013881691e+146\n",
            "Iter: 16 - Batch: 1/184 - Loss: 5.9418061665028645 -  And now the now the self ow the now the now the now the now the now t\n",
            "Iter: 16 - Batch: 11/184 - Loss: 32738190.155124616\n",
            "Iter: 16 - Batch: 21/184 - Loss: 683772630963148.6\n",
            "Iter: 16 - Batch: 31/184 - Loss: 7.178313184880761e+22\n",
            "Iter: 16 - Batch: 41/184 - Loss: 1.1549699177949146e+31\n",
            "Iter: 16 - Batch: 51/184 - Loss: 1.393333385630475e+39\n",
            "Iter: 16 - Batch: 61/184 - Loss: 1.345626086970596e+47\n",
            "Iter: 16 - Batch: 71/184 - Loss: 2.919525830080756e+55\n",
            "Iter: 16 - Batch: 81/184 - Loss: 7.437205289509692e+63\n",
            "Iter: 16 - Batch: 91/184 - Loss: 1.5394606265100598e+72\n",
            "Iter: 16 - Batch: 101/184 - Loss: 2.4169450881571577e+80\n",
            "Iter: 16 - Batch: 111/184 - Loss: 1.124566426761259e+88\n",
            "Iter: 16 - Batch: 121/184 - Loss: 1.481367387654732e+96\n",
            "Iter: 16 - Batch: 131/184 - Loss: 3.442468054986237e+104\n",
            "Iter: 16 - Batch: 141/184 - Loss: 2.204697636357607e+112\n",
            "Iter: 16 - Batch: 151/184 - Loss: 3.2200689474804864e+120\n",
            "Iter: 16 - Batch: 161/184 - Loss: 3.614140578111124e+128\n",
            "Iter: 16 - Batch: 171/184 - Loss: 4.110314382630792e+136\n",
            "Iter: 16 - Batch: 181/184 - Loss: 4.388366178185882e+143\n",
            "Iter: 17 - Batch: 1/184 - Loss: 5.703196005052408 -  And of the now the now the self of the now the to self of the to self\n",
            "Iter: 17 - Batch: 11/184 - Loss: 21959980.65709336\n",
            "Iter: 17 - Batch: 21/184 - Loss: 346328033985638.1\n",
            "Iter: 17 - Batch: 31/184 - Loss: 2.593779591821432e+22\n",
            "Iter: 17 - Batch: 41/184 - Loss: 3.011941685895585e+30\n",
            "Iter: 17 - Batch: 51/184 - Loss: 2.711626129055109e+38\n",
            "Iter: 17 - Batch: 61/184 - Loss: 1.8676016112945268e+46\n",
            "Iter: 17 - Batch: 71/184 - Loss: 2.842749228658178e+54\n",
            "Iter: 17 - Batch: 81/184 - Loss: 5.26975955410079e+62\n",
            "Iter: 17 - Batch: 91/184 - Loss: 8.598509903589922e+70\n",
            "Iter: 17 - Batch: 101/184 - Loss: 9.679689809863155e+78\n",
            "Iter: 17 - Batch: 111/184 - Loss: 3.272298422669011e+86\n",
            "Iter: 17 - Batch: 121/184 - Loss: 3.1036654421678087e+94\n",
            "Iter: 17 - Batch: 131/184 - Loss: 4.887161669400374e+102\n",
            "Iter: 17 - Batch: 141/184 - Loss: 2.278666781184178e+110\n",
            "Iter: 17 - Batch: 151/184 - Loss: 2.5859052646276742e+118\n",
            "Iter: 17 - Batch: 161/184 - Loss: 2.2122532590443265e+126\n",
            "Iter: 17 - Batch: 171/184 - Loss: 1.7887918954690953e+134\n",
            "Iter: 17 - Batch: 181/184 - Loss: 1.3870116618119355e+141\n",
            "Iter: 18 - Batch: 1/184 - Loss: 5.49800292876595 -  And of the to self of the to self of the to self on the to sen self o\n",
            "Iter: 18 - Batch: 11/184 - Loss: 15113432.104599297\n",
            "Iter: 18 - Batch: 21/184 - Loss: 181637178752758.44\n",
            "Iter: 18 - Batch: 31/184 - Loss: 9.671672645441724e+21\n",
            "Iter: 18 - Batch: 41/184 - Loss: 8.098580291957392e+29\n",
            "Iter: 18 - Batch: 51/184 - Loss: 5.519545117860571e+37\n",
            "Iter: 18 - Batch: 61/184 - Loss: 2.6696607072531547e+45\n",
            "Iter: 18 - Batch: 71/184 - Loss: 2.8218698639913137e+53\n",
            "Iter: 18 - Batch: 81/184 - Loss: 3.748936796331826e+61\n",
            "Iter: 18 - Batch: 91/184 - Loss: 4.74542221359906e+69\n",
            "Iter: 18 - Batch: 101/184 - Loss: 3.834093204845126e+77\n",
            "Iter: 18 - Batch: 111/184 - Loss: 9.39999069000881e+84\n",
            "Iter: 18 - Batch: 121/184 - Loss: 6.46066696535043e+92\n",
            "Iter: 18 - Batch: 131/184 - Loss: 6.89613155938989e+100\n",
            "Iter: 18 - Batch: 141/184 - Loss: 2.336903835619326e+108\n",
            "Iter: 18 - Batch: 151/184 - Loss: 2.0331849006407515e+116\n",
            "Iter: 18 - Batch: 161/184 - Loss: 1.3005470116815089e+124\n",
            "Iter: 18 - Batch: 171/184 - Loss: 7.533084346003619e+131\n",
            "Iter: 18 - Batch: 181/184 - Loss: 4.2782862114590487e+138\n",
            "Iter: 19 - Batch: 1/184 - Loss: 5.330568028800674 -  And of the to sen self of the now the to self of the now the to self \n",
            "Iter: 19 - Batch: 11/184 - Loss: 10647209.48563967\n",
            "Iter: 19 - Batch: 21/184 - Loss: 97358105013801.78\n",
            "Iter: 19 - Batch: 31/184 - Loss: 3.671348202785e+21\n",
            "Iter: 19 - Batch: 41/184 - Loss: 2.2298791280237173e+29\n",
            "Iter: 19 - Batch: 51/184 - Loss: 1.1547529689179714e+37\n",
            "Iter: 19 - Batch: 61/184 - Loss: 3.893010504807167e+44\n",
            "Iter: 19 - Batch: 71/184 - Loss: 2.844369809906304e+52\n",
            "Iter: 19 - Batch: 81/184 - Loss: 2.6590750157738156e+60\n",
            "Iter: 19 - Batch: 91/184 - Loss: 2.56977609657125e+68\n",
            "Iter: 19 - Batch: 101/184 - Loss: 1.4948811269375075e+76\n",
            "Iter: 19 - Batch: 111/184 - Loss: 2.6551008166634877e+83\n",
            "Iter: 19 - Batch: 121/184 - Loss: 1.3275229001457141e+91\n",
            "Iter: 19 - Batch: 131/184 - Loss: 9.643053357058192e+98\n",
            "Iter: 19 - Batch: 141/184 - Loss: 2.3650882822930287e+106\n",
            "Iter: 19 - Batch: 151/184 - Loss: 1.5576827278354567e+114\n",
            "Iter: 19 - Batch: 161/184 - Loss: 7.261246272560227e+121\n",
            "Iter: 19 - Batch: 171/184 - Loss: 3.0012763780831664e+129\n",
            "Iter: 19 - Batch: 181/184 - Loss: 1.261478903501644e+136\n",
            "Iter: 20 - Batch: 1/184 - Loss: 5.196551814538293 -  And of the to self of the to self of the to self of the to sen snow t\n",
            "Iter: 20 - Batch: 11/184 - Loss: 7594624.866580233\n",
            "Iter: 20 - Batch: 21/184 - Loss: 52159798849407.35\n",
            "Iter: 20 - Batch: 31/184 - Loss: 1.385779020599374e+21\n",
            "Iter: 20 - Batch: 41/184 - Loss: 6.17261035204674e+28\n",
            "Iter: 20 - Batch: 51/184 - Loss: 2.3919965061182133e+36\n",
            "Iter: 20 - Batch: 61/184 - Loss: 5.590777057392564e+43\n",
            "Iter: 20 - Batch: 71/184 - Loss: 2.8274206115077015e+51\n",
            "Iter: 20 - Batch: 81/184 - Loss: 1.8370049582852403e+59\n",
            "Iter: 20 - Batch: 91/184 - Loss: 1.335184766043731e+67\n",
            "Iter: 20 - Batch: 101/184 - Loss: 5.589715220844242e+74\n",
            "Iter: 20 - Batch: 111/184 - Loss: 7.183822225031434e+81\n",
            "Iter: 20 - Batch: 121/184 - Loss: 2.611153196904032e+89\n",
            "Iter: 20 - Batch: 131/184 - Loss: 1.3028400596445979e+97\n",
            "Iter: 20 - Batch: 141/184 - Loss: 2.301997985691653e+104\n",
            "Iter: 20 - Batch: 151/184 - Loss: 1.1323147612892016e+112\n",
            "Iter: 20 - Batch: 161/184 - Loss: 3.742307247794451e+119\n",
            "Iter: 20 - Batch: 171/184 - Loss: 1.083495804536325e+127\n",
            "Iter: 20 - Batch: 181/184 - Loss: 3.385779247412973e+133\n",
            "Iter: 21 - Batch: 1/184 - Loss: 5.084874448870331 -  The now do sen stor the to self of the to self of the to self of the \n",
            "Iter: 21 - Batch: 11/184 - Loss: 5409142.733420179\n",
            "Iter: 21 - Batch: 21/184 - Loss: 27408787655573.81\n",
            "Iter: 21 - Batch: 31/184 - Loss: 5.0960142926580607e+20\n",
            "Iter: 21 - Batch: 41/184 - Loss: 1.6902893398933297e+28\n",
            "Iter: 21 - Batch: 51/184 - Loss: 4.780378446757356e+35\n",
            "Iter: 21 - Batch: 61/184 - Loss: 7.673835146835515e+42\n",
            "Iter: 21 - Batch: 71/184 - Loss: 2.710310739156651e+50\n",
            "Iter: 21 - Batch: 81/184 - Loss: 1.2248751503164545e+58\n",
            "Iter: 21 - Batch: 91/184 - Loss: 6.602031804741494e+65\n",
            "Iter: 21 - Batch: 101/184 - Loss: 1.969476430256196e+73\n",
            "Iter: 21 - Batch: 111/184 - Loss: 1.840065078784619e+80\n",
            "Iter: 21 - Batch: 121/184 - Loss: 4.859060127899849e+87\n",
            "Iter: 21 - Batch: 131/184 - Loss: 1.6834841287826235e+95\n",
            "Iter: 21 - Batch: 141/184 - Loss: 2.135833771646509e+102\n",
            "Iter: 21 - Batch: 151/184 - Loss: 7.789432548212151e+109\n",
            "Iter: 21 - Batch: 161/184 - Loss: 1.7881743771825264e+117\n",
            "Iter: 21 - Batch: 171/184 - Loss: 3.5357213437629363e+124\n",
            "Iter: 21 - Batch: 181/184 - Loss: 8.196662503221811e+130\n",
            "Iter: 22 - Batch: 1/184 - Loss: 4.9806884533334665 -  And of the to self of the to self of the to self of the to self of th\n",
            "Iter: 22 - Batch: 11/184 - Loss: 3823874.051596652\n",
            "Iter: 22 - Batch: 21/184 - Loss: 14021537140317.467\n",
            "Iter: 22 - Batch: 31/184 - Loss: 1.8165488913873568e+20\n",
            "Iter: 22 - Batch: 41/184 - Loss: 4.5331208433984976e+27\n",
            "Iter: 22 - Batch: 51/184 - Loss: 9.128952851805695e+34\n",
            "Iter: 22 - Batch: 61/184 - Loss: 1.0010393450278912e+42\n",
            "Iter: 22 - Batch: 71/184 - Loss: 2.4953508731243786e+49\n",
            "Iter: 22 - Batch: 81/184 - Loss: 7.894336512040229e+56\n",
            "Iter: 22 - Batch: 91/184 - Loss: 3.1381881109775223e+64\n",
            "Iter: 22 - Batch: 101/184 - Loss: 6.668836478074565e+71\n",
            "Iter: 22 - Batch: 111/184 - Loss: 4.5547603969283e+78\n",
            "Iter: 22 - Batch: 121/184 - Loss: 8.785962617334795e+85\n",
            "Iter: 22 - Batch: 131/184 - Loss: 2.122872777043082e+93\n",
            "Iter: 22 - Batch: 141/184 - Loss: 1.930961095023328e+100\n",
            "Iter: 22 - Batch: 151/184 - Loss: 5.191112233334473e+107\n",
            "Iter: 22 - Batch: 161/184 - Loss: 8.195455036845931e+114\n",
            "Iter: 22 - Batch: 171/184 - Loss: 1.0852549039279063e+122\n",
            "Iter: 22 - Batch: 181/184 - Loss: 1.866287254306263e+128\n",
            "Iter: 23 - Batch: 1/184 - Loss: 4.8726124183969475 -  now do sen store on the to self of the to self of the to self of the \n",
            "Iter: 23 - Batch: 11/184 - Loss: 2692188.1845611497\n",
            "Iter: 23 - Batch: 21/184 - Loss: 6999538532433.268\n",
            "Iter: 23 - Batch: 31/184 - Loss: 6.344296208094328e+19\n",
            "Iter: 23 - Batch: 41/184 - Loss: 1.1940232610967265e+27\n",
            "Iter: 23 - Batch: 51/184 - Loss: 1.6614238993474926e+34\n",
            "Iter: 23 - Batch: 61/184 - Loss: 1.2452396274793851e+41\n",
            "Iter: 23 - Batch: 71/184 - Loss: 2.2050273251501642e+48\n",
            "Iter: 23 - Batch: 81/184 - Loss: 4.8913373111179907e+55\n",
            "Iter: 23 - Batch: 91/184 - Loss: 1.4331099033890878e+63\n",
            "Iter: 23 - Batch: 101/184 - Loss: 2.1850285150982123e+70\n",
            "Iter: 23 - Batch: 111/184 - Loss: 1.0987528890824483e+77\n",
            "Iter: 23 - Batch: 121/184 - Loss: 1.562259863918956e+84\n",
            "Iter: 23 - Batch: 131/184 - Loss: 2.6249577758642446e+91\n",
            "Iter: 23 - Batch: 141/184 - Loss: 1.7112120684717554e+98\n",
            "Iter: 23 - Batch: 151/184 - Loss: 3.352514426855123e+105\n",
            "Iter: 23 - Batch: 161/184 - Loss: 3.6039262749348146e+112\n",
            "Iter: 23 - Batch: 171/184 - Loss: 3.163929646775156e+119\n",
            "Iter: 23 - Batch: 181/184 - Loss: 4.0632676881611234e+125\n",
            "Iter: 24 - Batch: 1/184 - Loss: 4.756365585587904 -  Thn to seed, And now do ser snowing of the to ser sen stor the to sel\n",
            "Iter: 24 - Batch: 11/184 - Loss: 1892427.3818682795\n",
            "Iter: 24 - Batch: 21/184 - Loss: 3458880018565.332\n",
            "Iter: 24 - Batch: 31/184 - Loss: 2.196525636707127e+19\n",
            "Iter: 24 - Batch: 41/184 - Loss: 3.126234132050407e+26\n",
            "Iter: 24 - Batch: 51/184 - Loss: 2.9325562920991786e+33\n",
            "Iter: 24 - Batch: 61/184 - Loss: 1.5076344555623669e+40\n",
            "Iter: 24 - Batch: 71/184 - Loss: 1.899467535277501e+47\n",
            "Iter: 24 - Batch: 81/184 - Loss: 2.9390578425795944e+54\n",
            "Iter: 24 - Batch: 91/184 - Loss: 6.325728260509005e+61\n",
            "Iter: 24 - Batch: 101/184 - Loss: 6.986913044290846e+68\n",
            "Iter: 24 - Batch: 111/184 - Loss: 2.5982411501849722e+75\n",
            "Iter: 24 - Batch: 121/184 - Loss: 2.7576761096569257e+82\n",
            "Iter: 24 - Batch: 131/184 - Loss: 3.1893711876401995e+89\n",
            "Iter: 24 - Batch: 141/184 - Loss: 1.4890639048726327e+96\n",
            "Iter: 24 - Batch: 151/184 - Loss: 2.085882098061456e+103\n",
            "Iter: 24 - Batch: 161/184 - Loss: 1.487406168490923e+110\n",
            "Iter: 24 - Batch: 171/184 - Loss: 8.586576885834117e+116\n",
            "Iter: 24 - Batch: 181/184 - Loss: 8.316934723544774e+122\n",
            "Iter: 25 - Batch: 1/184 - Loss: 4.627261160194555 -  now do ser snown out own now do ser store of thee on the to sen store\n",
            "Iter: 25 - Batch: 11/184 - Loss: 1326021.2160374098\n",
            "Iter: 25 - Batch: 21/184 - Loss: 1710368076044.284\n",
            "Iter: 25 - Batch: 31/184 - Loss: 7.601975521351728e+18\n",
            "Iter: 25 - Batch: 41/184 - Loss: 8.156703565331498e+25\n",
            "Iter: 25 - Batch: 51/184 - Loss: 5.09441761515178e+32\n",
            "Iter: 25 - Batch: 61/184 - Loss: 1.8026608671564834e+39\n",
            "Iter: 25 - Batch: 71/184 - Loss: 1.5920858700520694e+46\n",
            "Iter: 25 - Batch: 81/184 - Loss: 1.7142703252030064e+53\n",
            "Iter: 25 - Batch: 91/184 - Loss: 2.6943352251650823e+60\n",
            "Iter: 25 - Batch: 101/184 - Loss: 2.1691917120099472e+67\n",
            "Iter: 25 - Batch: 111/184 - Loss: 5.968113013291387e+73\n",
            "Iter: 25 - Batch: 121/184 - Loss: 4.789870827352664e+80\n",
            "Iter: 25 - Batch: 131/184 - Loss: 3.746114241498287e+87\n",
            "Iter: 25 - Batch: 141/184 - Loss: 1.2423817682512507e+94\n",
            "Iter: 25 - Batch: 151/184 - Loss: 1.2133788021537678e+101\n",
            "Iter: 25 - Batch: 161/184 - Loss: 5.548052276623012e+107\n",
            "Iter: 25 - Batch: 171/184 - Loss: 2.0975635893130674e+114\n",
            "Iter: 25 - Batch: 181/184 - Loss: 1.513697270212342e+120\n",
            "Iter: 26 - Batch: 1/184 - Loss: 4.494174525771442 -  Thn to seed, And to self now do sen store of then the to self of thee\n",
            "Iter: 26 - Batch: 11/184 - Loss: 935742.4906683147\n",
            "Iter: 26 - Batch: 21/184 - Loss: 848926173274.9386\n",
            "Iter: 26 - Batch: 31/184 - Loss: 2.641038636393614e+18\n",
            "Iter: 26 - Batch: 41/184 - Loss: 2.111784476261394e+25\n",
            "Iter: 26 - Batch: 51/184 - Loss: 8.741701483577538e+31\n",
            "Iter: 26 - Batch: 61/184 - Loss: 2.1041697974113273e+38\n",
            "Iter: 26 - Batch: 71/184 - Loss: 1.2695275785842972e+45\n",
            "Iter: 26 - Batch: 81/184 - Loss: 9.494299508782976e+51\n",
            "Iter: 26 - Batch: 91/184 - Loss: 1.0837904246980337e+59\n",
            "Iter: 26 - Batch: 101/184 - Loss: 6.348870294223267e+65\n",
            "Iter: 26 - Batch: 111/184 - Loss: 1.291812686597778e+72\n",
            "Iter: 26 - Batch: 121/184 - Loss: 7.893224807196542e+78\n",
            "Iter: 26 - Batch: 131/184 - Loss: 4.112928877831663e+85\n",
            "Iter: 26 - Batch: 141/184 - Loss: 9.556448391792372e+91\n",
            "Iter: 26 - Batch: 151/184 - Loss: 6.356228773019189e+98\n",
            "Iter: 26 - Batch: 161/184 - Loss: 1.7994407212149082e+105\n",
            "Iter: 26 - Batch: 171/184 - Loss: 4.431724682159311e+111\n",
            "Iter: 26 - Batch: 181/184 - Loss: 2.3281851638084403e+117\n",
            "Iter: 27 - Batch: 1/184 - Loss: 4.377097652133901 -  To seed, And to self lover out of thee of thee of thee of thee of the\n",
            "Iter: 27 - Batch: 11/184 - Loss: 663174.9838138975\n",
            "Iter: 27 - Batch: 21/184 - Loss: 416342826933.6465\n",
            "Iter: 27 - Batch: 31/184 - Loss: 8.989792590496946e+17\n",
            "Iter: 27 - Batch: 41/184 - Loss: 5.326328867517714e+24\n",
            "Iter: 27 - Batch: 51/184 - Loss: 1.4698734253543379e+31\n",
            "Iter: 27 - Batch: 61/184 - Loss: 2.3685278754650456e+37\n",
            "Iter: 27 - Batch: 71/184 - Loss: 9.60034681606212e+43\n",
            "Iter: 27 - Batch: 81/184 - Loss: 4.890161311399387e+50\n",
            "Iter: 27 - Batch: 91/184 - Loss: 4.04652854601815e+57\n",
            "Iter: 27 - Batch: 101/184 - Loss: 1.7298615960443567e+64\n",
            "Iter: 27 - Batch: 111/184 - Loss: 2.5866426643836038e+70\n",
            "Iter: 27 - Batch: 121/184 - Loss: 1.1901946828941466e+77\n",
            "Iter: 27 - Batch: 131/184 - Loss: 4.087078548639034e+83\n",
            "Iter: 27 - Batch: 141/184 - Loss: 6.526394612651336e+89\n",
            "Iter: 27 - Batch: 151/184 - Loss: 2.872328585532344e+96\n",
            "Iter: 27 - Batch: 161/184 - Loss: 4.863112522147839e+102\n",
            "Iter: 27 - Batch: 171/184 - Loss: 7.712001281300863e+108\n",
            "Iter: 27 - Batch: 181/184 - Loss: 2.91714866047135e+114\n",
            "Iter: 28 - Batch: 1/184 - Loss: 4.273062605051787 -  To seed, And now do self lover seed, And do seed, And now do self lov\n",
            "Iter: 28 - Batch: 11/184 - Loss: 464138.4361963786\n",
            "Iter: 28 - Batch: 21/184 - Loss: 201453853570.00156\n",
            "Iter: 28 - Batch: 31/184 - Loss: 2.971204854286859e+17\n",
            "Iter: 28 - Batch: 41/184 - Loss: 1.3026938150299366e+24\n",
            "Iter: 28 - Batch: 51/184 - Loss: 2.445608252792876e+30\n",
            "Iter: 28 - Batch: 61/184 - Loss: 2.6287463795679475e+36\n",
            "Iter: 28 - Batch: 71/184 - Loss: 7.119415372528056e+42\n",
            "Iter: 28 - Batch: 81/184 - Loss: 2.394820155391408e+49\n",
            "Iter: 28 - Batch: 91/184 - Loss: 1.4326911300973568e+56\n",
            "Iter: 28 - Batch: 101/184 - Loss: 4.499408039683186e+62\n",
            "Iter: 28 - Batch: 111/184 - Loss: 4.89653239692919e+68\n",
            "Iter: 28 - Batch: 121/184 - Loss: 1.6568776483618704e+75\n",
            "Iter: 28 - Batch: 131/184 - Loss: 3.6627805401590706e+81\n",
            "Iter: 28 - Batch: 141/184 - Loss: 3.907335861698461e+87\n",
            "Iter: 28 - Batch: 151/184 - Loss: 1.1064827575346258e+94\n",
            "Iter: 28 - Batch: 161/184 - Loss: 1.1045308315096988e+100\n",
            "Iter: 28 - Batch: 171/184 - Loss: 1.113533331650261e+106\n",
            "Iter: 28 - Batch: 181/184 - Loss: 3.035133374000852e+111\n",
            "Iter: 29 - Batch: 1/184 - Loss: 4.172188302616402 -  To seed, And now do self lover seen, And now do self lover snat to se\n",
            "Iter: 29 - Batch: 11/184 - Loss: 320525.0179581697\n",
            "Iter: 29 - Batch: 21/184 - Loss: 96468928500.24489\n",
            "Iter: 29 - Batch: 31/184 - Loss: 9.553157842624322e+16\n",
            "Iter: 29 - Batch: 41/184 - Loss: 3.075749040116083e+23\n",
            "Iter: 29 - Batch: 51/184 - Loss: 3.9867473834040666e+29\n",
            "Iter: 29 - Batch: 61/184 - Loss: 2.8678480148650053e+35\n",
            "Iter: 29 - Batch: 71/184 - Loss: 5.163977396405227e+41\n",
            "Iter: 29 - Batch: 81/184 - Loss: 1.1202724791392946e+48\n",
            "Iter: 29 - Batch: 91/184 - Loss: 4.786274981855349e+54\n",
            "Iter: 29 - Batch: 101/184 - Loss: 1.0982248373692867e+61\n",
            "Iter: 29 - Batch: 111/184 - Loss: 8.64760006370386e+66\n",
            "Iter: 29 - Batch: 121/184 - Loss: 2.081337387286652e+73\n",
            "Iter: 29 - Batch: 131/184 - Loss: 2.87860020993055e+79\n",
            "Iter: 29 - Batch: 141/184 - Loss: 1.99151009724006e+85\n",
            "Iter: 29 - Batch: 151/184 - Loss: 3.56206375275957e+91\n",
            "Iter: 29 - Batch: 161/184 - Loss: 2.1138139193560863e+97\n",
            "Iter: 29 - Batch: 171/184 - Loss: 1.334438244122207e+103\n",
            "Iter: 29 - Batch: 181/184 - Loss: 2.635948825279828e+108\n",
            "Iter: 30 - Batch: 1/184 - Loss: 4.062984571753738 -  To seed, And to seed, And store on thee of thee of thee of thee on th\n",
            "Iter: 30 - Batch: 11/184 - Loss: 217314.92043878057\n",
            "Iter: 30 - Batch: 21/184 - Loss: 45120912062.807236\n",
            "Iter: 30 - Batch: 31/184 - Loss: 2.9248208329253772e+16\n",
            "Iter: 30 - Batch: 41/184 - Loss: 6.879701780624685e+22\n",
            "Iter: 30 - Batch: 51/184 - Loss: 6.132883210215762e+28\n",
            "Iter: 30 - Batch: 61/184 - Loss: 2.9786119534235357e+34\n",
            "Iter: 30 - Batch: 71/184 - Loss: 3.536838310569776e+40\n",
            "Iter: 30 - Batch: 81/184 - Loss: 4.881076442149227e+46\n",
            "Iter: 30 - Batch: 91/184 - Loss: 1.455660712628787e+53\n",
            "Iter: 30 - Batch: 101/184 - Loss: 2.3812748608795846e+59\n",
            "Iter: 30 - Batch: 111/184 - Loss: 1.3527725921611122e+65\n",
            "Iter: 30 - Batch: 121/184 - Loss: 2.2364086448492413e+71\n",
            "Iter: 30 - Batch: 131/184 - Loss: 1.9024792046601145e+77\n",
            "Iter: 30 - Batch: 141/184 - Loss: 8.459593009532223e+82\n",
            "Iter: 30 - Batch: 151/184 - Loss: 9.534088834764373e+88\n",
            "Iter: 30 - Batch: 161/184 - Loss: 3.380523264246824e+94\n",
            "Iter: 30 - Batch: 171/184 - Loss: 1.3141725268037539e+100\n",
            "Iter: 30 - Batch: 181/184 - Loss: 1.8926279840268855e+105\n",
            "Iter: 31 - Batch: 1/184 - Loss: 3.934482312378941 -  To seed, And snows to seed, And snat to seed, And so sen store on the\n",
            "Iter: 31 - Batch: 11/184 - Loss: 143318.29639431363\n",
            "Iter: 31 - Batch: 21/184 - Loss: 20352261000.779438\n",
            "Iter: 31 - Batch: 31/184 - Loss: 8392096836146780.0\n",
            "Iter: 31 - Batch: 41/184 - Loss: 1.4371479303370006e+22\n",
            "Iter: 31 - Batch: 51/184 - Loss: 8.605783026558444e+27\n",
            "Iter: 31 - Batch: 61/184 - Loss: 2.851952434534412e+33\n",
            "Iter: 31 - Batch: 71/184 - Loss: 2.226037768455063e+39\n",
            "Iter: 31 - Batch: 81/184 - Loss: 1.9346501555767797e+45\n",
            "Iter: 31 - Batch: 91/184 - Loss: 3.902883276305335e+51\n",
            "Iter: 31 - Batch: 101/184 - Loss: 4.4067813433654634e+57\n",
            "Iter: 31 - Batch: 111/184 - Loss: 1.7845034159099754e+63\n",
            "Iter: 31 - Batch: 121/184 - Loss: 1.9856465426620756e+69\n",
            "Iter: 31 - Batch: 131/184 - Loss: 1.0233984144394917e+75\n",
            "Iter: 31 - Batch: 141/184 - Loss: 2.9188222006328367e+80\n",
            "Iter: 31 - Batch: 151/184 - Loss: 2.087197966994753e+86\n",
            "Iter: 31 - Batch: 161/184 - Loss: 4.366653076379231e+91\n",
            "Iter: 31 - Batch: 171/184 - Loss: 1.0316670046383532e+97\n",
            "Iter: 31 - Batch: 181/184 - Loss: 1.0847613838513794e+102\n",
            "Iter: 32 - Batch: 1/184 - Loss: 3.783526066284638 -  To seen, To seed, And store of thee of thee on thee of thee of thee o\n",
            "Iter: 32 - Batch: 11/184 - Loss: 92202.71167092107\n",
            "Iter: 32 - Batch: 21/184 - Loss: 8953481446.202957\n",
            "Iter: 32 - Batch: 31/184 - Loss: 2310438690247477.5\n",
            "Iter: 32 - Batch: 41/184 - Loss: 2.852506872917781e+21\n",
            "Iter: 32 - Batch: 51/184 - Loss: 1.1412300365607462e+27\n",
            "Iter: 32 - Batch: 61/184 - Loss: 2.583327476086465e+32\n",
            "Iter: 32 - Batch: 71/184 - Loss: 1.3361411565481114e+38\n",
            "Iter: 32 - Batch: 81/184 - Loss: 7.24579926361892e+43\n",
            "Iter: 32 - Batch: 91/184 - Loss: 9.584721201754525e+49\n",
            "Iter: 32 - Batch: 101/184 - Loss: 7.148411333444695e+55\n",
            "Iter: 32 - Batch: 111/184 - Loss: 2.0092399881703127e+61\n",
            "Iter: 32 - Batch: 121/184 - Loss: 1.4979236605561723e+67\n",
            "Iter: 32 - Batch: 131/184 - Loss: 4.62728818966947e+72\n",
            "Iter: 32 - Batch: 141/184 - Loss: 8.297449025889941e+77\n",
            "Iter: 32 - Batch: 151/184 - Loss: 3.741593075512384e+83\n",
            "Iter: 32 - Batch: 161/184 - Loss: 4.5109460517853136e+88\n",
            "Iter: 32 - Batch: 171/184 - Loss: 6.435233628581412e+93\n",
            "Iter: 32 - Batch: 181/184 - Loss: 4.8816393975727253e+98\n",
            "Iter: 33 - Batch: 1/184 - Loss: 3.621486272785106 -  To seeis own now disdectice now dath thee of thee on thee of thee of \n",
            "Iter: 33 - Batch: 11/184 - Loss: 57971.37094348332\n",
            "Iter: 33 - Batch: 21/184 - Loss: 3812527975.0141544\n",
            "Iter: 33 - Batch: 31/184 - Loss: 605989747742098.9\n",
            "Iter: 33 - Batch: 41/184 - Loss: 5.3344317557663465e+20\n",
            "Iter: 33 - Batch: 51/184 - Loss: 1.4286735404490948e+26\n",
            "Iter: 33 - Batch: 61/184 - Loss: 2.1938041884783237e+31\n",
            "Iter: 33 - Batch: 71/184 - Loss: 7.555074429626256e+36\n",
            "Iter: 33 - Batch: 81/184 - Loss: 2.5475180423937438e+42\n",
            "Iter: 33 - Batch: 91/184 - Loss: 2.1457901385031324e+48\n",
            "Iter: 33 - Batch: 101/184 - Loss: 1.0021240970133057e+54\n",
            "Iter: 33 - Batch: 111/184 - Loss: 1.8978640514370926e+59\n",
            "Iter: 33 - Batch: 121/184 - Loss: 9.461596437995982e+64\n",
            "Iter: 33 - Batch: 131/184 - Loss: 1.767978811577484e+70\n",
            "Iter: 33 - Batch: 141/184 - Loss: 1.957035481518511e+75\n",
            "Iter: 33 - Batch: 151/184 - Loss: 5.517916468671514e+80\n",
            "Iter: 33 - Batch: 161/184 - Loss: 3.8326667935454615e+85\n",
            "Iter: 33 - Batch: 171/184 - Loss: 3.297972509489991e+90\n",
            "Iter: 33 - Batch: 181/184 - Loss: 1.7487541959614605e+95\n",
            "Iter: 34 - Batch: 1/184 - Loss: 3.4633013223336246 -  To seeis own con thee now disdess night thee of thee of thee on thee \n",
            "Iter: 34 - Batch: 11/184 - Loss: 35956.55082823345\n",
            "Iter: 34 - Batch: 21/184 - Loss: 1552966075.1401541\n",
            "Iter: 34 - Batch: 31/184 - Loss: 148863214906490.0\n",
            "Iter: 34 - Batch: 41/184 - Loss: 9.021891857514478e+19\n",
            "Iter: 34 - Batch: 51/184 - Loss: 1.607531842685775e+25\n",
            "Iter: 34 - Batch: 61/184 - Loss: 1.6774825988347093e+30\n",
            "Iter: 34 - Batch: 71/184 - Loss: 3.797745234832957e+35\n",
            "Iter: 34 - Batch: 81/184 - Loss: 7.977932634455142e+40\n",
            "Iter: 34 - Batch: 91/184 - Loss: 4.1883115451375764e+46\n",
            "Iter: 34 - Batch: 101/184 - Loss: 1.190174229990813e+52\n",
            "Iter: 34 - Batch: 111/184 - Loss: 1.4829826116808217e+57\n",
            "Iter: 34 - Batch: 121/184 - Loss: 4.865562743021913e+62\n",
            "Iter: 34 - Batch: 131/184 - Loss: 5.653623669277936e+67\n",
            "Iter: 34 - Batch: 141/184 - Loss: 3.82871150441727e+72\n",
            "Iter: 34 - Batch: 151/184 - Loss: 6.684316938574843e+77\n",
            "Iter: 34 - Batch: 161/184 - Loss: 2.7757317840426078e+82\n",
            "Iter: 34 - Batch: 171/184 - Loss: 1.4629009261886062e+87\n",
            "Iter: 34 - Batch: 181/184 - Loss: 5.1923437272702675e+91\n",
            "Iter: 35 - Batch: 1/184 - Loss: 3.322356285294054 -  To seeis own compnace would of thee all night thee stan self all nate\n",
            "Iter: 35 - Batch: 11/184 - Loss: 22241.265529694618\n",
            "Iter: 35 - Batch: 21/184 - Loss: 615232886.8788248\n",
            "Iter: 35 - Batch: 31/184 - Loss: 34930355596704.85\n",
            "Iter: 35 - Batch: 41/184 - Loss: 1.3576929977761968e+19\n",
            "Iter: 35 - Batch: 51/184 - Loss: 1.604382991989144e+24\n",
            "Iter: 35 - Batch: 61/184 - Loss: 1.1369566287319287e+29\n",
            "Iter: 35 - Batch: 71/184 - Loss: 1.6599950245409507e+34\n",
            "Iter: 35 - Batch: 81/184 - Loss: 2.1906732060079167e+39\n",
            "Iter: 35 - Batch: 91/184 - Loss: 7.013600329661436e+44\n",
            "Iter: 35 - Batch: 101/184 - Loss: 1.2294244785054792e+50\n",
            "Iter: 35 - Batch: 111/184 - Loss: 9.893037817871286e+54\n",
            "Iter: 35 - Batch: 121/184 - Loss: 2.0903664945436298e+60\n",
            "Iter: 35 - Batch: 131/184 - Loss: 1.5519162222359364e+65\n",
            "Iter: 35 - Batch: 141/184 - Loss: 6.3806909741020554e+69\n",
            "Iter: 35 - Batch: 151/184 - Loss: 6.727378451136841e+74\n",
            "Iter: 35 - Batch: 161/184 - Loss: 1.6850043012347245e+79\n",
            "Iter: 35 - Batch: 171/184 - Loss: 5.405606885783057e+83\n",
            "Iter: 35 - Batch: 181/184 - Loss: 1.236803733651736e+88\n",
            "Iter: 36 - Batch: 1/184 - Loss: 3.1978278575467667 -  To seeied of thee stat our stall of thee on thee stat our stan ser st\n",
            "Iter: 36 - Batch: 11/184 - Loss: 13696.133278537764\n",
            "Iter: 36 - Batch: 21/184 - Loss: 243076353.27915198\n",
            "Iter: 36 - Batch: 31/184 - Loss: 8061128614293.033\n",
            "Iter: 36 - Batch: 41/184 - Loss: 1.9066790032483594e+18\n",
            "Iter: 36 - Batch: 51/184 - Loss: 1.4807187428242475e+23\n",
            "Iter: 36 - Batch: 61/184 - Loss: 6.96293647277256e+27\n",
            "Iter: 36 - Batch: 71/184 - Loss: 6.4374869878799715e+32\n",
            "Iter: 36 - Batch: 81/184 - Loss: 5.309930667453008e+37\n",
            "Iter: 36 - Batch: 91/184 - Loss: 9.835455910723364e+42\n",
            "Iter: 36 - Batch: 101/184 - Loss: 1.0946080676982936e+48\n",
            "Iter: 36 - Batch: 111/184 - Loss: 5.644090634705538e+52\n",
            "Iter: 36 - Batch: 121/184 - Loss: 7.589758013572279e+57\n",
            "Iter: 36 - Batch: 131/184 - Loss: 3.6804202524249774e+62\n",
            "Iter: 36 - Batch: 141/184 - Loss: 9.29686971947819e+66\n",
            "Iter: 36 - Batch: 151/184 - Loss: 5.783371370444712e+71\n",
            "Iter: 36 - Batch: 161/184 - Loss: 8.527356203580416e+75\n",
            "Iter: 36 - Batch: 171/184 - Loss: 1.5843600877444835e+80\n",
            "Iter: 36 - Batch: 181/184 - Loss: 2.29365038910537e+84\n",
            "Iter: 37 - Batch: 1/184 - Loss: 3.0788790447062198 -  To seeied now dnown now dath thee stat now dath thee stat our stall o\n",
            "Iter: 37 - Batch: 11/184 - Loss: 8457.518264197224\n",
            "Iter: 37 - Batch: 21/184 - Loss: 98366280.82910733\n",
            "Iter: 37 - Batch: 31/184 - Loss: 1842583766651.119\n",
            "Iter: 37 - Batch: 41/184 - Loss: 2.5836580852336275e+17\n",
            "Iter: 37 - Batch: 51/184 - Loss: 1.2853141113692091e+22\n",
            "Iter: 37 - Batch: 61/184 - Loss: 3.9524437732385695e+26\n",
            "Iter: 37 - Batch: 71/184 - Loss: 2.2702880602657206e+31\n",
            "Iter: 37 - Batch: 81/184 - Loss: 1.1528899868479135e+36\n",
            "Iter: 37 - Batch: 91/184 - Loss: 1.1554191880555258e+41\n",
            "Iter: 37 - Batch: 101/184 - Loss: 8.296406186075577e+45\n",
            "Iter: 37 - Batch: 111/184 - Loss: 2.731244191311671e+50\n",
            "Iter: 37 - Batch: 121/184 - Loss: 2.347033971574664e+55\n",
            "Iter: 37 - Batch: 131/184 - Loss: 7.66509992756797e+59\n",
            "Iter: 37 - Batch: 141/184 - Loss: 1.2179039010943902e+64\n",
            "Iter: 37 - Batch: 151/184 - Loss: 4.474418924711541e+68\n",
            "Iter: 37 - Batch: 161/184 - Loss: 3.899585071347767e+72\n",
            "Iter: 37 - Batch: 171/184 - Loss: 4.3027470521056356e+76\n",
            "Iter: 37 - Batch: 181/184 - Loss: 3.9589213768181774e+80\n",
            "Iter: 38 - Batch: 1/184 - Loss: 2.972690890066883 -  To seeied warture own to seeied warture ow thee stat our stall of me \n",
            "Iter: 38 - Batch: 11/184 - Loss: 5371.466782603315\n",
            "Iter: 38 - Batch: 21/184 - Loss: 40887782.09318631\n",
            "Iter: 38 - Batch: 31/184 - Loss: 441915208652.4148\n",
            "Iter: 38 - Batch: 41/184 - Loss: 3.5694102495666564e+16\n",
            "Iter: 38 - Batch: 51/184 - Loss: 1.1312091380234778e+21\n",
            "Iter: 38 - Batch: 61/184 - Loss: 2.262955251941964e+25\n",
            "Iter: 38 - Batch: 71/184 - Loss: 7.918022878035355e+29\n",
            "Iter: 38 - Batch: 81/184 - Loss: 2.3579681400502235e+34\n",
            "Iter: 38 - Batch: 91/184 - Loss: 1.2709252269189281e+39\n",
            "Iter: 38 - Batch: 101/184 - Loss: 5.719425605164557e+43\n",
            "Iter: 38 - Batch: 111/184 - Loss: 1.1790207104646679e+48\n",
            "Iter: 38 - Batch: 121/184 - Loss: 6.11888924180324e+52\n",
            "Iter: 38 - Batch: 131/184 - Loss: 1.3769059315803777e+57\n",
            "Iter: 38 - Batch: 141/184 - Loss: 1.4020471273801362e+61\n",
            "Iter: 38 - Batch: 151/184 - Loss: 2.9249603519615514e+65\n",
            "Iter: 38 - Batch: 161/184 - Loss: 1.5278128741276583e+69\n",
            "Iter: 38 - Batch: 171/184 - Loss: 1.0858499692973472e+73\n",
            "Iter: 38 - Batch: 181/184 - Loss: 6.47731825838372e+76\n",
            "Iter: 39 - Batch: 1/184 - Loss: 2.9009223929683565 -  To seeied warture ow thee stresher hate of mine, O holder true of the\n",
            "Iter: 39 - Batch: 11/184 - Loss: 3455.954523524597\n",
            "Iter: 39 - Batch: 21/184 - Loss: 17034961.659164358\n",
            "Iter: 39 - Batch: 31/184 - Loss: 113158129041.93944\n",
            "Iter: 39 - Batch: 41/184 - Loss: 5024839276632855.0\n",
            "Iter: 39 - Batch: 51/184 - Loss: 1.0159464272108968e+20\n",
            "Iter: 39 - Batch: 61/184 - Loss: 1.3039198828817904e+24\n",
            "Iter: 39 - Batch: 71/184 - Loss: 2.6600964799719574e+28\n",
            "Iter: 39 - Batch: 81/184 - Loss: 4.454221461537823e+32\n",
            "Iter: 39 - Batch: 91/184 - Loss: 1.3654419065575345e+37\n",
            "Iter: 39 - Batch: 101/184 - Loss: 3.637862432447651e+41\n",
            "Iter: 39 - Batch: 111/184 - Loss: 4.626323547408119e+45\n",
            "Iter: 39 - Batch: 121/184 - Loss: 1.3877827597241859e+50\n",
            "Iter: 39 - Batch: 131/184 - Loss: 2.147509244103702e+54\n",
            "Iter: 39 - Batch: 141/184 - Loss: 1.4624297062834615e+58\n",
            "Iter: 39 - Batch: 151/184 - Loss: 1.772072817046362e+62\n",
            "Iter: 39 - Batch: 161/184 - Loss: 5.242875354656795e+65\n",
            "Iter: 39 - Batch: 171/184 - Loss: 2.5490853374053045e+69\n",
            "Iter: 39 - Batch: 181/184 - Loss: 9.877137078314286e+72\n",
            "Iter: 40 - Batch: 1/184 - Loss: 2.8265545009843245 -  To seen, Whill'st, And do seeks night thee stresher nn seeks night th\n",
            "Iter: 40 - Batch: 11/184 - Loss: 2160.0004719845388\n",
            "Iter: 40 - Batch: 21/184 - Loss: 7035826.119419934\n",
            "Iter: 40 - Batch: 31/184 - Loss: 30554900785.076103\n",
            "Iter: 40 - Batch: 41/184 - Loss: 739216708479057.1\n",
            "Iter: 40 - Batch: 51/184 - Loss: 9.468675949253138e+18\n",
            "Iter: 40 - Batch: 61/184 - Loss: 7.565103197943538e+22\n",
            "Iter: 40 - Batch: 71/184 - Loss: 8.702184764633076e+26\n",
            "Iter: 40 - Batch: 81/184 - Loss: 8.196154552968462e+30\n",
            "Iter: 40 - Batch: 91/184 - Loss: 1.430885756162423e+35\n",
            "Iter: 40 - Batch: 101/184 - Loss: 2.1759901191977983e+39\n",
            "Iter: 40 - Batch: 111/184 - Loss: 1.7020213404814146e+43\n",
            "Iter: 40 - Batch: 121/184 - Loss: 2.9364884784134502e+47\n",
            "Iter: 40 - Batch: 131/184 - Loss: 3.052655540532381e+51\n",
            "Iter: 40 - Batch: 141/184 - Loss: 1.3392716134542525e+55\n",
            "Iter: 40 - Batch: 151/184 - Loss: 9.661972998680823e+58\n",
            "Iter: 40 - Batch: 161/184 - Loss: 1.6515032633173132e+62\n",
            "Iter: 40 - Batch: 171/184 - Loss: 5.477173469130921e+65\n",
            "Iter: 40 - Batch: 181/184 - Loss: 1.351325099965308e+69\n",
            "Iter: 41 - Batch: 1/184 - Loss: 2.7348566111919417 -  To seen, Whill'st, now natness disdece of my lave's arrow dather thee\n",
            "Iter: 41 - Batch: 11/184 - Loss: 1334.8951578197343\n",
            "Iter: 41 - Batch: 21/184 - Loss: 2893115.898867946\n",
            "Iter: 41 - Batch: 31/184 - Loss: 7963001043.347695\n",
            "Iter: 41 - Batch: 41/184 - Loss: 106743920930114.94\n",
            "Iter: 41 - Batch: 51/184 - Loss: 8.242096881083315e+17\n",
            "Iter: 41 - Batch: 61/184 - Loss: 4.204982862502462e+21\n",
            "Iter: 41 - Batch: 71/184 - Loss: 2.7496169306250797e+25\n",
            "Iter: 41 - Batch: 81/184 - Loss: 1.4630862396126703e+29\n",
            "Iter: 41 - Batch: 91/184 - Loss: 1.4588259477522776e+33\n",
            "Iter: 41 - Batch: 101/184 - Loss: 1.3079690791738707e+37\n",
            "Iter: 41 - Batch: 111/184 - Loss: 6.227344810378235e+40\n",
            "Iter: 41 - Batch: 121/184 - Loss: 6.2230113959312785e+44\n",
            "Iter: 41 - Batch: 131/184 - Loss: 4.173325726024079e+48\n",
            "Iter: 41 - Batch: 141/184 - Loss: 1.1862554757218708e+52\n",
            "Iter: 41 - Batch: 151/184 - Loss: 5.228271499225328e+55\n",
            "Iter: 41 - Batch: 161/184 - Loss: 5.275346231891176e+58\n",
            "Iter: 41 - Batch: 171/184 - Loss: 1.1698295905990573e+62\n",
            "Iter: 41 - Batch: 181/184 - Loss: 1.7739380294567715e+65\n",
            "Iter: 42 - Batch: 1/184 - Loss: 2.609518926688519 -  To seeks are would relned to seeked dather thee stresher healt loven \n",
            "Iter: 42 - Batch: 11/184 - Loss: 806.7555583770859\n",
            "Iter: 42 - Batch: 21/184 - Loss: 1125799.3070995486\n",
            "Iter: 42 - Batch: 31/184 - Loss: 1838618794.0800266\n",
            "Iter: 42 - Batch: 41/184 - Loss: 14109082003105.89\n",
            "Iter: 42 - Batch: 51/184 - Loss: 6.21517508349637e+16\n",
            "Iter: 42 - Batch: 61/184 - Loss: 2.0440820999111305e+20\n",
            "Iter: 42 - Batch: 71/184 - Loss: 7.743280539821487e+23\n",
            "Iter: 42 - Batch: 81/184 - Loss: 2.4525974589312543e+27\n",
            "Iter: 42 - Batch: 91/184 - Loss: 1.4284147689051746e+31\n",
            "Iter: 42 - Batch: 101/184 - Loss: 7.378413865428583e+34\n",
            "Iter: 42 - Batch: 111/184 - Loss: 2.091334064267991e+38\n",
            "Iter: 42 - Batch: 121/184 - Loss: 1.2252113713240027e+42\n",
            "Iter: 42 - Batch: 131/184 - Loss: 4.967392298077139e+45\n",
            "Iter: 42 - Batch: 141/184 - Loss: 9.277971723229129e+48\n",
            "Iter: 42 - Batch: 151/184 - Loss: 2.505808167517035e+52\n",
            "Iter: 42 - Batch: 161/184 - Loss: 1.4599461112894882e+55\n",
            "Iter: 42 - Batch: 171/184 - Loss: 2.1418772970056475e+58\n",
            "Iter: 42 - Batch: 181/184 - Loss: 2.0981577319047524e+61\n",
            "Iter: 43 - Batch: 1/184 - Loss: 2.4509620672416403 -  To stresher hn seen, O' of my disdece of my nief, thee stresher hnnin\n",
            "Iter: 43 - Batch: 11/184 - Loss: 483.77800753290717\n",
            "Iter: 43 - Batch: 21/184 - Loss: 436448.22450812056\n",
            "Iter: 43 - Batch: 31/184 - Loss: 418298566.3671922\n",
            "Iter: 43 - Batch: 41/184 - Loss: 1872416309477.286\n",
            "Iter: 43 - Batch: 51/184 - Loss: 4594323371271157.0\n",
            "Iter: 43 - Batch: 61/184 - Loss: 9.47043917483105e+18\n",
            "Iter: 43 - Batch: 71/184 - Loss: 2.1059371495634573e+22\n",
            "Iter: 43 - Batch: 81/184 - Loss: 4.213163340725452e+25\n",
            "Iter: 43 - Batch: 91/184 - Loss: 1.435354296101776e+29\n",
            "Iter: 43 - Batch: 101/184 - Loss: 4.181273867746968e+32\n",
            "Iter: 43 - Batch: 111/184 - Loss: 7.438574776984366e+35\n",
            "Iter: 43 - Batch: 121/184 - Loss: 2.6660593432499777e+39\n",
            "Iter: 43 - Batch: 131/184 - Loss: 6.301784115112825e+42\n",
            "Iter: 43 - Batch: 141/184 - Loss: 7.350578121807722e+45\n",
            "Iter: 43 - Batch: 151/184 - Loss: 1.1966296535534447e+49\n",
            "Iter: 43 - Batch: 161/184 - Loss: 4.077821154828907e+51\n",
            "Iter: 43 - Batch: 171/184 - Loss: 3.9472045700698065e+54\n",
            "Iter: 43 - Batch: 181/184 - Loss: 2.6726593238111167e+57\n",
            "Iter: 44 - Batch: 1/184 - Loss: 2.3066867776203095 -  To stresher healt love, horth thee stresher hnning of thee stresher h\n",
            "Iter: 44 - Batch: 11/184 - Loss: 306.82343266683654\n",
            "Iter: 44 - Batch: 21/184 - Loss: 183648.14918517176\n",
            "Iter: 44 - Batch: 31/184 - Loss: 102735409.93411559\n",
            "Iter: 44 - Batch: 41/184 - Loss: 262240872063.4421\n",
            "Iter: 44 - Batch: 51/184 - Loss: 368081541449852.56\n",
            "Iter: 44 - Batch: 61/184 - Loss: 4.7990236866431763e+17\n",
            "Iter: 44 - Batch: 71/184 - Loss: 6.466642670664222e+20\n",
            "Iter: 44 - Batch: 81/184 - Loss: 8.165943999928632e+23\n",
            "Iter: 44 - Batch: 91/184 - Loss: 1.5742788771369963e+27\n",
            "Iter: 44 - Batch: 101/184 - Loss: 2.659123437250916e+30\n",
            "Iter: 44 - Batch: 111/184 - Loss: 3.068396174425103e+33\n",
            "Iter: 44 - Batch: 121/184 - Loss: 6.854779591075373e+36\n",
            "Iter: 44 - Batch: 131/184 - Loss: 9.334176834993357e+39\n",
            "Iter: 44 - Batch: 141/184 - Loss: 6.534320938174452e+42\n",
            "Iter: 44 - Batch: 151/184 - Loss: 6.31642428989312e+45\n",
            "Iter: 44 - Batch: 161/184 - Loss: 1.43930774931871e+48\n",
            "Iter: 44 - Batch: 171/184 - Loss: 9.079225465634272e+50\n",
            "Iter: 44 - Batch: 181/184 - Loss: 4.37644644429415e+53\n",
            "Iter: 45 - Batch: 1/184 - Loss: 2.1933224510611002 -  To sne stresher hate of my disdece thee shall of my disdece thee stre\n",
            "Iter: 45 - Batch: 11/184 - Loss: 203.66142465362512\n",
            "Iter: 45 - Batch: 21/184 - Loss: 77337.91267944755\n",
            "Iter: 45 - Batch: 31/184 - Loss: 26101986.331947867\n",
            "Iter: 45 - Batch: 41/184 - Loss: 36913054287.594154\n",
            "Iter: 45 - Batch: 51/184 - Loss: 28691508518108.61\n",
            "Iter: 45 - Batch: 61/184 - Loss: 2.463833300954822e+16\n",
            "Iter: 45 - Batch: 71/184 - Loss: 2.1165373314661044e+19\n",
            "Iter: 45 - Batch: 81/184 - Loss: 1.6256646550900387e+22\n",
            "Iter: 45 - Batch: 91/184 - Loss: 1.6667208963279931e+25\n",
            "Iter: 45 - Batch: 101/184 - Loss: 1.6266507730119095e+28\n",
            "Iter: 45 - Batch: 111/184 - Loss: 1.1912901676231972e+31\n",
            "Iter: 45 - Batch: 121/184 - Loss: 1.629445822177555e+34\n",
            "Iter: 45 - Batch: 131/184 - Loss: 1.3327473361419358e+37\n",
            "Iter: 45 - Batch: 141/184 - Loss: 5.714949504437051e+39\n",
            "Iter: 45 - Batch: 151/184 - Loss: 3.225787428783615e+42\n",
            "Iter: 45 - Batch: 161/184 - Loss: 5.101837113618615e+44\n",
            "Iter: 45 - Batch: 171/184 - Loss: 2.096366447169501e+47\n",
            "Iter: 45 - Batch: 181/184 - Loss: 7.009824849264958e+49\n",
            "Iter: 46 - Batch: 1/184 - Loss: 2.0571754641046733 -  To seeks dath thee star len hn the true she lase,   As as it of my di\n",
            "Iter: 46 - Batch: 11/184 - Loss: 133.4357373740145\n",
            "Iter: 46 - Batch: 21/184 - Loss: 33595.83485770492\n",
            "Iter: 46 - Batch: 31/184 - Loss: 7313412.84719361\n",
            "Iter: 46 - Batch: 41/184 - Loss: 5828486657.138997\n",
            "Iter: 46 - Batch: 51/184 - Loss: 2402303336782.462\n",
            "Iter: 46 - Batch: 61/184 - Loss: 1415156122510242.0\n",
            "Iter: 46 - Batch: 71/184 - Loss: 7.973952618905664e+17\n",
            "Iter: 46 - Batch: 81/184 - Loss: 3.74953768701135e+20\n",
            "Iter: 46 - Batch: 91/184 - Loss: 2.1496696754965802e+23\n",
            "Iter: 46 - Batch: 101/184 - Loss: 1.091670825954168e+26\n",
            "Iter: 46 - Batch: 111/184 - Loss: 4.98831973684846e+28\n",
            "Iter: 46 - Batch: 121/184 - Loss: 4.326006338237566e+31\n",
            "Iter: 46 - Batch: 131/184 - Loss: 2.212693443328827e+34\n",
            "Iter: 46 - Batch: 141/184 - Loss: 5.730063069612958e+36\n",
            "Iter: 46 - Batch: 151/184 - Loss: 1.87123738852933e+39\n",
            "Iter: 46 - Batch: 161/184 - Loss: 1.8990260440436824e+41\n",
            "Iter: 46 - Batch: 171/184 - Loss: 5.13295613543574e+43\n",
            "Iter: 46 - Batch: 181/184 - Loss: 1.179615099105633e+46\n",
            "Iter: 47 - Batch: 1/184 - Loss: 1.9481334413821545 - ir now far my lase, horth thee star let disdece, mine, To seeks are th\n",
            "Iter: 47 - Batch: 11/184 - Loss: 89.21009758850856\n",
            "Iter: 47 - Batch: 21/184 - Loss: 15034.907850793948\n",
            "Iter: 47 - Batch: 31/184 - Loss: 2154906.2147625214\n",
            "Iter: 47 - Batch: 41/184 - Loss: 935908864.2166138\n",
            "Iter: 47 - Batch: 51/184 - Loss: 222128160875.82343\n",
            "Iter: 47 - Batch: 61/184 - Loss: 88982523802631.66\n",
            "Iter: 47 - Batch: 71/184 - Loss: 3.2309741689244068e+16\n",
            "Iter: 47 - Batch: 81/184 - Loss: 9.177071301981819e+18\n",
            "Iter: 47 - Batch: 91/184 - Loss: 2.9986560306643474e+21\n",
            "Iter: 47 - Batch: 101/184 - Loss: 8.000399480886953e+23\n",
            "Iter: 47 - Batch: 111/184 - Loss: 2.14091330549579e+26\n",
            "Iter: 47 - Batch: 121/184 - Loss: 1.0184817162353243e+29\n",
            "Iter: 47 - Batch: 131/184 - Loss: 3.191097846759822e+31\n",
            "Iter: 47 - Batch: 141/184 - Loss: 5.455075845912853e+33\n",
            "Iter: 47 - Batch: 151/184 - Loss: 1.0390279776430241e+36\n",
            "Iter: 47 - Batch: 161/184 - Loss: 7.06070630398287e+37\n",
            "Iter: 47 - Batch: 171/184 - Loss: 1.2495251755656273e+40\n",
            "Iter: 47 - Batch: 181/184 - Loss: 1.9294737897176776e+42\n",
            "Iter: 48 - Batch: 1/184 - Loss: 1.8813258842546179 - ir now far my lase, hore aweet hnace strom thee still nid con thee sti\n",
            "Iter: 48 - Batch: 11/184 - Loss: 60.29925231140858\n",
            "Iter: 48 - Batch: 21/184 - Loss: 6810.405880628164\n",
            "Iter: 48 - Batch: 31/184 - Loss: 636952.9439008599\n",
            "Iter: 48 - Batch: 41/184 - Loss: 152950309.5083782\n",
            "Iter: 48 - Batch: 51/184 - Loss: 21548286212.78803\n",
            "Iter: 48 - Batch: 61/184 - Loss: 5487328984687.879\n",
            "Iter: 48 - Batch: 71/184 - Loss: 1192035782359877.2\n",
            "Iter: 48 - Batch: 81/184 - Loss: 2.001138675858164e+17\n",
            "Iter: 48 - Batch: 91/184 - Loss: 4.081863595847968e+19\n",
            "Iter: 48 - Batch: 101/184 - Loss: 6.801372854155551e+21\n",
            "Iter: 48 - Batch: 111/184 - Loss: 1.1539661974157494e+24\n",
            "Iter: 48 - Batch: 121/184 - Loss: 3.605631818553509e+26\n",
            "Iter: 48 - Batch: 131/184 - Loss: 7.50100916224414e+28\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2d36593d25af>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-2d36593d25af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0fd394e6b29d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mfrom_prev_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_hh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_ih\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfrom_prev_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mnew_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0fd394e6b29d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0fd394e6b29d>\u001b[0m in \u001b[0;36mmm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             return Tensor(self.data.dot(x.data),\n\u001b[0m\u001b[1;32m    186\u001b[0m                           \u001b[0mautograd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                           \u001b[0mcreators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "relu = lambda x: (x>0).astype(float) * x\n",
        "\n",
        "weights = np.array([[1,4],[4,1]])\n",
        "activation = sigmoid(np.array([1,0,0.1]))\n",
        "\n",
        "print(\"Sigmoid activations:\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "    activation = sigmoid(activation.dot(weights))\n",
        "    activations.append(activation)\n",
        "    print(activation)\n",
        "\n",
        "print(\"Sigmoid gradients:\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "    gradient = (activation * (1 - activation) * gradient)\n",
        "    gradient = gradient.dot(weights.transpose())\n",
        "    print(gradient)\n",
        "\n",
        "print(\"Relu activations:\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "    activation = relu(activation.dot(weights))\n",
        "    activations.append(activation)\n",
        "    print(activation)\n",
        "\n",
        "print(\"Relu gradients:\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "      gradient = (activation > 0) * gradient\n",
        "      gradient = gradient.dot(weights.transpose())\n",
        "      print(gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "qoXCbxG46PkI",
        "outputId": "29abedea-1b4f-4b93-cbf3-6f74145c544d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid activations:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (3,) and (2,2) not aligned: 3 (dim 0) != 2 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5def93d918f5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (2,2) not aligned: 3 (dim 0) != 2 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        self.bias = bias\n",
        "\n",
        "        if (self.bias):\n",
        "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "\n",
        "        self.parameters.append(self.weight)\n",
        "\n",
        "        if (self.bias):\n",
        "            self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if (self.bias):\n",
        "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
        "        return input.mm(self.weight)\n",
        "\n",
        "class LSTMCell (Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_hidden, n_output):\n",
        "        super().__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.xf = Linear(n_inputs, n_hidden)\n",
        "        self.xi = Linear(n_inputs, n_hidden)\n",
        "        self.xo = Linear(n_inputs, n_hidden)\n",
        "        self.xc = Linear(n_inputs, n_hidden)\n",
        "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hc = Linear(n_hidden, n_hidden, bias=False)\n",
        "\n",
        "        self.w_ho = Linear(n_hidden, n_output, bias = False)\n",
        "\n",
        "        self.parameters += self.xf.get_parameters()\n",
        "        self.parameters += self.xi.get_parameters()\n",
        "        self.parameters += self.xo.get_parameters()\n",
        "        self.parameters += self.xc.get_parameters()\n",
        "        self.parameters += self.hf.get_parameters()\n",
        "        self.parameters += self.hi.get_parameters()\n",
        "        self.parameters += self.ho.get_parameters()\n",
        "        self.parameters += self.hc.get_parameters()\n",
        "\n",
        "        self.parameters += self.w_ho.get_parameters()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        prev_hidden = hidden[0]\n",
        "        prev_cell = hidden[1]\n",
        "\n",
        "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
        "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
        "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()\n",
        "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()\n",
        "        c = (f * prev_cell) + (i * g)\n",
        "        h = o * c.tanh()\n",
        "        output = self.w_ho.forward(h)\n",
        "        return output, (h,c)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
        "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
        "        h.data[:,0] += 1\n",
        "        c.data[:,0] += 1\n",
        "        return (h,c)"
      ],
      "metadata": {
        "id": "9aPIBky0_8NU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Freeze the seed for reproducability.\n",
        "np.random.seed(0)\n",
        "\n",
        "# Prepare the dataset.\n",
        "f = open(\"shakespeare.txt\", \"r\")\n",
        "raw = f.read()\n",
        "f.close()\n",
        "\n",
        "vocab = list(set(raw))\n",
        "word2index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "    word2index[word] = i\n",
        "indices = np.array(list(map(lambda x: word2index[x], raw)))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.05\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "input_dim = len(vocab)\n",
        "output_dim = len(vocab)\n",
        "hidden_dim = 512\n",
        "bptt = 25\n",
        "min_loss = 1_000\n",
        "\n",
        "# Batch the dataset\n",
        "n_batches = int((indices.shape[0] / (batch_size)))\n",
        "\n",
        "trimmed_indices = indices[:n_batches*batch_size]\n",
        "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
        "batched_indices = batched_indices.transpose()\n",
        "\n",
        "input_batched_indices = batched_indices[0:-1]\n",
        "target_batched_indices = batched_indices[1:]\n",
        "\n",
        "n_bptt = int((n_batches-1)/ bptt)\n",
        "input_batches = input_batched_indices[:n_bptt*bptt]\n",
        "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
        "target_batches = target_batched_indices[:n_bptt*bptt]\n",
        "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
        "\n",
        "# Initialize the model.\n",
        "embed = Embedding(vocab_size=input_dim, dim=512)\n",
        "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=output_dim)\n",
        "# This seemed to help training\n",
        "model.w_ho.weight.data *= 0\n",
        "\n",
        "# Loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "# Optimizer\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=alpha)\n",
        "\n",
        "# Training loop\n",
        "for j in range(epochs):\n",
        "        total_loss = 0\n",
        "        n_loss = 0\n",
        "        hidden = model.init_hidden(batch_size=batch_size)\n",
        "        for batch_i in range(len(input_batches)):\n",
        "            # Detach hidden state from previous graph\n",
        "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
        "            batch_loss = 0\n",
        "            for t in range(bptt):\n",
        "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
        "                rnn_input = embed.forward(input=input)\n",
        "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "\n",
        "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
        "                loss = criterion.forward(output, target)\n",
        "\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "\n",
        "                # Detach hidden state\n",
        "                hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
        "\n",
        "                batch_loss += loss.data\n",
        "\n",
        "            total_loss += batch_loss / bptt\n",
        "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
        "            log = f\"Iter: {j} - Batch: {batch_i+1}/{len(input_batches)} - Loss: {epoch_loss}\"\n",
        "\n",
        "            if (batch_i == 0):\n",
        "                log += \" - \" + generate_sample(70, '\\n').replace('\\n', ' ')\n",
        "            if (batch_i % 10 == 0 or batch_i - 1 == len(input_batches)):\n",
        "                print(log)\n",
        "        optim.alpha *= 0.99"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0VA2t14CvMj",
        "outputId": "33c27c95-3a4d-4481-8e89-97d6cb06d08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0 - Batch: 1/235 - Loss: 60.65867496848204 - nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn nnnnnnnnnnnnnnnnn\n",
            "Iter: 0 - Batch: 11/235 - Loss: 26.485573399719428\n",
            "Iter: 0 - Batch: 21/235 - Loss: 20.637684070316354\n",
            "Iter: 0 - Batch: 31/235 - Loss: 17.610659804462472\n",
            "Iter: 0 - Batch: 41/235 - Loss: 15.697698161018572\n",
            "Iter: 0 - Batch: 51/235 - Loss: 14.584184372991414\n",
            "Iter: 0 - Batch: 61/235 - Loss: 13.68921950422112\n",
            "Iter: 0 - Batch: 71/235 - Loss: 12.998557024774252\n",
            "Iter: 0 - Batch: 81/235 - Loss: 12.471154658312816\n",
            "Iter: 0 - Batch: 91/235 - Loss: 12.05421103329198\n",
            "Iter: 0 - Batch: 101/235 - Loss: 11.665176993495805\n",
            "Iter: 0 - Batch: 111/235 - Loss: 11.346881896201026\n",
            "Iter: 0 - Batch: 121/235 - Loss: 11.063402353078773\n",
            "Iter: 0 - Batch: 131/235 - Loss: 10.777177018962998\n",
            "Iter: 0 - Batch: 141/235 - Loss: 10.516338419410673\n",
            "Iter: 0 - Batch: 151/235 - Loss: 10.32922678346088\n",
            "Iter: 0 - Batch: 161/235 - Loss: 10.15114488385659\n",
            "Iter: 0 - Batch: 171/235 - Loss: 9.987193874227888\n",
            "Iter: 0 - Batch: 181/235 - Loss: 9.838415114103784\n",
            "Iter: 0 - Batch: 191/235 - Loss: 9.731224012506857\n",
            "Iter: 0 - Batch: 201/235 - Loss: 9.614857937170134\n",
            "Iter: 0 - Batch: 211/235 - Loss: 9.480204400885867\n",
            "Iter: 0 - Batch: 221/235 - Loss: 9.396885818915722\n",
            "Iter: 0 - Batch: 231/235 - Loss: 9.30373613639202\n",
            "Iter: 1 - Batch: 1/235 - Loss: 9.941368316860983 - The see the the see the see the not the see the see the not me the not\n",
            "Iter: 1 - Batch: 11/235 - Loss: 7.251409891760098\n",
            "Iter: 1 - Batch: 21/235 - Loss: 7.08966682950639\n",
            "Iter: 1 - Batch: 31/235 - Loss: 7.069291376318427\n",
            "Iter: 1 - Batch: 41/235 - Loss: 6.956962367649572\n",
            "Iter: 1 - Batch: 51/235 - Loss: 6.946268372843779\n",
            "Iter: 1 - Batch: 61/235 - Loss: 6.8947907431169035\n",
            "Iter: 1 - Batch: 71/235 - Loss: 6.879674304050089\n",
            "Iter: 1 - Batch: 81/235 - Loss: 6.874584601597741\n",
            "Iter: 1 - Batch: 91/235 - Loss: 6.8623400462965805\n",
            "Iter: 1 - Batch: 101/235 - Loss: 6.827396554138273\n",
            "Iter: 1 - Batch: 111/235 - Loss: 6.798488171590018\n",
            "Iter: 1 - Batch: 121/235 - Loss: 6.765676121367727\n",
            "Iter: 1 - Batch: 131/235 - Loss: 6.723550381908076\n",
            "Iter: 1 - Batch: 141/235 - Loss: 6.677448047537648\n",
            "Iter: 1 - Batch: 151/235 - Loss: 6.667360976142544\n",
            "Iter: 1 - Batch: 161/235 - Loss: 6.654389232116101\n",
            "Iter: 1 - Batch: 171/235 - Loss: 6.633982131712563\n",
            "Iter: 1 - Batch: 181/235 - Loss: 6.614545565072758\n",
            "Iter: 1 - Batch: 191/235 - Loss: 6.624853870394133\n",
            "Iter: 1 - Batch: 201/235 - Loss: 6.614550501615329\n",
            "Iter: 1 - Batch: 211/235 - Loss: 6.588039868533666\n",
            "Iter: 1 - Batch: 221/235 - Loss: 6.593219590933408\n",
            "Iter: 1 - Batch: 231/235 - Loss: 6.591266763595635\n",
            "Iter: 2 - Batch: 1/235 - Loss: 8.514725510958057 - The see the see not the see the see the see not not the see the see th\n",
            "Iter: 2 - Batch: 11/235 - Loss: 6.278700731102501\n",
            "Iter: 2 - Batch: 21/235 - Loss: 6.132388218415364\n",
            "Iter: 2 - Batch: 31/235 - Loss: 6.145040052261321\n",
            "Iter: 2 - Batch: 41/235 - Loss: 6.069961920182004\n",
            "Iter: 2 - Batch: 51/235 - Loss: 6.07107667872069\n",
            "Iter: 2 - Batch: 61/235 - Loss: 6.040794009475372\n",
            "Iter: 2 - Batch: 71/235 - Loss: 6.041304717207064\n",
            "Iter: 2 - Batch: 81/235 - Loss: 6.052363091766663\n",
            "Iter: 2 - Batch: 91/235 - Loss: 6.052147451253681\n",
            "Iter: 2 - Batch: 101/235 - Loss: 6.0344307206382135\n",
            "Iter: 2 - Batch: 111/235 - Loss: 6.019750379555566\n",
            "Iter: 2 - Batch: 121/235 - Loss: 6.001917852886025\n",
            "Iter: 2 - Batch: 131/235 - Loss: 5.973540726800106\n",
            "Iter: 2 - Batch: 141/235 - Loss: 5.947457773354632\n",
            "Iter: 2 - Batch: 151/235 - Loss: 5.947862884198959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkKIORfMEMz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}